---
title: "FERA 2017 - Addressing head pose in the third facial expression recognition and analysis challenge"
type: "proceeding"
author: "Valstar, Sánchez-Lozano, Cohn, et al."
year: "2017"
publication: "FG"
preprint: ""
doi: "https://doi.org/10.1109/FG.2017.107"
materials: ""
toc: false
categories:
  - nonverbal
  - database
  - proceeding
---

## Citation (APA 7)

> Valstar, M. F., Sanchez-Lozano, E., Cohn, J. F., Jeni, L. A., Girard, J. M., Zhang, Z., Yin, L., & Pantic, M. (2017). FERA 2017—Addressing head pose in the third facial expression recognition and analysis challenge. *Proceedings of the 12th IEEE Conference on Automatic Face & Gesture Recognition (FG)*, 839–847.

## Abstract

The field of Automatic Facial Expression Analysis has grown rapidly in recent years. However, despite progress in new approaches as well as benchmarking efforts, most evaluations still focus on either posed expressions, near-frontal recordings, or both. This makes it hard to tell how existing expression recognition approaches perform under conditions where faces appear in a wide range of poses (or camera views), displaying ecologically valid expressions. The main obstacle for assessing this is the availability of suitable data, and the challenge proposed here addresses this limitation. The FG 2017 Facial Expression Recognition and Analysis challenge (FERA 2017) extends FERA 2015 to the estimation of Action Units occurrence and intensity under different camera views. In this paper we present the third challenge in automatic recognition of facial expressions, to be held in conjunction with the 12th IEEE conference on Face and Gesture Recognition, May 2017, in Washington, United States. Two sub-challenges are defined: the detection of AU occurrence, and the estimation of AU intensity. In this work we outline the evaluation protocol, the data used, and the results of a baseline method for both sub-challenges.
