[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "AffCom Lab",
    "section": "",
    "text": "Home\n    Resources"
  },
  {
    "objectID": "resources.html#datasets",
    "href": "resources.html#datasets",
    "title": "AffCom Lab",
    "section": "Datasets",
    "text": "Datasets\n\nSeamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset A large-scale collection of over 4,000 hours of face-to-face interaction footage from over participants (n = 4000) in diverse contexts for social AI research. Primary Article/Citation: Agrawal et al. (2025) Website: https://ai.meta.com/research/seamless-interaction/ Open Access: https://github.com/facebookresearch/seamless_interaction\nDynAMoS: Dynamic Affective Movie Clip Database for Subjectivity Analysis A curated collection of affective movie clips (n = 22), metadata about those clips, and ratings of the emotional content of those movie clips from research participants (n = 83) Primary Article/Citation: Girard, Tie, & Liebenthal (2023) Website/Request Access: https://dynamos.mgb.org\nGFT: Group Formation Task Spontaneous Facial Expression Database A 2D video database of spontaneous behavior during unscripted social interactions Complete with frame-level FACS annotations of facial behavior (n = 96) Primary Article/Citation: Girard et al. (2017) Website/Request Access: http://osf.io/7wcyz\nMMSE/BP4D+: Multimodal Spontaneous Emotion Database An extension of BP4D that includes thermal and physiological data Complete with frame-level FACS annotations of facial behavior (n = 140) Primary Article/Citation: Zhang et al. (2016) Website/Request Access: http://bit.ly/2yg39Cn\nBP4D: Binghamton-Pittsburgh 4D Spontaneous Emotion Database A 3D video database of spontaneous behavior during emotion elicitation Complete with frame-level FACS annotations of facial behavior (n = 41) Primary Article/Citation: Zhang et al. (2014) Website/Request Access: http://bit.ly/2yg39Cn"
  },
  {
    "objectID": "resources.html#software",
    "href": "resources.html#software",
    "title": "AffCom Lab",
    "section": "Software",
    "text": "Software\n\nStatistics Software:\n\ncircumplex - A stable R package for analyzing and visualizing circular data.\nmReliability - A stable (but approaching end-of-life) set of MATLAB functions for estimating inter-rater reliability.\nwcc - An experimental R package for conducting windowed cross-correlation analyses.\nvarde - An experimental R package for decomposing the variance in mixed effects models.\nagreement - An experimental R package for estimating inter-rater reliability.\n\n\n\nResearch Software:\n\nCARMA - A stable (but approaching end-of-life) software for collecting continuous media annotations.\nDARMA - A stable (but approaching end-of-life) software for collecting continuous two-dimensional media annotations.\ntidymedia - An experimental R package for working with audio, video, and image files.\nopenac - An experimental R package for automating common affective computing (and social signal processing) tasks.\nfacs - An experimental R package for working with Facial Action Coding System (FACS) data.\nhitop - An experimental R package for working with Hierarchical Taxonomy of Psychopathology (HiTOP) data.\n\n\n\nQuarto Extensions\n\nlordicon - A stable Quarto extension for embedding animated Lordicon icons in HTML.\nhoneypot - An experimental Quarto extension for detecting LLM cheating on HTML assignments.\ndetails - A stable Quarto extension for embedding interactive details blocks in HTML documents.\nembedpdf - A stable (but questioning) Quarto extension for embedding PDF files in HTML.\n\n\n\nDocker Images:\n\nrstudio2u - A multiarchitecture (amd64/arm64) image combining rstudio and r2u\nrocker-bayes - A multiarchitecture (amd64/arm64) image for Bayesian analysis in R\nwsl-whisper - A WSL (amd64) image for AI transcription from R with CUDA-support"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "AffCom Lab",
    "section": "",
    "text": "Home\n    Publications\n  \n\nThis page lists all papers published by Dr. Girard, organized by year of publication. Click a paper’s title to visit the publisher page and access the official version. Use Abstract to reveal the summary, Citation for the APA 7th-edition reference, and BibTeX to view the BibTeX entry. Clicking the link a second time will close the display again. Preprint links to a free author version (not the final typeset copy), and Materials leads to an open repository with data, code, and other open-science resources. ★ = Work led or co-led by our lab (established in 2020)\n\n\n\n\n\n\n\n\n2025\n\n\n\nAlcohol and Emotion: Analyzing Convergence between Facially Expressed and Self-Reported Indices of Emotion under Alcohol Intoxication\n\n\nCaumiant, Kang, Girard & Fairbairn\n\nPsychology of Addictive Behaviors\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nObjective: Emotion measurement is central to capturing acute alcohol reinforcement and so to informing models of alcohol use disorder etiology. Yet our understanding of how alcohol impacts emotion as assessed across diverse response modalities remains incomplete. The present study leverages a social alcohol-administration paradigm to assess drinking-related emotions, aiming to elucidate impacts of intoxication on self-reported versus behaviorally expressed emotion. Method: Participants (N = 60; Mage = 22.5; 50\\% male; 55\\% White) attended two counterbalanced laboratory sessions, on one of which they were administered an alcoholic beverage (target blood alcohol content .08\\%) and on the other a nonalcoholic control beverage. Participants in both conditions were accurately informed of beverage contents and consumed study beverages in assigned groups of three while their behavior was videotaped. Emotion was assessed via self-report as well as continuous coding of facial muscle movements. Results: The relationship between self-reported and behaviorally expressed emotion diverged significantly across beverage conditions: positive affect: b = -0.174, t = -2.36, p = .022; negative affect, b = 0.4319, t = 2.37, p = .021. Specifically, self-reports and behavioral displays converged among sober but not intoxicated participants. Further, alcohol's effects on positive facial displays remained significant in models controlling for self-reported positive and negative emotion, with alcohol enhancing Duchenne smiles 20\\% beyond effects captured via self-reports, pointing to unique effects of alcohol on behavioral indicators of positive emotion. Conclusions: Findings highlight effects of acute intoxication on the convergence and divergence of emotion measures, thus informing our understanding of measures for capturing emotions that are most proximal to drinking and thus most immediately reinforcing of alcohol consumption. (PsycInfo Database Record (c) 2025 APA, all rights reserved)\n@article{caumiant2025,\n  title = {Alcohol and Emotion: Analyzing Convergence between Facially Expressed and Self-Reported Indices of Emotion under Alcohol Intoxication},\n  author = {Eddie P. Caumiant and Dahyeon Kang and Jeffrey M. Girard and Catharine E. Fairbairn},\n  year = {2025},\n  journal = {Psychology of Addictive Behaviors},\n  doi = {10.1037/adb0001053}\n}\n\n\n\n\nBuilding Open Science into Graduate Training in Clinical Psychology\n\n\nGirard\n\nClinical Psychological Science\n\n\n\n★\n This project was led by the AffCom lab\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nOpen science promises greater transparency and rigor, yet clinical psychology has lagged in integrating it into graduate training. This commentary builds on recent evidence that open-science instruction in U.S. programs remains uneven and largely conceptual. I argue that progress requires more than awareness: it depends on pedagogical scaffolding, computational infrastructure, and cultural alignment. Embedding reproducible, code-based workflows, literate programming, and ethical transparency throughout training can make openness both feasible and normative. By treating transparency as a professional competency, supported by open educational resources and institutional incentives, clinical psychology can model how to cultivate a sustainable culture of open science.\n@article{girard2025b,\n  title = {Building Open Science into Graduate Training in Clinical Psychology},\n  author = {Jeffrey M Girard},\n  year = {2025},\n  journal = {Clinical Psychological Science},\n  doi = {osf.io/preprints/psyarxiv/jeuyk}\n}\n\n\n\n\nComputational Analysis of Expressive Behavior in Clinical Assessment\n\n\nGirard, Yermol, Salah & Cohn\n\nAnnual Review of Clinical Psychology\n\n\n\n★\n This project was led by the AffCom lab\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nClinical psychological assessment often relies on self-report, interviews, and behavioral observation, methods that pose challenges for reliability, validity, and scalability. Computational approaches offer new opportunities to analyze expressive behavior (e.g., facial expressions, vocal prosody, and language use) with greater precision and efficiency. This paper provides an accessible conceptual framework for understanding how methods from computer vision, speech signal processing, and natural language processing can enhance clinical assessment. We outline the goals, frameworks, and methods of both clinical and computational approaches, and present an illustrative review of interdisciplinary research applying these techniques across a range of mental health conditions. We also examine key challenges related to data quality, measurement, interdisciplinarity, and ethics. Finally, we highlight future directions for building systems that are robust, interpretable, and clinically meaningful. This review is intended to support dialogue between clinical and computational communities and to guide ongoing research and development at their intersection.\n@article{girard2025,\n  title = {Computational Analysis of Expressive Behavior in Clinical Assessment},\n  author = {Jeffrey M Girard and Dasha A Yermol and Albert Ali Salah and Jeffrey F Cohn},\n  year = {2025},\n  journal = {Annual Review of Clinical Psychology},\n  doi = {10.1146/annurev-clinpsy-081423-024140}\n}\n\n\n\n\nDepressive Symptoms among Hispanic Americans: Investigating the Interplay of Acculturation and Demographics\n\n\nRincon Caicedo, Girard, Punt, Giovanetti & Ilardi\n\nJournal of Latinx Psychology\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nAs with many other racial and ethnic minorities, Hispanic Americans face substantial disparities in health care access and disease prevalence. The published literature on mental health disorders among Hispanic individuals, however, is not robust, and their experience of depressive disorders remains poorly understood. The construct of acculturation may help elucidate the risk of depression among Hispanic Americans and may inform the development of appropriate policy and treatment resources. We examined the degree to which acculturation may interact with key demographic variables (sex, age, socioeconomic status [SES], and Mexican ancestry) in accounting for depressive symptomatology among Hispanic Americans. We conducted a series of Bayesian generalized linear mixed models using data from the National Health and Nutrition Examination Survey to investigate the self-reported depressive symptomatology (measured by the Patient Health Questionnaire-9) of Mexican Americans and other Hispanic individuals and to examine possible effects of acculturation and demographics on depressive symptomatology in this population. Mexican Americans had substantially lower levels of depression than other Hispanic individuals. Acculturation was positively associated with depression severity, but this effect was moderated by sex and SES. High acculturation was more strongly linked to depression among men and those of high SES. Acculturation and several demographic factors were associated with depressive symptomatology among Hispanic individuals. Acculturation can be useful in understanding risk, developing culturally informed interventions, and implementing community-level changes to address the burden of Hispanic depression. (PsycInfo Database Record (c) 2025 APA, all rights reserved)\n@article{rinconcaicedo2025,\n  title = {Depressive Symptoms among Hispanic Americans: Investigating the Interplay of Acculturation and Demographics},\n  author = {Mariana {Rincon Caicedo} and Jeffrey M. Girard and Stephanie E. Punt and Annaleis K. Giovanetti and Stephen S. Ilardi},\n  year = {2025},\n  journal = {Journal of Latinx Psychology},\n  volume = {13},\n  number = {1},\n  pages = {68--84},\n  doi = {10.1037/lat0000266}\n}\n\n\n\n\nDynamic and Dyadic Relationships between Facial Behavior, Working Alliance, and Treatment Outcomes during Depression Therapy\n\n\nGirard, Yermol, Bylsma, Cohn, Fournier, Morency & Swartz\n\nJournal of Consulting and Clinical Psychology\n\n\n\n★\n This project was led by the AffCom lab\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nObjective: Previous work has yielded mixed results regarding the relationships of interpersonal synchrony with working alliance and treatment outcomes. We sought to clarify these relationships by applying more sophisticated dynamic and dyadic methods. Method: Adult outpatients with depression (N=65, Age 18-64, 65\\% Female, 68\\% White) participated in up to eight sessions of cognitive-behavioral or interpersonal psychotherapy. Sessions occurred either in-person or via teletherapy and were video-recorded. Facial computing tools estimated the momentary intensity of patients' and therapists' scowling and smiling, and the interpersonal synchrony of these measures during each session was quantified using windowed cross-correlation analyses. Hypotheses about within-dyad processes (i.e., session-to-session changes) and between-dyad associations (i.e., of average tendencies) were tested using hierarchical Bayesian mediation models. Results: Numerous and nuanced associations were found between facial behavior, working alliance, and treatment outcomes (e.g., session-to-session increases in patient-rated working alliance were predicted by higher-than-average therapist smiling). Interpersonal synchrony did not predict within-dyad changes in working alliance or depression symptoms; however, on the between-dyad level, higher average patient-rated working alliance was predicted by both higher average smile synchrony and lower average scowl synchrony. Conclusions: The facial behavior of patients and therapists provides a valuable window into the therapeutic process, especially with regard to the alliance. Contributions come from the behaviors of each person as well as their level of interpersonal synchrony. However, contrary to extant theories, the associations of synchrony with working alliance and outcomes depend on the behavior being synchronized, the informant providing ratings, and the level of analysis being conducted. Public Health Significance: These results help refine clinical theories about psychotherapy and unlock new possibilities for computationally assessing its progress.\n@article{girard2025a,\n  title = {Dynamic and Dyadic Relationships between Facial Behavior, Working Alliance, and Treatment Outcomes during Depression Therapy},\n  author = {Jeffrey M Girard and Dasha A Yermol and Lauren M Bylsma and Jeffrey F Cohn and Jay C Fournier and Louis-Philippe Morency and Holly A Swartz},\n  year = {2025},\n  journal = {Journal of Consulting and Clinical Psychology},\n  volume = {93},\n  number = {11},\n  pages = {749--760},\n  doi = {10.1037/ccp0000980}\n}\n\n\n\n\nEXPRESS: How Influencers Grow: An Empirical Study and Future Research Agenda\n\n\nCampbell, Girard, McDuff & Rosengren\n\nJournal of Interactive Marketing\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nMost research on influencers takes the perspective of marketers, examining how influencers can be leveraged to build brands. However, as influencers grow in popularity, they are becoming a marketing force in their own right, warranting deeper exploration from their perspective. This paper shifts the focus to influencers as active marketers, using a multilevel mixed-effects growth model to identify factors associated with follower growth. Analyzing a dataset of 14,311,145 pieces of Instagram content posted over more than two years from 6,079 influencers across 57 countries, we examine how attention labor (content strategy and persona appeal) and relationship labor (captioning strategy and ecosystem connectivity) relate to follower growth. Whereas existing research primarily focuses on engagement with specific pieces of content, this study takes a broader approach, investigating how influencers' content and engagement strategies are associated with differences in follower growth over time. By reframing influencers as marketers rather than media vehicles, this study contributes to marketing theory and provides insights for influencers, influencer agencies, and marketers who seek to better understand influencer growth.\n@article{campbell2025,\n  title = {{{EXPRESS}}: {{How}} Influencers Grow: An Empirical Study and Future Research Agenda},\n  author = {Colin Campbell and Jeffrey M. Girard and Daniel McDuff and Sara Rosengren},\n  year = {2025},\n  journal = {Journal of Interactive Marketing},\n  pages = {10949968251360683},\n  doi = {10.1177/10949968251360683},\n  note = {Last visited on 09/03/2025}\n}\n\n\n\n\nFrom Intuition to Innovation: Empirical Illustrations of Multimodal Measurement in Psychotherapy Research\n\n\nAafjes-van Doorn & Girard\n\nPsychotherapy Research\n\n\n\n★\n This project was led by the AffCom lab\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nThis special section underscores the potential of multimodal measurement approaches to transform psychotherapy research. A multimodal approach provides a more comprehensive understanding than any single modality (type of collected information) can provide on its own. Traditionally, clinicians and researchers have relied on their intuition, experience, and training to integrate different types of information in a psychotherapy session/treatment. Increasingly, however, computational methods offer a complementary alternative, enabling more automated, data-driven, and reproducible solutions. The six empirical examples in this special section illustrate the emerging---and often interdisciplinary---methodologies, including text, audio, video, and physiological measures, that are relevant in the psychotherapy setting. While each study addressed distinct research questions and employed unique methodologies, they all demonstrated a commitment to leveraging multimodal measurement and tackling the challenges of integrating diverse data sources.\n@article{aafjes-vandoorn2025,\n  title = {From Intuition to Innovation: Empirical Illustrations of Multimodal Measurement in Psychotherapy Research},\n  author = {Katie {Aafjes-van Doorn} and Jeffrey M. Girard},\n  year = {2025},\n  journal = {Psychotherapy Research},\n  volume = {35},\n  number = {2},\n  pages = {171--173},\n  doi = {10/g824tc}\n}\n\n\n\n\nIndividualized Machine-Learning-Based Clinical Assessment Recommendation System\n\n\nSetiawan, Wiranto, Girard, Watts & Ashourvan\n\nPLOS Digital Health\n\n\nAbstract\nCitation\nBibTeX\nPreprint\n\n\nCopy\nCopied!\n—\n\n\n\nTraditional clinical assessments often lack individualization, relying on standardized procedures that may not accommodate the diverse needs of patients, especially in early stages where personalized diagnosis could offer significant benefits. We aim to provide a machine-learning framework that addresses the individualized feature addition problem and enhances diagnostic accuracy for clinical assessments.Individualized Clinical Assessment Recommendation System (iCARE) employs locally weighted logistic regression and Shapley Additive Explanations (SHAP) value analysis to tailor feature selection to individual patient characteristics. Evaluations were conducted on synthetic and real-world datasets, including early-stage diabetes risk prediction and heart failure clinical records from the UCI Machine Learning Repository. We compared the performance of iCARE with a Global approach using statistical analysis on accuracy and area under the ROC curve (AUC) to select the best additional features. The iCARE framework enhances predictive accuracy and AUC metrics when additional features exhibit distinct predictive capabilities, as evidenced by synthetic datasets 1--3 and the early diabetes dataset. Specifically, in synthetic dataset 1, iCARE achieved an accuracy of 0.999 and an AUC of 1.000, outperforming the Global approach with an accuracy of 0.689 and an AUC of 0.639. In the early diabetes and heart disease dataset, iCARE shows improvements of 6--12\\% in accuracy and AUC across different numbers of initial features over other feature selection methods. Conversely, in synthetic datasets 4--5 and the heart failure dataset, where features lack discernible predictive distinctions, iCARE shows no significant advantage over global approaches on accuracy and AUC metrics. iCARE provides personalized feature recommendations that enhance diagnostic accuracy in scenarios where individualized approaches are critical, improving the precision and effectiveness of medical diagnoses.\n@article{setiawan2025,\n  title = {Individualized Machine-Learning-Based Clinical Assessment Recommendation System},\n  author = {Devin Setiawan and Yumiko Wiranto and Jeffrey M. Girard and Amber Watts and Arian Ashourvan},\n  year = {2025},\n  journal = {PLOS Digital Health},\n  volume = {4},\n  number = {9},\n  pages = {e0001022},\n  doi = {10.1371/journal.pdig.0001022},\n  note = {Last visited on 12/01/2025}\n}\n\n\n\n\nNarcissism from Every Angle: An Interpersonal Analysis of Narcissism in Young Adults\n\n\nEdershile, Girard, Woods, Williams, Simms & Wright\n\nAssessment\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nThe construct of narcissism can be conceptualized very differently depending on the psychological literature. The social-personality conceptualization of narcissism often emphasizes high self-esteem as well as a range of associated maladaptive and adaptive outcomes. The clinical literature focuses on the pathological aspects of narcissism and highlights maladaptive aspects that correspond to the relationship between narcissistic grandiosity and narcissistic vulnerability. Reflecting these varying views of narcissism, many measures have become popular in the assessment of the construct, each with varying interpersonal characterizations. The current study (N\\,=\\,1,111) evaluated the interpersonal profiles captured by popular measures of narcissism and examined whether measures capture overlapping, differentiated, and/or intended interpersonal styles. Results revealed that measures of narcissism capture a wide range of interpersonal styles, from warm/dominant to submissive. However, most measures emphasize the role of interpersonal dominance in the measure content. Viewing narcissism from a three-factor structure, including narcissistic agency, antagonism, and vulnerability, helps to integrate the wide range of interpersonal styles apparent across narcissism measures. Furthermore, the level of (mal)adaptivity and general interpersonal style somewhat maps onto the literature of origin for the scales. Implications for measurement selection in the assessment of narcissism are discussed.\n@article{edershile2025,\n  title = {Narcissism from Every Angle: An Interpersonal Analysis of Narcissism in Young Adults},\n  author = {Elizabeth A. Edershile and Jeffrey M. Girard and William C. Woods and Trevor F. Williams and Leonard J. Simms and Aidan G. C. Wright},\n  year = {2025},\n  journal = {Assessment},\n  pages = {10731911251356150},\n  doi = {10.1177/10731911251356150},\n  note = {Last visited on 09/03/2025}\n}\n\n\n\n\nSeamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset\n\n\nAgrawal, Akinyemi, Alvero, Behrooz, Buffalini, Carlucci, Chen, Chen, Chen, Cheng, Chowdary, Chuang, D'Avirro, Daly, Dong, Duppenthaler, Gao, Girard, Gleize, Gomez, Gong, Govindarajan, Han, He, Hernandez, Hristov, Huang, Inaguma, Jain, Janardhan, Jia, Klaiber, Kovachev, Kumar, Li, Li, Litvin, Liu, Ma, Ma, Ma, Ma, Mantovani, Miglani, Mohan, Morency, Ng, Ng, Nguyen, Oberai, Peloquin, Pino, Popovic, Poursaeed, Prada, Rakotoarison, Ranjan, Richard, Ropers, Saleem, Sharma, Shcherbyna, Shen, Shen, Stathopoulos, Sun, Tomasello, Tran, Turkatenko, Wan, Wang, Wang, Williamson, Wood, Xiang, Yang, Yao, Zhang, Zhang, Zhang, Zheng, Zhyzheria, Zikes & Zollhoefer\n\narXiv:2506.22554 [cs.CV]\n\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nHuman communication involves a complex interplay of verbal and nonverbal signals, essential for conveying meaning and achieving interpersonal goals. To develop socially intelligent AI technologies, it is crucial to develop models that can both comprehend and generate dyadic behavioral dynamics. To this end, we introduce the Seamless Interaction Dataset, a large-scale collection of over 4,000 hours of face-to-face interaction footage from over 4,000 participants in diverse contexts. This dataset enables the development of AI technologies that understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents, telepresence experiences, and multimodal content analysis tools. We also develop a suite of models that utilize the dataset to generate dyadic motion gestures and facial expressions aligned with human speech. These models can take as input both the speech and visual behavior of their interlocutors. We present a variant with speech from an LLM model and integrations with 2D and 3D rendering methods, bringing us closer to interactive virtual agents. Additionally, we describe controllable variants of our motion models that can adapt emotional responses and expressivity levels, as well as generating more semantically-relevant gestures. Finally, we discuss methods for assessing the quality of these dyadic motion models, which are demonstrating the potential for more intuitive and responsive human-AI interactions.\n@misc{agrawal2025,\n  title = {Seamless Interaction: {{Dyadic}} Audiovisual Motion Modeling and Large-Scale Dataset},\n  author = {Vasu Agrawal and Akinniyi Akinyemi and Kathryn Alvero and Morteza Behrooz and Julia Buffalini and Fabio Maria Carlucci and Joy Chen and Junming Chen and Zhang Chen and Shiyang Cheng and Praveen Chowdary and Joe Chuang and Antony D'Avirro and Jon Daly and Ning Dong and Mark Duppenthaler and Cynthia Gao and Jeff Girard and Martin Gleize and Sahir Gomez and Hongyu Gong and Srivathsan Govindarajan and Brandon Han and Sen He and Denise Hernandez and Yordan Hristov and Rongjie Huang and Hirofumi Inaguma and Somya Jain and Raj Janardhan and Qingyao Jia and Christopher Klaiber and Dejan Kovachev and Moneish Kumar and Hang Li and Yilei Li and Pavel Litvin and Wei Liu and Guangyao Ma and Jing Ma and Martin Ma and Xutai Ma and Lucas Mantovani and Sagar Miglani and Sreyas Mohan and Louis-Philippe Morency and Evonne Ng and Kam-Woh Ng and Tu Anh Nguyen and Amia Oberai and Benjamin Peloquin and Juan Pino and Jovan Popovic and Omid Poursaeed and Fabian Prada and Alice Rakotoarison and Rakesh Ranjan and Alexander Richard and Christophe Ropers and Safiyyah Saleem and Vasu Sharma and Alex Shcherbyna and Jia Shen and Jie Shen and Anastasis Stathopoulos and Anna Sun and Paden Tomasello and Tuan Tran and Arina Turkatenko and Bo Wan and Chao Wang and Jeff Wang and Mary Williamson and Carleigh Wood and Tao Xiang and Yilin Yang and Julien Yao and Chen Zhang and Jiemin Zhang and Xinyue Zhang and Jason Zheng and Pavlo Zhyzheria and Jan Zikes and Michael Zollhoefer},\n  year = {2025},\n  publisher = {arXiv:2506.22554 [cs.CV]},\n  doi = {10.48550/arXiv.2506.22554}\n}\n\n\n\n\nThe Effects of Alcohol in Groups of Heavy-Drinking Young Adults: A Multimodal Investigation of Alcohol Responses in a Laboratory Social Setting\n\n\nCreswell, Wright, Sayette, Girard, Lyons & Smyth\n\nClinical Psychological Science\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nYoung adults typically drink socially, yet most lab studies testing alcohol responses have administered alcohol in isolation. This is the first study to examine alcohol responses and social reward in a group setting among a young-adult at-risk sample. Heavy-drinking young adults (N = 393; 50\\% female) were grouped in threes and drank a moderate dose of alcohol or a placebo beverage. These social interactions were recorded, and the duration and sequence of facial expressions, speech, and laughter were coded. Results revealed a comprehensive, multimodal, positive effect of alcohol on socioemotional experiences across self-report (e.g., increased positive affect and social bonding, greater relief of unpleasant feelings) and behavioral outcomes at both the individual (e.g., more rapid increases in Duchenne smiling) and group levels (e.g., more three-way conversations). Findings underscore the potential for group-formation paradigms to yield valuable data regarding etiological mechanisms underlying alcohol use disorder. All data and code are available (https://osf.io/3q42z/).\n@article{creswell2025,\n  title = {The Effects of Alcohol in Groups of Heavy-Drinking Young Adults: A Multimodal Investigation of Alcohol Responses in a Laboratory Social Setting},\n  author = {Kasey G. Creswell and Aidan G. C. Wright and Michael A. Sayette and Jeffrey M. Girard and Greta Lyons and Joshua M. Smyth},\n  year = {2025},\n  journal = {Clinical Psychological Science},\n  pages = {21677026251333784},\n  doi = {10.1177/21677026251333784},\n  note = {Last visited on 09/03/2025}\n}\n\n\n\n\nThe Role of Hyper-Palatable Foods in Energy Intake Measured Using Mobile Food Photography Methodology\n\n\nJun, Girard, Martin & Fazzino\n\nEating Behaviors\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nObjective Hyper-palatable foods (HPF) contain nutrient combinations that are hypothesized to maximize their rewarding effects during consumption. Due to their strong reinforcing properties, HPF are hypothesized to lead to greater energy intake within a meal. However, this premise has not been tested in free-living conditions. The current study examined the association between within-meal HPF intake and 1) measured energy intake and 2) self-reported overeating, assessed within eating occasions using smartphone-based food photography methodology. Methods A total of 29 participants reported food intake and eating experiences (N=345 total eating occasions) in real-time for 4~days using smartphone-based food photography methodology. HPF were identified using a standardized definition. Bayesian multilevel modeling was conducted to investigate the within-person effects of proportional calorie intake from HPF (\\%kcal from HPF) on total energy intake and subjective overeating. Pre-meal hunger and proportional energy intake from high energy dense (HED) foods were included as covariates. Results Results revealed that when participants consumed more \\%kcal from HPF than their average, they consumed greater total energy during eating occasions, even when controlling for pre-meal hunger and \\%kcal from HED foods (median {$\\beta~$}=~0.09, 95\\% HDI [0.02, 0.16], pd.~=~99.56\\%). Additionally, consuming more \\%kcal from HPF than average was associated with greater eating despite feeling full, when controlling covariates (median {$\\beta~$}=~0.15, 95\\% HDI [-0.02, 0.34], pd~=~96.45\\%). Conclusions The findings supported the premise that HPF themselves may yield greater energy intake and eating despite satiation, measured in real-time and free-living conditions.\n@article{jun2025,\n  title = {The Role of Hyper-Palatable Foods in Energy Intake Measured Using Mobile Food Photography Methodology},\n  author = {Daiil Jun and Jeffrey M. Girard and Corby K. Martin and Tera L. Fazzino},\n  year = {2025},\n  journal = {Eating Behaviors},\n  volume = {57},\n  pages = {101983},\n  doi = {10.1016/j.eatbeh.2025.101983},\n  note = {Last visited on 09/03/2025}\n}\n\n\n\n\nTransdiagnostic Modeling of Clinician-Rated Symptoms in Affective and Nonaffective Psychotic Disorders\n\n\nChung, Girard, Ravichandran, Öngür, Cohen & Baker\n\nJournal of Psychopathology and Clinical Science\n\n\n\n★\n This project was led by the AffCom lab\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nPrevailing factor models of psychosis are centered on schizophrenia-related disorders defined by the Diagnostic and Statistical Manual of Mental Disorders and International Classification of Diseases, restricting generalizability to other clinical presentations featuring psychosis, even though affective psychoses are more common. This study aims to bridge this gap by conducting exploratory and confirmatory factor analyses, utilizing clinical ratings collected from patients with either affective or nonaffective psychoses ( n = 1,042). Drawing from established clinical instruments, such as the Positive and Negative Syndrome Scale, Young Mania Rating Scale, and Montgomery-\\AA sberg Depression Rating Scale, a broad spectrum of core psychotic symptoms was considered for the model development. Among the candidate models considered, including correlated factors and multifactor models, a model with seven correlated factors encompassing positive symptoms, negative symptoms, depression, mania, disorganization, hostility, and anxiety was most interpretable with acceptable fit. The seven factors exhibited expected associations with external validators, were replicable through cross-validation, and were generalizable across affective and nonaffective psychoses. (PsycInfo Database Record (c) 2024 APA, all rights reserved) (Source: journal abstract) The aim of this study is to formulate a transdiagnostic dimensional model by integrating well-established clinical ratings that encompass a wide range of core symptoms observed in both affective and nonaffective psychoses. We demonstrate that a multidimensional symptom model representing both psychotic (positive symptoms, negative symptoms, and disorganization) and mood symptoms (depression, mania, hostility, and anxiety) is most interpretable and applicable across psychotic diagnoses. Considering the complex interrelationships among identified symptom dimensions, a dimensional approach may be more suitable for characterizing the symptom profiles of psychotic patients than a categorical diagnostic approach. (PsycInfo Database Record (c) 2024 APA, all rights reserved)\n@article{chung2025,\n  title = {Transdiagnostic Modeling of Clinician-Rated Symptoms in Affective and Nonaffective Psychotic Disorders},\n  author = {Yoonho Chung and Jeffrey M. Girard and Caitlin Ravichandran and Dost {\\\"O}ng{\\\"u}r and Bruce M. Cohen and Justin T. Baker},\n  year = {2025},\n  journal = {Journal of Psychopathology and Clinical Science},\n  volume = {134},\n  number = {1},\n  pages = {81--96},\n  doi = {10/g8pwmn},\n  note = {Last visited on 10/31/2024}\n}\n\n\n\n\nWorth the Weight: An Examination of Unstructured and Structured Data in Graduate Admissions\n\n\nAdaryukov, Biernat, Girard, Villicana & Pleskac\n\nDecision\n\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nIn graduate admissions, as in many multiattribute decisions, evaluators must judge candidates from a flood of information, including recommendation letters, personal statements, grades, and standardized test scores. Some of this information is structured, while some is unstructured. Yet most studies of multiattribute decisions focus on decisions made from structured information. This study evaluated how structured and unstructured information is used within graduate admissions decisions. We examined a uniquely comprehensive data set of N = 2,231 graduate applications to the University of Kansas, containing full application packages, demographics, and final admissions decisions for each applicant. To make sense of our documents, we applied structural topic modeling (STM), a topic model that allows topic content and prevalence to covary based on other metadata (e.g., department of study). STM allowed us to examine what information the letters and statements contain and the relationships between variables like gender and race and the textual information. We found that most topics in the unstructured data related to specific fields of study. The STMs did not uncover strong differences among applicants regarding race and gender, though the recommendation letters and personal statements for international applicants did show some different topic profiles than domestic applicants. We also found that admissions decision makers behaved as if they prioritized structured numeric metrics, using unstructured information to check for disqualifications, if at all. However, we found that topics were less reliable than admissions documents, meaning that additional ways of using them cannot be completely ruled out. The implications of our findings on graduate admissions decisions are discussed. (PsycInfo Database Record (c) 2025 APA, all rights reserved)\n@article{adaryukov2025,\n  title = {Worth the Weight: An Examination of Unstructured and Structured Data in Graduate Admissions},\n  author = {James Adaryukov and Monica Biernat and Jeffrey M. Girard and Adrian J. Villicana and Timothy J. Pleskac},\n  year = {2025},\n  journal = {Decision},\n  volume = {12},\n  number = {1},\n  pages = {4--30},\n  doi = {10.1037/dec0000251}\n}\n\n\n\n\n\n2024\n\n\n\nAre Within- and between-Session Changes in Distress Associated with Treatment Outcomes? Findings from Two Clinical Trials of Exposure for Eating Disorders\n\n\nButler, Christian, Girard, Vanzhula & Levinson\n\nBehaviour Research and Therapy\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nObjective Imaginal exposure is a novel intervention for eating disorders (EDs) that has been investigated as a method for targeting ED symptoms and fears. Research is needed to understand mechanisms of change during imaginal exposure for EDs, including whether within- and between-session distress reduction is related to treatment outcomes. Method Study 1 tested four sessions of online imaginal exposure (N~=~143). Study 2 examined combined imaginal and in vivo exposure, comprising six imaginal exposure sessions (N~=~26). ED symptoms and fears were assessed pre- and posttreatment, and subjective distress and state anxiety were collected during sessions. Results Subjective distress tended to increase within-session in both studies, and within-session reduction was not associated with change in ED symptoms or fears. In Study 1, between-session reduction of distress and state anxiety was associated with greater decreases in ED symptoms and fears pre-to posttreatment. In Study 2, between-session distress reduction occurred but was not related to outcomes. Conclusions Within-session distress reduction may not promote change during exposure for EDs, whereas between-session distress reduction may be associated with better treatment outcomes. These findings corroborate research on distress reduction during exposure for anxiety disorders. Clinicians might consider approaches to exposure-based treatment that focus on distress tolerance and promote between-session distress reduction.\n@article{butler2024,\n  title = {Are Within- and between-Session Changes in Distress Associated with Treatment Outcomes? {{Findings}} from Two Clinical Trials of Exposure for Eating Disorders},\n  author = {Rachel M. Butler and Caroline Christian and Jeffrey M. Girard and Irina A. Vanzhula and Cheri A. Levinson},\n  year = {2024},\n  journal = {Behaviour Research and Therapy},\n  volume = {180},\n  pages = {104577},\n  doi = {10.1016/j.brat.2024.104577},\n  note = {Last visited on 09/03/2025}\n}\n\n\n\n\nAssociations between Transdiagnostic Traits of Psychopathology and Hybrid Posttraumatic Stress Disorder Factors in a Trauma-Exposed Community Sample\n\n\nSprunger, Girard & Chard\n\nJournal of Traumatic Stress\n\n\n\n★\n This project was led by the AffCom lab\n\nAbstract\nCitation\nBibTeX\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nDimensional conceptualizations of psychopathology hold promise for understanding the high rates of comorbidity with posttraumatic stress disorder (PTSD). Linking PTSD symptoms to transdiagnostic dimensions of psychopathology may enable researchers and clinicians to understand the patterns and breadth of behavioral sequelae following traumatic experiences that may be shared with other psychiatric disorders. To explore this premise, we recruited a trauma-exposed online community sample (N = 462) and measured dimensional transdiagnostic traits of psychopathology using parceled facets derived from the Personality Inventory for DSM-5 Faceted--Short Form. PTSD symptom factors were measured using the PTSD Checklist for DSM-5 and derived using confirmatory factor analysis according to the seven-factor hybrid model (i.e., Intrusions, Avoidance, Negative Affect, Anhedonia, Externalizing Behaviors, Anxious Arousal, And Dysphoric Arousal). We observed hypothesized associations between PTSD factors and transdiagnostic traits indicating that some transdiagnostic dimensions were associated with nearly all PTSD symptom factors (e.g., emotional lability: rmean = .35), whereas others showed more unique relationships (e.g., hostility--Externalizing Behavior: r = .60; hostility with other PTSD factors: rs = .12--.31). All PTSD factors were correlated with traits beyond those that would appear to be construct-relevant, suggesting the possibility of indirect associations that should be explicated in future research. The results indicate the breadth of trait-like consequences associated with PTSD symptom exacerbation, with implications for case conceptualization and treatment planning. Although PTSD is not a personality disorder, the findings indicate that increased PTSD factor severity is moderately associated with different patterns of trait-like disruptions in many areas of functioning.\n@article{sprunger2024,\n  title = {Associations between Transdiagnostic Traits of Psychopathology and Hybrid Posttraumatic Stress Disorder Factors in a Trauma-Exposed Community Sample},\n  author = {Joel G. Sprunger and Jeffrey M. Girard and Kathleen M. Chard},\n  year = {2024},\n  journal = {Journal of Traumatic Stress},\n  volume = {37},\n  number = {3},\n  pages = {384--396},\n  doi = {10.1002/jts.23023},\n  note = {Last visited on 03/02/2024}\n}\n\n\n\n\nGeSTICS: A Multimodal Corpus for Studying Gesture Synthesis in Two-Party Interactions with Contextualized Speech\n\n\nKebe, Birlikci, Boudin, Ishii, Girard & Morency\n\nProceedings of the 24th ACM International Conference on Intelligent Virtual Agents\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nGenerating natural co-speech gestures and facial expressions for effective human-agent interactions requires modeling the intricate interplay between verbal, non-verbal, and contextual cues observed in dyadic human communication. Two types of contextual cues are of particular interest: (1) individual factors of the interlocutors, such as their demographic attributes, and (2) situational factors, like the outcome of a preceding event. To facilitate their study, we introduce the GeSTICS Dataset, a novel multimodal corpus comprising 9,853 questions and 10,460 answers from audiovisual recordings of post-game sports interviews by 147 interviewees. The dataset contains speech data, including textual transcriptions, lexical descriptors, and acoustic features, as well as visual data encompassing the interviewee's body pose and facial expressions, with an emphasis on capturing these modalities during both the question-listening and answering phases of the interview. Furthermore, GeSTICS incorporates metadata about individual factors, such as the age and cultural background of the interviewees, and situational factors, like the results of the games, which are often overlooked in existing multimodal datasets. Our preliminary analysis of GeSTICS reveals that the effects of speech features, such as loudness and lexical choice, on the production of co-speech gestures in both speaking and listening phases are moderated by situational factors and the interviewee's individual factors. GeSTICS is designed to enhance the generation of realistic nonverbal behaviors in virtual agents, animated characters, and human-robot interaction systems, thus contributing to more engaging and effective human-agent communication. The analysis code and the dataset are available at https://gestics.github.io.\n@inproceedings{kebe2024,\n  title = {{{GeSTICS}}: A Multimodal Corpus for Studying Gesture Synthesis in Two-Party Interactions with Contextualized Speech},\n  booktitle = {Proceedings of the 24th {{ACM International Conference}} on {{Intelligent Virtual Agents}}},\n  author = {Gaoussou Youssouf Kebe and Mehmet Deniz Birlikci and Auriane Boudin and Ryo Ishii and Jeffrey M. Girard and Louis-Philippe Morency},\n  year = {2024},\n  month = {sep},\n  pages = {1--10},\n  publisher = {ACM},\n  address = {GLASGOW United Kingdom},\n  doi = {10.1145/3652988.3673917},\n  note = {Last visited on 07/02/2025}\n}\n\n\n\n\nIt's the Sentiment That Counts: Comparing Sentiment Analysis Tools for Estimating Affective Valence in Dream Reports\n\n\nBaber, Hamilton, Girard, Cohen, Gratton, Ellis & Hemmer\n\nSleep\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nNA\n@article{baber2024,\n  title = {It's the Sentiment That Counts: Comparing Sentiment Analysis Tools for Estimating Affective Valence in Dream Reports},\n  author = {Garrett R Baber and Nancy A Hamilton and Jeffrey M Girard and Jamie M Cohen and Matthew K P Gratton and Samantha Ellis and Eliza Hemmer},\n  year = {2024},\n  journal = {Sleep},\n  volume = {47},\n  number = {12},\n  pages = {zsae210},\n  doi = {10.1093/sleep/zsae210},\n  note = {Last visited on 09/20/2024}\n}\n\n\n\n\nSources of Environmental Reinforcement and Engagement in Health Risk Behaviors among a General Population Sample of US Adults\n\n\nL'Insalata, Girard & Fazzino\n\nInternational Journal of Environmental Research and Public Health\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nResearch supports the premise that greater substance use is associated with fewer sources of environmental reinforcement. However, it remains unclear whether types of environmental reinforcement (e.g., social or work) may differentially influence use. This study tested the association between types of environmental reinforcement and engagement in multiple health risk behaviors (alcohol use, binge eating, and nicotine use). Cross-sectional data were collected from a general population sample of US adults (N = 596). The Pleasant Events Schedule (PES) was used to measure sources of reinforcement. Exploratory structural equation modeling (ESEM) characterized different areas of environmental reinforcement and correlations with alcohol consumption, binge eating, and nicotine use. A four-factor structure of the PES demonstrated a conceptually cohesive model with acceptable fit and partial strict invariance. Social-related reinforcement was positively associated with alcohol consumption ({$\\beta$} = 0.30, p {$&lt;$} 0.001) and binge eating ({$\\beta$} = 0.26, p {$&lt;$} 0.001). Work/school-related reinforcement was negatively associated with binge eating ({$\\beta$} = -0.14, p = 0.006). No areas of reinforcement were significantly associated with nicotine use (p values = 0.069 to 0.755). Social-related activities may be associated with engagement in multiple health risk behaviors (more binge eating and alcohol use), whereas work/school-related activities may be preventative against binge eating. Understanding these relationships can inform prevention efforts targeting health risk behaviors.\n@article{linsalata2024,\n  title = {Sources of Environmental Reinforcement and Engagement in Health Risk Behaviors among a General Population Sample of {{US}} Adults},\n  author = {Alexa M. L'Insalata and Jeffrey M. Girard and Tera L. Fazzino},\n  year = {2024},\n  month = {nov},\n  journal = {International Journal of Environmental Research and Public Health},\n  volume = {21},\n  number = {11},\n  pages = {1390},\n  doi = {10.3390/ijerph21111390},\n  note = {Last visited on 09/03/2025}\n}\n\n\n\n\n\n2023\n\n\n\nDynAMoS: The Dynamic Affective Movie Clip Database for Subjectivity Analysis\n\n\nGirard, Tie & Liebenthal\n\nProceedings of the 11th International Conference on Affective Computing and Intelligent Interaction (ACII)\n\n\n\n★\n This project was led by the AffCom lab\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nIn this paper, we describe the design, collection, and validation of a new video database that includes holistic and dynamic emotion ratings from 83 participants watching 22 affective movie clips. In contrast to previous work in Affective Computing, which pursued a single ``ground truth'' label for the affective content of each moment of each video (e.g., by averaging the ratings of 2 to 7 trained participants), we embrace the subjectivity inherent to emotional experiences and provide the full distribution of all participants' ratings (with an average of 76.7 raters per video). We argue that this choice represents a paradigm shift with the potential to unlock new research directions, generate new hypotheses, and inspire novel methods in the Affective Computing community. We also describe several interdisciplinary use cases for the database: to provide dynamic norms for emotion elicitation studies (e.g., in psychology, medicine, and neuroscience), to train and test affective content analysis algorithms (e.g., for dynamic emotion recognition, video summarization, and movie recommendation), and to study subjectivity in emotional reactions (e.g., to identify moments of emotional ambiguity or ambivalence within movies, identify predictors of subjectivity, and develop personalized affective content analysis algorithms). The database is made freely available to researchers for noncommercial use at https://dynamos.mgb.org.\n@inproceedings{girard2023,\n  title = {{{DynAMoS}}: The Dynamic Affective Movie Clip Database for Subjectivity Analysis},\n  booktitle = {Proceedings of the 11th {{International Conference}} on {{Affective Computing}} and {{Intelligent Interaction}} ({{ACII}})},\n  author = {Jeffrey M. Girard and Yanmei Tie and Einat Liebenthal},\n  year = {2023},\n  pages = {1--8},\n  address = {Cambridge, MA},\n  doi = {10.1109/ACII59096.2023.10388135},\n  note = {Last visited on 01/25/2024}\n}\n\n\n\n\nHuman and Machine Recognition of Dynamic and Static Facial Expressions: Prototypicality, Ambiguity, and Complexity\n\n\nKim, Küster, Girard & Krumhuber\n\nFrontiers in Psychology\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nA growing body of research suggests that movement aids facial expression recognition. However, less is known about the conditions under which the dynamic advantage occurs. The aim of this research was to test emotion recognition in static and dynamic facial expressions, thereby exploring the role of three featural parameters (prototypicality, ambiguity and complexity) in human and machine analysis. In two studies, facial expression videos and corresponding images depicting the peak of the target and non-target emotion were presented to human observers and the machine classifier (FACET). Results revealed higher recognition rates for dynamic stimuli compared to non-target images. Such benefit disappeared in the context of target-emotion images which were similarly well (or even better) recognised than videos, and more prototypical, less ambiguous, and more complex in appearance than non-target images. While prototypicality and ambiguity exerted more predictive power in machine performance, complexity was more indicative of human emotion recognition. Interestingly, recognition performance by the machine was found to be superior to humans for both target and non-target images. Together, the findings point towards a compensatory role of dynamic information, particularly when static-based stimuli lack relevant features of the target emotion. Implications for research using automatic facial expression analysis (AFEA) are discussed.\n@article{kim2023,\n  title = {Human and Machine Recognition of Dynamic and Static Facial Expressions: Prototypicality, Ambiguity, and Complexity},\n  author = {Hyunwoo Kim and Dennis K{\\\"u}ster and Jeffrey M. Girard and Eva G. Krumhuber},\n  year = {2023},\n  journal = {Frontiers in Psychology},\n  volume = {14},\n  doi = {10.3389/fpsyg.2023.1221081},\n  note = {Last visited on 09/03/2025}\n}\n\n\n\n\nRandomized Trial of Brief Interpersonal Psychotherapy and Cognitive Behavioral Therapy for Depression Delivered Both In-Person and by Telehealth\n\n\nSwartz, Bylsma, Fournier, Girard, Spotts, Cohn & Morency\n\nJournal of Affective Disorders\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nBackground Expert consensus guidelines recommend Cognitive Behavioral Therapy (CBT) and Interpersonal Psychotherapy (IPT), interventions that were historically delivered face-to-face, as first-line treatments for Major Depressive Disorder (MDD). Despite the ubiquity of telehealth following the COVID-19 pandemic, little is known about differential outcomes with CBT versus IPT delivered in-person (IP) or via telehealth (TH) or whether working alliance is affected. Methods Adults meeting DSM-5 criteria for MDD were randomly assigned to either 8 sessions of IPT or CBT (group). Mid-trial, COVID-19 forced a change of therapy delivery from IP to TH (study phase). We compared changes in Hamilton Rating Scale for Depression (HRSD-17) and Working Alliance Inventory (WAI) scores for individuals by group and phase: CBT-IP (n = 24), CBT-TH (n = 11), IPT-IP (n = 25) and IPT-TH (n = 17). Results HRSD-17 scores declined significantly from pre to post treatment (pre: M = 17.7\n@article{swartz2023,\n  title = {Randomized Trial of Brief Interpersonal Psychotherapy and Cognitive Behavioral Therapy for Depression Delivered Both In-Person and by Telehealth},\n  author = {Holly A Swartz and Lauren M Bylsma and Jay C Fournier and Jeffrey M Girard and Crystal Spotts and Jeffrey F Cohn and Louis-Philippe Morency},\n  year = {2023},\n  journal = {Journal of Affective Disorders},\n  volume = {333},\n  pages = {543--552},\n  doi = {10.1016/j.jad.2023.04.092}\n}\n\n\n\n\nRepresentation Learning for Interpersonal and Multimodal Behavior Dynamics: A Multiview Extension of Latent Change Score Models\n\n\nVail, Girard, Bylsma, Fournier, Swartz, Cohn & Morency\n\nProceedings of the 25th International Conference on Multimodal Interaction\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nCharacterizing the dynamics of behavior across multiple modalities and individuals is a vital component of computational behavior analysis. This is especially important in certain applications, such as psychotherapy, where individualized tracking of behavior patterns can provide valuable information about the patient's mental state. Conventional methods that rely on aggregate statistics and correlational metrics may not always suffice, as they are often unable to capture causal relationships or evaluate the true probability of identified patterns. To address these challenges, we present a novel approach to learning multimodal and interpersonal representations of behavior dynamics during one-on-one interaction. Our approach is enabled by the introduction of a multiview extension of latent change score models, which facilitates the concurrent capture of both inter-modal and interpersonal behavior dynamics and the identification of directional relationships between them. A core advantage of our approach is its high level of interpretability while simultaneously achieving strong predictive performance. We evaluate our approach within the domain of therapist-client interactions, with the objective of gaining a deeper understanding about the collaborative relationship between the two, a crucial element of the therapeutic process. Our results demonstrate improved performance over conventional approaches that rely upon summary statistics or correlational metrics. Furthermore, since our multiview approach includes the explicit modeling of uncertainty, it naturally lends itself to integration with probabilistic classifiers, such as Gaussian process models. We demonstrate that this integration leads to even further improved performance, all the while maintaining highly interpretable qualities. Our analysis provides compelling motivation for further exploration of stochastic systems within computational models of behavior.\n@inproceedings{vail2023,\n  title = {Representation Learning for Interpersonal and Multimodal Behavior Dynamics: A Multiview Extension of Latent Change Score Models},\n  booktitle = {Proceedings of the 25th {{International Conference}} on {{Multimodal Interaction}}},\n  author = {Alexandria K. Vail and Jeffrey M. Girard and Lauren M. Bylsma and Jay Fournier and Holly A. Swartz and Jeffrey F. Cohn and Louis-Philippe Morency},\n  year = {2023},\n  series = {{{ICMI}} '23},\n  pages = {517--526},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3577190.3614118},\n  note = {Last visited on 09/02/2025}\n}\n\n\n\n\n\n2022\n\n\n\nComputational Analysis of Spoken Language in Acute Psychosis and Mania\n\n\nGirard, Vail, Liebenthal, Brown, Kilciksiz, Pennant, Liebson, Öngür, Morency & Baker\n\nSchizophrenia Research\n\n\n\n★\n This project was led by the AffCom lab\n\nAbstract\nCitation\nBibTeX\nPreprint\n\n\nCopy\nCopied!\n—\n\n\n\nObjectives This study aimed to (1) determine the feasibility of collecting behavioral data from participants hospitalized with acute psychosis and (2) begin to evaluate the clinical information that can be computationally derived from such data. Methods Behavioral data was collected across 99 sessions from 38 participants recruited from an inpatient psychiatric unit. Each session started with a semi-structured interview modeled on a typical ``clinical rounds'' encounter and included administration of the Positive and Negative Syndrome Scale (PANSS). Analysis We quantified aspects of participants' verbal behavior during the interview using lexical, coherence, and disfluency features. We then used two complementary approaches to explore our second objective. The first approach used predictive models to estimate participants' PANSS scores from their language features. Our second approach used inferential models to quantify the relationships between individual language features and symptom measures. Results Our predictive models showed promise but lacked sufficient data to achieve clinically useful accuracy. Our inferential models identified statistically significant relationships between numerous language features and symptom domains. Conclusion Our interview recording procedures were well-tolerated and produced adequate data for transcription and analysis. The results of our inferential modeling suggest that automatic measurements of expressive language contain signals highly relevant to the assessment of psychosis. These findings establish the potential of measuring language during a clinical interview in a naturalistic setting and generate specific hypotheses that can be tested in future studies. This, in turn, will lead to more accurate modeling and better understanding of the relationships between expressive language and psychosis.\n@article{girard2022,\n  title = {Computational Analysis of Spoken Language in Acute Psychosis and Mania},\n  author = {Jeffrey M. Girard and Alexandria K. Vail and Einat Liebenthal and Katrina Brown and Can Misel Kilciksiz and Luciana Pennant and Elizabeth Liebson and Dost {\\\"O}ng{\\\"u}r and Louis-Philippe Morency and Justin T. Baker},\n  year = {2022},\n  journal = {Schizophrenia Research},\n  volume = {245},\n  pages = {97--115},\n  doi = {10.1016/j.schres.2021.06.040},\n  note = {Last visited on 02/20/2022}\n}\n\n\n\n\nToward Causal Understanding of Therapist-Client Relationships: A Study of Language Modality and Social Entrainment\n\n\nVail, Girard, Bylsma, Cohn, Fournier, Swartz & Morency\n\nProceedings of the 24th ACM International Conference on Multimodal Interaction\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nThe relationship between a therapist and their client is one of the most critical determinants of successful therapy. The working alliance is a multifaceted concept capturing the collaborative aspect of the therapist-client relationship; a strong working alliance has been extensively linked to many positive therapeutic outcomes. Although therapy sessions are decidedly multimodal interactions, the language modality is of particular interest given its recognized relationship to similar dyadic concepts such as rapport, cooperation, and affiliation. Specifically, in this work we study language entrainment, which measures how much the therapist and client adapt toward each other's use of language over time. Despite the growing body of work in this area, however, relatively few studies examine causal relationships between human behavior and these relationship metrics: does an individual's perception of their partner affect how they speak, or does how they speak affect their perception? We explore these questions in this work through the use of structural equation modeling (SEM) techniques, which allow for both multilevel and temporal modeling of the relationship between the quality of the therapist-client working alliance and the participants' language entrainment. In our first experiment, we demonstrate that these techniques perform well in comparison to other common machine learning models, with the added benefits of interpretability and causal analysis. In our second analysis, we interpret the learned models to examine the relationship between working alliance and language entrainment and address our exploratory research questions. The results reveal that a therapist's language entrainment can have a significant impact on the client's perception of the working alliance, and that the client's language entrainment is a strong indicator of their perception of the working alliance. We discuss the implications of these results and consider several directions for future work in multimodality.\n@inproceedings{vail2022,\n  title = {Toward Causal Understanding of Therapist-Client Relationships: A Study of Language Modality and Social Entrainment},\n  booktitle = {Proceedings of the 24th {{ACM International Conference}} on {{Multimodal Interaction}}},\n  author = {Alexandria K Vail and Jeffrey M Girard and Lauren M Bylsma and Jeffrey F Cohn and Jay Fournier and Holly A Swartz and Louis-Philippe Morency},\n  year = {2022},\n  pages = {487--494},\n  address = {Bengaluru, India},\n  doi = {10/gt3bwj}\n}\n\n\n\n\nWeighting Schemes and Incomplete Data: A Generalized Bayesian Framework for Chance-Corrected Interrater Agreement\n\n\nvan Oest & Girard\n\nPsychological Methods\n\n\nAbstract\nCitation\nBibTeX\nPreprint\n\n\nCopy\nCopied!\n—\n\n\n\nVan Oest (2019) developed a framework to assess interrater agreement for nominal categories and complete data. We generalize this framework to all four situations of nominal or ordinal categories and complete or incomplete data. The mathematical solution yields a chance-corrected agreement coefficient that accommodates any weighting scheme for penalizing rater disagreements and any number of raters and categories. By incorporating Bayesian estimates of the category proportions, the generalized coefficient also captures situations in which raters classify only subsets of items; that is, incomplete data. Furthermore, this coefficient encompasses existing chance-corrected agreement coefficients: the S-coefficient, Scott's pi, Fleiss' kappa, and Van Oest's uniform prior coefficient, all augmented with a weighting scheme and the option of incomplete data. We use simulation to compare these nested coefficients. The uniform prior coefficient tends to perform best, in particular, if one category has a much larger proportion than others. The gap with Scott's pi and Fleiss' kappa widens if the weighting scheme becomes more lenient to small disagreements and often if more item classifications are missing; missingness biases play a moderating role. The uniform prior coefficient often performs much better than the S-coefficient, but the S-coefficient sometimes performs best for small samples, missing data, and lenient weighting schemes. The generalized framework implies a new interpretation of chance-corrected weighted agreement coefficients: These coefficients estimate the probability that both raters in a pair assign an item to its correct category without guessing. Whereas Van Oest showed this interpretation for unweighted agreement, we generalize to weighted agreement. (PsycInfo Database Record (c) 2021 APA, all rights reserved) (Source: journal abstract) {$&lt;$}strong xmlns:lang=\"en\"{$&gt;$}Translational Abstract---Many studies and assessments require classification of subjective items (e.g., text) into categories (e.g., based on content). To assess whether the results are reproducible, it is good practice to let two or more raters independently classify the items, compute the proportion of pairwise rater agreement, and adjust for agreement expected by chance. Most chance-corrected agreement coefficients assume nominal categories and include only full agreements in which raters choose the same category. However, many situations (e.g., point scales) imply ordinal categories, where raters may receive partial credit for disagreements, based on the distance of their chosen categories and captured by a weighting scheme. Furthermore, raters often classify only subsets of items, where the missing data occur either by accident or by design. The present study develops a framework to estimate chance-corrected agreement for all four combinations of nominal or ordinal categories and complete or incomplete data. The resulting coefficient requires only a few lines of programming code and captures several existing coefficients via different values of its input parameters; it augments all nested coefficients with a weighting scheme and the option of missing item classifications. We use simulation to compare the coefficient performances for different weighting schemes, missing data mechanisms, and category proportions: The so-called uniform prior coefficient often (but not always) performs best. Furthermore, our framework implies that chance-corrected agreement coefficients, both unweighted and weighted, estimate the probability that both raters in a pair assign an item to its correct category without guessing. (PsycInfo Database Record (c) 2021 APA, all rights reserved)\n@article{vanoest2022,\n  title = {Weighting Schemes and Incomplete Data: A Generalized {{Bayesian}} Framework for Chance-Corrected Interrater Agreement},\n  author = {Rutger {van Oest} and Jeffrey M. Girard},\n  year = {2022},\n  journal = {Psychological Methods},\n  volume = {27},\n  number = {6},\n  pages = {1069--1088},\n  doi = {10.1037/met0000412},\n  note = {Last visited on 11/16/2021}\n}\n\n\n\n\n\n2021\n\n\n\nA Bayesian Multilevel Analysis of the Longitudinal Associations between Relationship Quality and Suicidal Ideation and Attempts among Youth with Bipolar Disorder\n\n\nSewall, Girard, Merranko, Hafeman, Goldstein, Strober, Hower, Weinstock, Yen, Ryan, Keller, Liao, Diler, Gill, Axelson, Birmaher & Goldstein\n\nThe Journal of Child Psychology and Psychiatry\n\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nBackground Youth with bipolar disorder (BD) are at high risk for suicidal thoughts and behaviors and frequently experience interpersonal impairment, which is a risk factor for suicide. Yet, no study to date has examined the longitudinal associations between relationship quality in family/peer domains and suicidal thoughts and behaviors among youth with BD. Thus, we investigated how between-person differences -- reflecting the average relationship quality across time -- and within-person changes, reflecting recent fluctuations in relationship quality, act as distal and/or proximal risk factors for suicidal ideation (SI) and suicide attempts. Methods We used longitudinal data from the Course and Outcome of Bipolar Youth Study (N = 413). Relationship quality variables were decomposed into stable (i.e., average) and varying (i.e., recent) components and entered, along with major clinical covariates, into separate Bayesian multilevel models predicting SI and suicide attempt. We also examined how the relationship quality effects interacted with age and sex. Results Poorer average relationship quality with parents ({$\\beta$} = -.33, 95\\% Bayesian highest density interval (HDI) [-0.54, -0.11]) or friends ({$\\beta$} = -.33, 95\\% HDI [-0.55, -0.11]) was longitudinally associated with increased risk of SI but not suicide attempt. Worsening recent relationship quality with parents ({$\\beta$} = -.10, 95\\% HDI [-0.19, -0.03]) and, to a lesser extent, friends ({$\\beta$} = -.06, 95\\% HDI [-0.15, 0.03]) was longitudinally associated with increased risk of SI, but only worsening recent relationship quality with parents was also associated with increased risk of suicide attempt ({$\\beta$} = -.15, 95\\% HDI [-0.31, 0.01]). The effects of certain relationship quality variables were moderated by gender but not age. Conclusions Among youth with BD, having poorer average relationship quality with peers and/or parents represents a distal risk factor for SI but not suicide attempts. Additionally, worsening recent relationship quality with parents may be a time-sensitive indicator of increased risk for SI or suicide attempt.\n@article{sewall2021,\n  title = {A {{Bayesian}} Multilevel Analysis of the Longitudinal Associations between Relationship Quality and Suicidal Ideation and Attempts among Youth with Bipolar Disorder},\n  author = {Craig J. R. Sewall and Jeffrey M. Girard and John Merranko and Danella Hafeman and Benjamin I. Goldstein and Michael Strober and Heather Hower and Lauren M. Weinstock and Shirley Yen and Neal D. Ryan and Martin B. Keller and Fangzi Liao and Rasim S. Diler and Mary Kay Gill and Davidson Axelson and Boris Birmaher and Tina R. Goldstein},\n  year = {2021},\n  journal = {The Journal of Child Psychology and Psychiatry},\n  volume = {62},\n  number = {7},\n  pages = {905--9115},\n  doi = {10.1111/jcpp.13343}\n}\n\n\n\n\nGoals, Tasks, and Bonds: Toward the Computational Assessment of Therapist versus Client Perception of Working Alliance\n\n\nVail, Girard, Bylsma, Cohn, Fournier, Swartz & Morency\n\nProceedings of the 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG)\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nEarly client dropout is one of the most significant challenges facing psychotherapy: recent studies suggest that at least one in five clients will leave treatment prematurely. Clients may terminate therapy for various reasons, but one of the most common causes is the lack of a strong working alliance. The concept of working alliance captures the collaborative relationship between a client and their therapist when working toward the progress and recovery of the client seeking treatment. Unfortunately, clients are often unwilling to directly express dissatisfaction in care until they have already decided to terminate therapy. On the other side, therapists may miss subtle signs of client discontent during treatment before it is too late. In this work, we demonstrate that nonverbal behavior analysis may aid in bridging this gap. The present study focuses primarily on the head gestures of both the client and therapist, contextualized within conversational turn-taking actions between the pair during psychotherapy sessions. We identify multiple behavior patterns suggestive of an individual's perspective on the working alliance; interestingly, these patterns also differ between the client and the therapist. These patterns inform the development of predictive models for self-reported ratings of working alliance, which demonstrate significant predictive power for both client and therapist ratings. Future applications of such models may stimulate preemptive intervention to strengthen a weak working alliance, whether explicitly attempting to repair the existing alliance or establishing a more suitable client-therapist pairing, to ensure that clients encounter fewer barriers to receiving the treatment they need.\n@inproceedings{vail2021,\n  title = {Goals, Tasks, and Bonds: Toward the Computational Assessment of Therapist versus Client Perception of Working Alliance},\n  booktitle = {Proceedings of the 16th {{IEEE International Conference}} on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},\n  author = {Alexandria K. Vail and Jeffrey M. Girard and Lauren Bylsma and Jeffrey F. Cohn and Jay Fournier and Holly A. Swartz and Louis-Philippe Morency},\n  year = {2021},\n  pages = {1--8},\n  doi = {10/gpfjrn}\n}\n\n\n\n\nIn the Eye of the Beholder: A Comprehensive Analysis of Stimulus Type, Perceiver, and Target in Physical Attractiveness Perceptions\n\n\nBowdring, Sayette, Girard & Woods\n\nJournal of Nonverbal Behavior\n\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nPhysical attractiveness plays a central role in psychosocial experiences. One of the top research priorities has been to identify factors affecting perceptions of physical attractiveness (PPA). Recent work suggests PPA derives from different sources (e.g., target, perceiver, stimulus type). Although smiles in particular are believed to enhance PPA, support has been surprisingly limited. This study comprehensively examines the effect of smiles on PPA and, more broadly, evaluates the roles of target, perceiver, and stimulus type in PPA variation. Perceivers (n\\,=\\,181) rated both static images and 5-s videos of targets displaying smiling and neutral-expressions. Smiling images were rated as more attractive than neutral-expression images (regardless of stimulus motion format). Interestingly, perceptions of physical attractiveness were based more on the perceiver than on either the target or format in which the target was presented. Results clarify the effect of smiles, and highlight the significant role of the perceiver, in PPA.\n@article{bowdring2021,\n  title = {In the Eye of the Beholder: A Comprehensive Analysis of Stimulus Type, Perceiver, and Target in Physical Attractiveness Perceptions},\n  author = {Molly A. Bowdring and Michael A. Sayette and Jeffrey M. Girard and William C. Woods},\n  year = {2021},\n  journal = {Journal of Nonverbal Behavior},\n  volume = {45},\n  number = {2},\n  pages = {241--259},\n  doi = {10.1007/s10919-020-00350-2},\n  note = {Last visited on 01/10/2021}\n}\n\n\n\n\nReconsidering the Duchenne Smile: Formalizing and Testing Hypotheses about Eye Constriction and Positive Emotion\n\n\nGirard, Cohn, Yin & Morency\n\nAffective Science\n\n\n\n★\n This project was led by the AffCom lab\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nThe common view of emotional expressions is that certain configurations of facial-muscle movements reliably reveal certain categories of emotion. The principal exemplar of this view is the Duchenne smile, a configuration of facial-muscle movements (i.e., smiling with eye constriction) that has been argued to reliably reveal genuine positive emotion. In this paper, we formalized a list of hypotheses that have been proposed regarding the Duchenne smile, briefly reviewed the literature weighing on these hypotheses, identified limitations and unanswered questions, and conducted two empirical studies to begin addressing these limitations and answering these questions. Both studies analyzed a database of 751 smiles observed while 136 participants completed experimental tasks designed to elicit amusement, embarrassment, fear, and physical pain. Study 1 focused on participants' self-reported positive emotion and Study 2 focused on how third-party observers would perceive videos of these smiles. Most of the hypotheses that have been proposed about the Duchenne smile were either contradicted by or only weakly supported by our data. Eye constriction did provide some information about experienced positive emotion, but this information was lacking in specificity, already provided by other smile characteristics, and highly dependent on context. Eye constriction provided more information about perceived positive emotion, including some unique information over other smile characteristics, but context was also important here as well. Overall, our results suggest that accurately inferring positive emotion from a smile requires more sophisticated methods than simply looking for the presence/absence (or even the intensity) of eye constriction.\n@article{girard2021,\n  title = {Reconsidering the {{Duchenne}} Smile: Formalizing and Testing Hypotheses about Eye Constriction and Positive Emotion},\n  author = {Jeffrey M. Girard and Jeffrey F. Cohn and Lijun Yin and Louis-Philippe Morency},\n  year = {2021},\n  journal = {Affective Science},\n  volume = {2},\n  number = {1},\n  pages = {32--47},\n  doi = {10.1007/s42761-020-00030-w},\n  note = {Last visited on 01/18/2021}\n}\n\n\n\n\nTo Rate or Not to Rate: Investigating Evaluation Methods for Generated Co-Speech Gestures\n\n\nWolfert, Girard, Kucherenko & Belpaeme\n\nProceedings of the 23rd International Conference on Multimodal Interaction\n\n\nAbstract\nCitation\nBibTeX\nPreprint\n\n\nCopy\nCopied!\n—\n\n\n\nWhile automatic performance metrics are crucial for machine learning of artificial human-like behaviour, the gold standard for evaluation remains human judgement. The subjective evaluation of artificial human-like behaviour in embodied conversational agents is however expensive and little is known about the quality of the data it returns. Two approaches to subjective evaluation can be largely distinguished, one relying on ratings, the other on pairwise comparisons. In this study we use co-speech gestures to compare the two against each other and answer questions about their appropriateness for evaluation of artificial behaviour. We consider their ability to rate quality, but also aspects pertaining to the effort of use and the time required to collect subjective data. We use crowd sourcing to rate the quality of co-speech gestures in avatars, assessing which method picks up more detail in subjective assessments. We compared gestures generated by three different machine learning models with various level of behavioural quality. We found that both approaches were able to rank the videos according to quality and that the ranking significantly correlated, showing that in terms of quality there is no preference of one method over the other. We also found that pairwise comparisons were slightly faster and came with improved inter-rater reliability, suggesting that for small-scale studies pairwise comparisons are to be favoured over ratings.\n@inproceedings{wolfert2021,\n  title = {To Rate or Not to Rate: Investigating Evaluation Methods for Generated Co-Speech Gestures},\n  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Multimodal Interaction}}},\n  author = {Pieter Wolfert and Jeffrey M. Girard and Taras Kucherenko and Tony Belpaeme},\n  year = {2021},\n  pages = {494--502},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3462244.3479889},\n  note = {Last visited on 02/17/2022}\n}\n\n\n\n\n\n2020\n\n\n\nContext-Dependent Models for Predicting and Characterizing Facial Expressiveness\n\n\nLin, Girard & Morency\n\nProceedings of the 3rd Workshop on Affective Content Analysis Co-Located with the 34th AAAI Conference on Artificial Intelligence\n\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nIn recent years, extensive research has emerged in affective computing on topics like automatic emotion recognition and determining the signals that characterize individual emotions. Much less studied, however, is expressiveness---the extent to which someone shows any feeling or emotion. Expressiveness is related to personality and mental health and plays a crucial role in social interaction. As such, the ability to automatically detect or predict expressiveness can facilitate significant advancements in areas ranging from psychiatric care to artificial social intelligence. Motivated by these potential applications, we present an extension of the BP4D+ dataset (Zhang et al. 2016) with human ratings of expressiveness and develop methods for (1) automatically predicting expressiveness from visual data and (2) defining relationships between interpretable visual signals and expressiveness. In addition, we study the emotional context in which expressiveness occurs and hypothesize that different sets of signals are indicative of expressiveness in different contexts (e.g., in response to surprise or in response to pain). Analysis of our statistical models confirms our hypothesis. Consequently, by looking at expressiveness separately in distinct emotional contexts, our predictive models show significant improvements over baselines and achieve comparable results to human performance in terms of correlation with the ground truth.\n@inproceedings{lin2020a,\n  title = {Context-Dependent Models for Predicting and Characterizing Facial Expressiveness},\n  booktitle = {Proceedings of the 3rd {{Workshop}} on {{Affective Content Analysis}} Co-Located with the 34th {{AAAI Conference}} on {{Artificial Intelligence}}},\n  author = {Victoria Lin and Jeffrey M. Girard and Louis-Philippe Morency},\n  year = {2020},\n  volume = {2614},\n  pages = {11--28},\n  publisher = {AAAI Press},\n  address = {New York, NY}\n}\n\n\n\n\nDepression Severity Assessment for Adolescents at High Risk of Mental Disorders\n\n\nMuszynski, Zelazny, Girard & Morency\n\nProceedings of the 22nd International Conference on Multimodal Interaction\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nRecent progress in artificial intelligence has led to the development of automatic behavioral marker recognition, such as facial and vocal expressions. Those automatic tools have enormous potential to support mental health assessment, clinical decision making, and treatment planning.\n@inproceedings{muszynski2020,\n  title = {Depression Severity Assessment for Adolescents at High Risk of Mental Disorders},\n  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Multimodal Interaction}}},\n  author = {Michal Muszynski and Jamie Zelazny and Jeffrey M. Girard and Louis-Philippe Morency},\n  year = {2020},\n  pages = {70--78},\n  publisher = {ACM},\n  address = {Virtual Event Netherlands},\n  doi = {10.1145/3382507.3418859},\n  note = {Last visited on 10/26/2020}\n}\n\n\n\n\nProperties of the Continuous Assessment of Interpersonal Dynamics across Sex, Level of Familiarity, and Interpersonal Conflict\n\n\nHopwood, Harrison, Amole, Girard, Wright, Thomas, Sadler, Ansell, Chaplin, Morey, Crowley, Durbin & Kashy\n\nAssessment\n\n\nAbstract\nCitation\nBibTeX\nPreprint\n\n\nCopy\nCopied!\n—\n\n\n\nThe Continuous Assessment of Interpersonal Dynamics (CAID) is a method in which trained observers continuously code the dominance and warmth of individuals who interact with one another in dyads. This method has significant promise for assessing dynamic interpersonal processes. The purpose of this study was to examine the impact of individual sex, dyadic familiarity, and situational conflict on patterns of interpersonal warmth, dominance, and complementarity as assessed via CAID. We used six samples with 603 dyads, including two samples of unacquainted mixed-sex undergraduates interacting in a collaborative task, two samples of couples interacting in both collaborative and conflict tasks, and two samples of mothers and children interacting in both collaborative and conflict tasks. Complementarity effects were robust across all samples, and individuals tended to be relatively warm and dominant. Results from multilevel models indicated that women were slightly warmer than men, whereas there were no sex differences in dominance. Unfamiliar dyads and dyads interacting in more collaborative tasks were relatively warmer, more submissive, and more complementary on warmth but less complementary on dominance. These findings speak to the utility of the CAID method for assessing interpersonal dynamics and provide norms for researchers who use the method for different types of samples and applications.\n@article{hopwood2020,\n  title = {Properties of the Continuous Assessment of Interpersonal Dynamics across Sex, Level of Familiarity, and Interpersonal Conflict},\n  author = {Christopher J. Hopwood and Alana L. Harrison and Marlissa C Amole and Jeffrey M. Girard and Aidan G. C. Wright and Katherine M. Thomas and Pamela Sadler and Emily B. Ansell and Tara M. Chaplin and Leslie C. Morey and Michael J. Crowley and C. Emily Durbin and Deborah A. Kashy},\n  year = {2020},\n  journal = {Assessment},\n  volume = {27},\n  number = {1},\n  pages = {40--56},\n  doi = {10.1177/1073191118798916}\n}\n\n\n\n\nToward Multimodal Modeling of Emotional Expressiveness\n\n\nLin, Girard, Sayette & Morency\n\nProceedings of the 22nd International Conference on Multimodal Interaction\n\n\nAbstract\nCitation\nBibTeX\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nEmotional expressiveness captures the extent to which a person tends to outwardly display their emotions through behavior. Due to the close relationship between emotional expressiveness and behavioral health, as well as the crucial role that it plays in social interaction, the ability to automatically predict emotional expressiveness stands to spur advances in science, medicine, and industry. In this paper, we explore three related research questions. First, how well can emotional expressiveness be predicted from visual, linguistic, and multimodal behavioral signals? Second, how important is each behavioral modality to the prediction of emotional expressiveness? Third, which behavioral signals are reliably related to emotional expressiveness? To answer these questions, we add highly reliable transcripts and human ratings of perceived emotional expressiveness to an existing video database and use this data to train, validate, and test predictive models. Our best model shows promising predictive performance on this dataset (\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend\\dbend{} = 0.65, \\dbend\\dbend\\dbend\\dbend\\dbend\\dbend 2 = 0.45, \\dbend\\dbend\\dbend\\dbend\\dbend\\dbend{} = 0.74). Multimodal models tend to perform best overall, and models trained on the linguistic modality tend to outperform models trained on the visual modality. Finally, examination of our interpretable models' coefficients reveals a number of visual and linguistic behavioral signals---such as facial action unit intensity, overall word count, and use of words related to social processes---that reliably predict emotional expressiveness.\n@inproceedings{lin2020,\n  title = {Toward Multimodal Modeling of Emotional Expressiveness},\n  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Multimodal Interaction}}},\n  author = {Victoria Lin and Jeffrey M. Girard and Michael A. Sayette and Louis-Philippe Morency},\n  year = {2020},\n  pages = {548--557},\n  publisher = {ACM},\n  address = {Virtual Event Netherlands},\n  doi = {10.1145/3382507.3418887},\n  note = {Last visited on 10/26/2020}\n}\n\n\n\n\n\n2019\n\n\n\nAlexithymia -- Not Autism -- Is Associated with Frequency of Social Interactions in Adults\n\n\nGerber, Girard, Scott & Lerner\n\nBehaviour Research and Therapy\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nObjective While much is known about the quality of social behavior among neurotypical individuals and those with autism spectrum disorder (ASD), little work has evaluated quantity of social interactions. This study used ecological momentary assessment (EMA) to quantify in vivo daily patterns of social interaction in adults as a function of demographic and clinical factors. Method Adults with and without ASD (NASD\\,=\\,23, NNeurotypical\\,=\\,52) were trained in an EMA protocol to report their social interactions via smartphone over one week. Participants completed measures of IQ, ASD symptom severity and alexithymia symptom severity. Results Cyclical multilevel models were used to account for nesting of observations. Results suggest a daily cyclical pattern of social interaction that was robust to ASD and alexithymia symptoms. Adults with ASD did not have fewer social interactions than neurotypical peers; however, severity of alexithymia symptoms predicted fewer social interactions regardless of ASD status. Conclusions These findings suggest that alexithymia, not ASD severity, may drive social isolation and highlight the need to reevaluate previously accepted notions regarding differences in social behavior utilizing modern methods.\n@article{gerber2019,\n  title = {Alexithymia -- Not Autism -- Is Associated with Frequency of Social Interactions in Adults},\n  author = {Alan H. Gerber and Jeffrey M. Girard and Stacey B. Scott and Matthew D. Lerner},\n  year = {2019},\n  journal = {Behaviour Research and Therapy},\n  volume = {123},\n  pages = {103477},\n  doi = {10.1016/j.brat.2019.103477}\n}\n\n\n\n\nDemocratizing Psychological Insights from Analysis of Nonverbal Behavior\n\n\nMcDuff & Girard\n\nProceedings of the 8th International Conference on Affective Computing and Intelligent Interaction (ACII)\n\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nThe affective computing community has invested heavily in building automated tools for the analysis of facial behavior and the expression of emotion. These tools present a valuable, but largely untapped, opportunity for social scientists to perform observational analyses of nonverbal behavior at very large scale. Various tech companies are collecting huge corpora of images and videos from around the world that could be used to study important scientific questions. However, privacy restrictions and intellectual property concerns render these data inaccessible to most academics. Unfortunately, this limits the potential for scientific advancement and leads to the consolidation of data and opportunity into the hands of a few powerful institutions. In this paper, we ask whether similar psychological insights can be gained by analyzing smaller, public datasets that are more within reach for academic researchers. As a proof-of-concept for this idea, we gather, analyze, and release a corpus of public images and metadata and use it to replicate recent psychological findings about smiling, gender, and culture. In so doing, we provide evidence that psychological insights can indeed by democratized through the automated analysis of nonverbal behavior.\n@inproceedings{mcduff2019,\n  title = {Democratizing Psychological Insights from Analysis of Nonverbal Behavior},\n  booktitle = {Proceedings of the 8th {{International Conference}} on {{Affective Computing}} and {{Intelligent Interaction}} ({{ACII}})},\n  author = {Daniel McDuff and Jeffrey M. Girard},\n  year = {2019},\n  pages = {220--226},\n  publisher = {IEEE},\n  address = {Cambridge, UK},\n  doi = {10.1109/acii.2019.8925503}\n}\n\n\n\n\nNarcissistic Admiration and Rivalry: An Interpersonal Approach to Construct Validation\n\n\nGrove, Smith, Girard & Wright\n\nJournal of Personality Disorders\n\n\nAbstract\nCitation\nBibTeX\nPreprint\n\n\nCopy\nCopied!\n—\n\n\n\nThe present study applied the interpersonal perspective in testing the narcissistic admiration and rivalry concept (NARC) and examining the construct validity of the corresponding Narcissistic Admiration and Rivalry Questionnaire (NARQ). Two undergraduate samples (Sample 1: N = 290; Sample 2: N = 188) completed self-report measures of interpersonal processes based in the interpersonal circumplex (IPC), as well as measures of related constructs. In examining IPC correlates, the authors used a novel bootstrapping approach to determine if admiration and rivalry related to differing interpersonal profiles. Consistent with the authors' hypotheses, admiration was distinctly related to generally agentic (i.e., dominant) interpersonal processes, whereas rivalry generally reflected (low) communal (i.e., hostile) interpersonal processes. Furthermore, NARQ-admiration and NARQ-rivalry related to generally adaptive and maladaptive aspects of status-related constructs, emotional, personality, and social adjustment, respectively. This research provides further support for the NARC, as well as construct validation for the NARQ.\n@article{grove2019,\n  title = {Narcissistic Admiration and Rivalry: An Interpersonal Approach to Construct Validation},\n  author = {Jeremy L. Grove and Timothy W. Smith and Jeffrey M. Girard and Aidan G. Wright},\n  year = {2019},\n  journal = {Journal of Personality Disorders},\n  volume = {33},\n  number = {6},\n  pages = {751--775},\n  doi = {10.1521/pedi_2019_33_374},\n  note = {Last visited on 01/17/2020}\n}\n\n\n\n\nReconsidering the Duchenne Smile: Indicator of Positive Emotion or Artifact of Smile Intensity?\n\n\nGirard, Shandar, Liu, Cohn, Yin & Morency\n\nProceedings of the 8th International Conference on Affective Computing and Intelligent Interaction (ACII)\n\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nThe Duchenne smile hypothesis is that smiles that include eye constriction (AU6) are the product of genuine positive emotion, whereas smiles that do not are either falsified or related to negative emotion. This hypothesis has become very influential and is often used in scientific and applied settings to justify the inference that a smile is either true or false. However, empirical support for this hypothesis has been equivocal and some researchers have proposed that, rather than being a reliable indicator of positive emotion, AU6 may just be an artifact produced by intense smiles. Initial support for this proposal has been found when comparing smiles related to genuine and feigned positive emotion; however, it has not yet been examined when comparing smiles related to genuine positive and negative emotion. The current study addressed this gap in the literature by examining spontaneous smiles from 136 participants during the elicitation of amusement, embarrassment, fear, and pain (from the BP4D+ dataset). Bayesian multilevel regression models were used to quantify the associations between AU6 and self-reported amusement while controlling for smile intensity. Models were estimated to infer amusement from AU6 and to explain the intensity of AU6 using amusement. In both cases, controlling for smile intensity substantially reduced the hypothesized association, whereas the effect of smile intensity itself was quite large and reliable. These results provide further evidence that the Duchenne smile is likely an artifact of smile intensity rather than a reliable and unique indicator of genuine positive emotion.\n@inproceedings{girard2019,\n  title = {Reconsidering the Duchenne Smile: Indicator of Positive Emotion or Artifact of Smile Intensity?},\n  booktitle = {Proceedings of the 8th {{International Conference}} on {{Affective Computing}} and {{Intelligent Interaction}} ({{ACII}})},\n  author = {Jeffrey M. Girard and Gayatri Shandar and Zhun Liu and Jeffrey F. Cohn and Lijun Yin and Louis-Philippe Morency},\n  year = {2019},\n  pages = {594--599},\n  publisher = {IEEE},\n  address = {Cambridge, UK},\n  doi = {10.1109/acii.2019.8925535}\n}\n\n\n\n\n\n2018\n\n\n\nAffective Facial Computing: Generalizability across Domains\n\n\nCohn, Ertugrul, Chu, Girard & Hammal\n\nMultimodal Behavior Analysis in the Wild: Advances and Challenges\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nNA\n@incollection{cohn2018,\n  title = {Affective Facial Computing: Generalizability across Domains},\n  booktitle = {Multimodal Behavior Analysis in the Wild: {{Advances}} and Challenges},\n  author = {Jeffrey F. Cohn and Itir Onal Ertugrul and Wen-Sheng Chu and Jeffrey M. Girard and Zakia Hammal},\n  editor = {Xavier Alameda-Pineda and Elisa Ricci and Nicu Sebe},\n  year = {2018},\n  pages = {407--441},\n  publisher = {Academic Press}\n}\n\n\n\n\nDARMA: Software for Dual Axis Rating and Media Annotation\n\n\nGirard & Wright\n\nBehavior Research Methods\n\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nContinuous measurement systems provide a means of measuring dynamic behavioral and experiential processes as they play out over time. DARMA is a modernized continuous measurement system that synchronizes media playback and the continuous recording of two-dimensional measurements. These measurements can be observational or self-reported and are provided in real-time through the manipulation of a computer joystick. DARMA also provides tools for reviewing and comparing collected measurements and for customizing various settings. DARMA is a domain-independent software tool that was designed to aid researchers who are interested in gaining a deeper understanding of behavior and experience. It is especially well-suited to the study of affective and interpersonal processes, such as the perception and expression of emotional states and the communication of social signals. DARMA is open-source using the GNU General Public License (GPL) and is available for free download from http://darma.jmgirard.com.\n@article{girard2018,\n  title = {{{DARMA}}: Software for Dual Axis Rating and Media Annotation},\n  author = {Jeffrey M. Girard and Aidan G C Wright},\n  year = {2018},\n  journal = {Behavior Research Methods},\n  volume = {50},\n  number = {3},\n  pages = {902--909},\n  doi = {10.3758/s13428-017-0915-5}\n}\n\n\n\n\nThe Association between Daily Posttraumatic Stress Symptoms and Pain over the First 14 Days after Injury: An Experience Sampling Study\n\n\nPacella, Girard, Wright, Suffoletto & Callaway\n\nAcademic Emergency Medicine\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nObjectives Psychosocial factors and responses to injury modify the transition from acute to chronic pain. Specifically, posttraumatic stress disorder (PTSD) symptoms (reexperiencing, avoidance, and hyperarousal symptoms) exacerbate and cooccur with chronic pain. Yet no study has prospectively considered the associations among these psychological processes and pain reports using experience sampling methods (ESMs) during the acute aftermath of injury. This study applied ESM via daily text messaging to monitor and detect relationships among psychosocial factors and postinjury pain across the first 14 days after emergency department (ED) discharge. Methods We recruited 75 adults (59\\% male; mean \\textpm{} SD age = 34 \\textpm{} 11.73 years) who experienced a potentially traumatic injury (i.e., involving life threat or serious injury) in the past 24 hours from the EDs of two Level I trauma centers. Participants received five questions per day via text messaging from Day 1 to Day 14 post--ED discharge; three questions measured PTSD symptoms, one question measured perceived social support, and one question measured physical pain. Results Sixty-seven participants provided sufficient data for inclusion in the final analyses, and the average response rate per subject was 86\\%. Pain severity score decreased from a mean \\textpm{} SD of 7.2 \\textpm{} 2.0 to 4.4 \\textpm{} 2.69 over 14 days and 50\\% of the variance in daily pain scores was within person. In multilevel structural equation models, pain scores decreased over time, and daily fluctuations of hyperarousal (B = 0.22, 95\\% confidetnce interval = 0.08--0.36) were uniquely associated with daily fluctuations in reported pain level within each person. Conclusions Daily hyperarousal symptoms predict same-day pain severity over the acute postinjury recovery period. We also demonstrated feasibility to screen and identify patients at risk for pain chronicity in the acute aftermath of injury. Early interventions aimed at addressing hyperarousal (e.g., anxiolytics) could potentially aid in reducing experience of pain.\n@article{pacella2018,\n  title = {The Association between Daily Posttraumatic Stress Symptoms and Pain over the First 14 Days after Injury: {{An}} Experience Sampling Study},\n  author = {Maria L. Pacella and Jeffrey M. Girard and Aidan G. C. Wright and Brian Suffoletto and Clifton W. Callaway},\n  year = {2018},\n  journal = {Academic Emergency Medicine},\n  volume = {25},\n  number = {8},\n  pages = {844--855},\n  doi = {10.1111/acem.13406}\n}\n\n\n\n\n\n2017\n\n\n\nComparison of Comprehensive and Abstinence-Only Sexuality Education in Young African American Adolescents\n\n\nShepherd, Sly & Girard\n\nJournal of Adolescence\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nThe purpose of this study was to identify predictors of sexual behavior and condom use in African American adolescents, as well as to evaluate the effectiveness of comprehensive sexuality and abstinence-only education to reduce adolescent sexual behavior and increase condom use. Participants included 450 adolescents aged 12--14 years in the southern United States. Regression analyses showed favorable attitudes toward sexual behavior and social norms significantly predicted recent sexual behavior, and favorable attitudes toward condoms significantly predicted condom usage. Self-efficacy was not found to be predictive of adolescents' sexual behavior or condom use. There were no significant differences in recent sexual behavior based on type of sexuality education. Adolescents who received abstinence-only education had reduced favorable attitudes toward condom use, and were more likely to have unprotected sex than the comparison group. Findings suggest that adolescents who receive abstinence-only education are at greater risk of engaging in unprotected sex.\n@article{shepherd2017,\n  title = {Comparison of Comprehensive and Abstinence-Only Sexuality Education in Young African American Adolescents},\n  author = {Lindsay M. Shepherd and Kaye F. Sly and Jeffrey M. Girard},\n  year = {2017},\n  journal = {Journal of Adolescence},\n  volume = {61},\n  pages = {50--63},\n  doi = {10.1016/j.adolescence.2017.09.006}\n}\n\n\n\n\nFERA 2017 - Addressing Head Pose in the Third Facial Expression Recognition and Analysis Challenge\n\n\nValstar, Sanchez-Lozano, Cohn, Jeni, Girard, Zhang, Yin & Pantic\n\nProceedings of the 12th IEEE Conference on Automatic Face and Gesture Recognition (FG)\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nThe field of Automatic Facial Expression Analysis has grown rapidly in recent years. However, despite progress in new approaches as well as benchmarking efforts, most evaluations still focus on either posed expressions, near-frontal recordings, or both. This makes it hard to tell how existing expression recognition approaches perform under conditions where faces appear in a wide range of poses (or camera views), displaying ecologically valid expressions. The main obstacle for assessing this is the availability of suitable data, and the challenge proposed here addresses this limitation. The FG 2017 Facial Expression Recognition and Analysis challenge (FERA 2017) extends FERA 2015 to the estimation of Action Units occurrence and intensity under different camera views. In this paper we present the third challenge in automatic recognition of facial expressions, to be held in conjunction with the 12th IEEE conference on Face and Gesture Recognition, May 2017, in Washington, United States. Two sub-challenges are defined: the detection of AU occurrence, and the estimation of AU intensity. In this work we outline the evaluation protocol, the data used, and the results of a baseline method for both sub-challenges.\n@inproceedings{valstar2017,\n  title = {{{FERA}} 2017 - Addressing Head Pose in the Third Facial Expression Recognition and Analysis Challenge},\n  booktitle = {Proceedings of the 12th {{IEEE Conference}} on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},\n  author = {Michel F Valstar and Enrique Sanchez-Lozano and Jeffrey F. Cohn and Laszlo A Jeni and Jeffrey M. Girard and Zheng Zhang and Lijun Yin and Maja Pantic},\n  year = {2017},\n  pages = {839--847},\n  publisher = {IEEE},\n  address = {Washington, DC},\n  doi = {10.1109/fg.2017.107}\n}\n\n\n\n\nHistorical Heterogeneity Predicts Smiling: Evidence from Large-Scale Observational Analyses\n\n\nGirard & McDuff\n\nProceedings of the 12th IEEE International Conference on Automatic Face and Gesture Recognition (FG)\n\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nFacial behavior is a valuable source of informationabout an individual's feelings and intentions. However, manyfactors combine to influence and moderate facial behaviorincluding personality, gender, context, and culture. Due to thehigh cost of traditional observational methods, the relationshipbetween culture and facial behavior is not well-understood. Inthe current study, we explored the sociocultural factors that influencefacial behavior using large-scale observational analyses.We developed and implemented an algorithm to automaticallyanalyze the smiling of 866,726 participants across 31 differentcountries. We found that participants smiled more when from acountry that is higher in individualism, has a lower populationdensity, and has a long history of immigration diversity (i.e.,historical heterogeneity). Our findings provide the first evidencethat historical heterogeneity predicts actual smiling behavior.Furthermore, they converge with previous findings using selfreportmethods. Taken together, these findings support thetheory that historical heterogeneity explains, and may evencontribute to the development of, permissive cultural displayrules that encourage the open expression of emotion.\n@inproceedings{girard2017a,\n  title = {Historical Heterogeneity Predicts Smiling: Evidence from Large-Scale Observational Analyses},\n  booktitle = {Proceedings of the 12th {{IEEE International Conference}} on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},\n  author = {Jeffrey M. Girard and Daniel McDuff},\n  year = {2017},\n  pages = {719--726},\n  publisher = {IEEE},\n  address = {Washington, DC},\n  doi = {10.1109/fg.2017.135}\n}\n\n\n\n\nInterpersonal Problems across Levels of the Psychopathology Hierarchy\n\n\nGirard, Wright, Beeney, Lazarus, Scott, Stepp & Pilkonis\n\nComprehensive Psychiatry\n\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nWe examined the relationship between psychopathology and interpersonal problems in a sample of 825 clinical and community participants. Sixteen psychiatric diagnoses and five transdiagnostic dimensions were examined in relation to self-reported interpersonal problems. The structural summary method was used with the Inventory of Interpersonal Problems Circumplex Scales to examine interpersonal problem profiles for each diagnosis and dimension. We built a structural model of mental disorders including factors corresponding to detachment (avoidant personality, social phobia, major depression), internalizing (dependent personality, borderline personality, panic disorder, posttraumatic stress, major depression), disinhibition (antisocial personality, drug dependence, alcohol dependence, borderline personality), dominance (histrionic personality, narcissistic personality, paranoid personality), and compulsivity (obsessive-compulsive personality). All dimensions showed good interpersonal prototypicality (e.g., detachment was defined by a socially avoidant/nonassertive interpersonal profile) except for internalizing, which was diffusely associated with elevated interpersonal distress. The findings for individual disorders were largely consistent with the dimension that each disorder loaded on, with the exception of the internalizing and dominance disorders, which were interpersonally heterogeneous. These results replicate previous findings and provide novel insights into social dysfunction in psychopathology by wedding the power of hierarchical (i.e., dimensional) modeling and interpersonal circumplex assessment.\n@article{girard2017,\n  title = {Interpersonal Problems across Levels of the Psychopathology Hierarchy},\n  author = {Jeffrey M. Girard and Aidan G. C. Wright and Joseph E Beeney and Sophie A Lazarus and Lori N Scott and Stephanie D Stepp and Paul A Pilkonis},\n  year = {2017},\n  journal = {Comprehensive Psychiatry},\n  volume = {79},\n  pages = {53--69},\n  doi = {10.1016/j.comppsych.2017.06.014}\n}\n\n\n\n\nLarge-Scale Observational Evidence of Cross-Cultural Differences in Facial Behavior\n\n\nMcDuff, Girard & El Kaliouby\n\nJournal of Nonverbal Behavior\n\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nSelf-report studies have found evidence that cultures differ in the display rules they have for facial expressions (i.e., for what is appropriate for different people at different times). However, observational studies of actual patterns of facial behavior have been rare and typically limited to the analysis of dozens of participants from two or three regions. We present the first large-scale evidence of cultural differences in observed facial behavior, including 740,984 participants from 12 countries around the world. We used an Internet-based framework to collect video data of participants in two different settings: in their homes and in market research facilities. Using computer vision algorithms designed for this data set, we measured smiling and brow furrowing expressions as participants watched television ads. Our results reveal novel findings and provide empirical evidence to support theories about cultural and gender differences in display rules. Participants from more individualist cultures displayed more brow furrowing overall, whereas smiling depended on both culture and setting. Specifically, participants from more individualist countries were more expressive in the facility setting, while participants from more collectivist countries were more expressive in the home setting. Female participants displayed more smiling and less brow furrowing than male participants overall, with the latter difference being more pronounced in more individualist countries. This is the first study to leverage advances in computer science to enable large-scale observational research that would not have been possible using traditional methods\n@article{mcduff2017,\n  title = {Large-Scale Observational Evidence of Cross-Cultural Differences in Facial Behavior},\n  author = {Daniel McDuff and Jeffrey M. Girard and Rana {El Kaliouby}},\n  year = {2017},\n  journal = {Journal of Nonverbal Behavior},\n  volume = {41},\n  number = {1},\n  pages = {1--19},\n  doi = {10/f92t52}\n}\n\n\n\n\nMomentary Patterns of Covariation between Specific Affects and Interpersonal Behavior: Linking Relationship Science and Personality Assessment\n\n\nRoss, Girard, Wright, Beeney, Scott, Hallquist, Lazarus, Stepp & Pilkonis\n\nPsychological Assessment\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nRelationships are among the most salient factors affecting happiness and wellbeing for individuals and families. Relationship science has identified the study of dyadic behavioral patterns between couple members during conflict as an important window in to relational functioning with both short-term and long-term consequences. Several methods have been developed for the momentary assessment of behavior during interpersonal transactions. Among these, the most popular is the Specific Affect Coding System (SPAFF), which organizes social behavior into a set of discrete behavioral constructs. This study examines the interpersonal meaning of the SPAFF codes through the lens of interpersonal theory, which uses the fundamental dimensions of Dominance and Affiliation to organize interpersonal behavior. A sample of 67 couples completed a conflict task, which was video recorded and coded using SPAFF and a method for rating momentary interpersonal behavior, the Continuous Assessment of Interpersonal Dynamics (CAID). Actor partner interdependence models in a multilevel structural equation modeling framework were used to study the covariation of SPAFF codes and CAID ratings. Results showed that a number of SPAFF codes had clear interpersonal signatures, but many did not. Additionally, actor and partner effects for the same codes were strongly consistent with interpersonal theory's principle of complementarity. Thus, findings reveal points of convergence and divergence in the 2 systems and provide support for central tenets of interpersonal theory. Future directions based on these initial findings are discussed.\n@article{ross2017,\n  title = {Momentary Patterns of Covariation between Specific Affects and Interpersonal Behavior: {{Linking}} Relationship Science and Personality Assessment},\n  author = {Jaclyn M. Ross and Jeffrey M. Girard and Aidan G. C. Wright and Joseph E Beeney and Lori N. Scott and Michael N. Hallquist and Sophie A. Lazarus and Stephanie D. Stepp and Paul A. Pilkonis},\n  year = {2017},\n  journal = {Psychological Assessment},\n  volume = {29},\n  number = {2},\n  pages = {123--134},\n  doi = {10.1037/pas0000338}\n}\n\n\n\n\nOpen-Source Software for Continuous Measurement and Media Annotation\n\n\nGirard\n\nProceedings of the 12th IEEE International Conference on Automated Face and Gesture Recognition (FG)\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nFull understanding of behavior and experience requires anappreciation of time-dependent patterns. However, traditionalmethods of observational measurement and self-reportingare ill-suited to capturing such patterns. These methodstend to polarize into either macro-level (gist) analyses oflarge swaths of time or micro-level (atomic) analyses ofdiscrete segments. Unfortunately, both approaches miss thecontinuous, dynamic flow of many psychological processes.Specialized methods are needed that can capture such processesas they unfold over time and across dimensions.\n@inproceedings{girard2017c,\n  title = {Open-Source Software for Continuous Measurement and Media Annotation},\n  booktitle = {Proceedings of the 12th {{IEEE}} International Conference on Automated Face and Gesture Recognition ({{FG}})},\n  author = {Jeffrey M. Girard},\n  year = {2017},\n  volume = {31},\n  pages = {995--995},\n  doi = {10.1109/fg.2017.151}\n}\n\n\n\n\nSayette Group Formation Task (GFT) Spontaneous Facial Expression Database\n\n\nGirard, Chu, Jeni, Cohn, De La Torre & Sayette\n\nProceedings of the 12th IEEE International Conference on Automatic Face and Gesture Recognition (FG)\n\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nDespite the important role that facial expressionsplay in interpersonal communication and our knowledge thatinterpersonal behavior is influenced by social context, nocurrently available facial expression database includes multipleinteracting participants. The Sayette Group Formation Task(GFT) database addresses the need for well-annotated videoof multiple participants during unscripted interactions. Thedatabase includes 172,800 video frames from 96 participantsin 32 three-person groups. To aid in the development ofautomated facial expression analysis systems, GFT includesexpert annotations of FACS occurrence and intensity, faciallandmark tracking, and baseline results for linear SVM, deeplearning, active patch learning, and personalized classification.Baseline performance is quantified and compared using identicalpartitioning and a variety of metrics (including meansand confidence intervals). The highest performance scores werefound for the deep learning and active patch learning methods.Learn more at http://osf.io/7wcyz.\n@inproceedings{girard2017b,\n  title = {Sayette Group Formation Task ({{GFT}}) Spontaneous Facial Expression Database},\n  booktitle = {Proceedings of the 12th {{IEEE International Conference}} on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},\n  author = {Jeffrey M. Girard and Wen-Sheng Chu and Laszlo A. Jeni and Jeffrey F. Cohn and F. {De La Torre} and Michael A. Sayette},\n  year = {2017},\n  pages = {581--588},\n  publisher = {IEEE},\n  address = {Washington, DC},\n  doi = {10.1109/fg.2017.144}\n}\n\n\n\n\n\n2016\n\n\n\nA Primer on Observational Measurement\n\n\nGirard & Cohn\n\nAssessment\n\n\nAbstract\nCitation\nBibTeX\nPreprint\n\n\nCopy\nCopied!\n—\n\n\n\nObservational measurement plays an integral role in a variety of scientific endeavors within biology, psychology, sociology, education, medicine, and marketing. The current article provides an interdisciplinary primer on observational measurement; in particular, it highlights recent advances in observational methodology and the challenges that accompany such growth. First, we detail the various types of instrument that can be used to standardize measurements across observers. Second, we argue for the importance of validity in observational measurement and provide several approaches to validation based on contemporary validity theory. Third, we outline the challenges currently faced by observational researchers pertaining to measurement drift, observer reactivity, reliability analysis, and time/expense. Fourth, we describe recent advances in computer-assisted measurement, fully automated measurement, and statistical data analysis. Finally, we identify several key directions for future observational research to explore.\n@article{girard2016a,\n  title = {A Primer on Observational Measurement},\n  author = {Jeffrey M. Girard and Jeffrey F. Cohn},\n  year = {2016},\n  journal = {Assessment},\n  volume = {23},\n  number = {4},\n  pages = {404--413},\n  doi = {10.1177/1073191116635807},\n  note = {Last visited on 01/13/2019}\n}\n\n\n\n\nMultimodal Spontaneous Emotion Corpus for Human Behavior Analysis\n\n\nZhang, Girard, Wu, Zhang, Liu, Ciftci, Canavan, Reale, Horowitz, Yang, Cohn, Ji & Yin\n\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nEmotion is expressed in multiple modalities, yet most research has considered at most one or two. This stems in part from the lack of large, diverse, well-annotated, multimodal databases with which to develop and test algorithms. We present a well-annotated, multimodal, multidimensional spontaneous emotion corpus of 140 participants. Emotion inductions were highly varied. Data were acquired from a variety of sensors of the face that included high-resolution 3D dynamic imaging, high-resolution 2D video, and thermal (infrared) sensing, and contact physiological sensors that included electrical conductivity of the skin, respiration, blood pressure, and heart rate. Facial expression was annotated for both the occurrence and intensity of facial action units from 2D video by experts in the Facial Action Coding System (FACS). The corpus further includes derived features from 3D, 2D, and IR (infrared) sensors and baseline results for facial expression and action unit detection. The entire corpus will be made available to the research community.\n@inproceedings{zhang2016,\n  title = {Multimodal Spontaneous Emotion Corpus for Human Behavior Analysis},\n  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},\n  author = {Zheng Zhang and Jeffrey M. Girard and Yue Wu and Xing Zhang and Peng Liu and Umur Ciftci and Shaun Canavan and Michael Reale and Andrew Horowitz and Huiyuan Yang and Jeffrey F. Cohn and Qiang Ji and Lijun Yin},\n  year = {2016},\n  pages = {3438--3446},\n  publisher = {IEEE},\n  address = {Las Vegas, NV},\n  doi = {10.1109/cvpr.2016.374}\n}\n\n\n\n\n\n2015\n\n\n\nAutomated Audiovisual Depression Analysis\n\n\nGirard & Cohn\n\nCurrent Opinion in Psychology\n\n\nAbstract\nCitation\nBibTeX\nPreprint\n\n\nCopy\nCopied!\n—\n\n\n\nAnalysis of observable behavior in depression primarily relies on subjective measures. New computational approaches make possible automated audiovisual measurement of behaviors that humans struggle to quantify (e.g., movement velocity and voice inflection). These tools have the potential to improve screening and diagnosis, identify new behavioral indicators of depression, measure response to clinical intervention, and test clinical theories about underlying mechanisms. Highlights include a study that measured the temporal coordination of vocal tract and facial movements, a study that predicted which adolescents would go on to develop depression based on their voice qualities, and a study that tested the behavioral predictions of clinical theories using automated measures of facial actions and head motion.\n@article{girard2015c,\n  title = {Automated Audiovisual Depression Analysis},\n  author = {Jeffrey M. Girard and Jeffrey F. Cohn},\n  year = {2015},\n  journal = {Current Opinion in Psychology},\n  volume = {4},\n  pages = {75--79},\n  doi = {10.1016/j.copsyc.2014.12.010}\n}\n\n\n\n\nEstimating Smile Intensity: A Better Way\n\n\nGirard, Cohn & De la Torre\n\nPattern Recognition Letters\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nBoth the occurrence and intensity of facial expressions are critical to what the face reveals. While much progress has been made toward the automatic detection of facial expression occurrence, controversy exists about how to estimate expression intensity. The most straight-forward approach is to train multiclass or regression models using intensity ground truth. However, collecting intensity ground truth is even more time consuming and expensive than collecting binary ground truth. As a shortcut, some researchers have proposed using the decision values of binary-trained maximum margin classifiers as a proxy for expression intensity. We provide empirical evidence that this heuristic is flawed in practice as well as in theory. Unfortunately, there are no shortcuts when it comes to estimating smile intensity: researchers must take the time to collect and train on intensity ground truth. However, if they do so, high reliability with expert human coders can be achieved. Intensity-trained multiclass and regression models outperformed binary-trained classifier decision values on smile intensity estimation across multiple databases and methods for feature extraction and dimensionality reduction. Multiclass models even outperformed binary-trained classifiers on smile occurrence detection.\n@article{girard2015b,\n  title = {Estimating Smile Intensity: A Better Way},\n  author = {Jeffrey M. Girard and Jeffrey F. Cohn and Fernando {De la Torre}},\n  year = {2015},\n  journal = {Pattern Recognition Letters},\n  volume = {66},\n  pages = {13--21},\n  doi = {10/f7tkjg}\n}\n\n\n\n\nFERA 2015 - Second Facial Expression Recognition and Analysis Challenge\n\n\nValstar, Almaev, Girard, McKeown, Mehu, Yin, Pantic & Cohn\n\nProceedings of the 11th IEEE International Conference on Automatic Face and Gesture Recognition (FG)\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nDespite efforts towards evaluation standards in facial expression analysis (e.g. FERA 2011), there is a need for up-to-date standardised evaluation procedures, focusing in par- ticular on current challenges in the field. One of the challenges that is actively being addressed is the automatic estimation of expression intensities. To continue to provide a standardisation platform and to help the field progress beyond its current limitations, the FG 2015 Facial Expression Recognition and Analysis challenge (FERA 2015) will challenge participants to estimate FACS Action Unit (AU) intensity as well as AU occurrence on a common benchmark dataset with reliable manual annotations. Evaluation will be done using a clear and well-defined protocol. In this paper we present the second such challenge in automatic recognition of facial expressions, to be held in conjunction with the 11 IEEE conference on Face and Gesture Recognition, May 2015, in Ljubljana, Slovenia. Three sub-challenges are defined: the detection of AU occurrence, the estimation of AU intensity for pre-segmented data, and fully automatic AU intensity estimation. In this work we outline the evaluation protocol, the data used, and the results of a baseline method for the three sub-challenges.\n@inproceedings{valstar2015,\n  title = {{{FERA}} 2015 - Second Facial Expression Recognition and Analysis Challenge},\n  booktitle = {Proceedings of the 11th {{IEEE International Conference}} on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},\n  author = {Michel F. Valstar and Timur Almaev and Jeffrey M. Girard and Gary McKeown and Marc Mehu and Lijun Yin and Maja Pantic and Jeffrey F. Cohn},\n  year = {2015},\n  pages = {1--8},\n  publisher = {IEEE},\n  address = {Ljubljana, Slovenia},\n  doi = {10.1109/fg.2015.7284874}\n}\n\n\n\n\nHow Much Training Data for Facial Action Unit Detection?\n\n\nGirard, Cohn, Jeni, Lucey & De la Torre\n\nProceedings of the 11th IEEE International Conference on Automatic Face and Gesture Recognition (FG)\n\n\nAbstract\nCitation\nBibTeX\nPreprint\n\n\nCopy\nCopied!\n—\n\n\n\nBy systematically varying the number of subjects and the number of frames per subject, we explored the influence of training set size on appearance and shape-based approaches to facial action unit (AU) detection. Digital video and expert coding of spontaneous facial activity from 80 subjects (over 350,000 frames) were used to train and test support vector machine classifiers. Appearance features were shape-normalized SIFT descriptors and shape features were 66 facial landmarks. Ten-fold cross-validation was used in all evaluations. Number of subjects and number of frames per subject differentially affected appearance and shape-based classifiers. For appearance features, which are high-dimensional, increasing the number of training subjects from 8 to 64 incrementally improved performance, regardless of the number of frames taken from each subject (ranging from 450 through 3600). In contrast, for shape features, increases in the number of training subjects and frames were associated with mixed results. In summary, maximal performance was attained using appearance features from large numbers of subjects with as few as 450 frames per subject. These findings suggest that variation in the number of subjects rather than number of frames per subject yields most efficient performance.\n@inproceedings{girard2015a,\n  title = {How Much Training Data for Facial Action Unit Detection?},\n  booktitle = {Proceedings of the 11th {{IEEE International Conference}} on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},\n  author = {Jeffrey M. Girard and Jeffrey F Cohn and L{\\a'a}szl{\\a'o} A Jeni and Simon Lucey and Fernando {De la Torre}},\n  year = {2015},\n  pages = {1--8},\n  publisher = {IEEE},\n  address = {Ljubljana, Slovenia},\n  doi = {10.1109/fg.2015.7163106}\n}\n\n\n\n\nReal-Time Dense 3D Face Alignment from 2D Video with Automatic Facial Action Unit Coding\n\n\nJeni, Girard, Cohn & Kanade\n\nIEEE International Conference and Workshops on Automatic Face and Gesture Recognition\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nFace alignment is the problem of automatically locating detailed facial landmarks across different subjects, illuminations, and viewpoints. Previous methods can be divided into two broad categories. 2D-based methods locate a relatively small number of 2D fiducial points in real time while 3D-based methods fit a high-resolution 3D model offline at a much higher computational cost.\n@inproceedings{jeni2015,\n  title = {Real-Time Dense {{3D}} Face Alignment from {{2D}} Video with Automatic Facial Action Unit Coding},\n  booktitle = {{{IEEE International Conference}} and {{Workshops}} on {{Automatic Face}} and {{Gesture Recognition}}},\n  author = {L{\\a'a}szl{\\a'o} A Jeni and Jeffrey M. Girard and Jeffrey F. Cohn and Takeo Kanade},\n  year = {2015},\n  doi = {10.1109/fg.2015.7163165}\n}\n\n\n\n\nSpeech Volume Indexes Sex Differences in the Social-Emotional Effects of Alcohol\n\n\nFairbairn, Sayette, Amole, Dimoff, Cohn & Girard\n\nExperimental and Clinical Psychopharmacology\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nMen and women differ dramatically in their rates of alcohol use disorder (AUD), and researchers have long been interested in identifying mechanisms underlying male vulnerability to problem drinking. Surveys suggest that social processes underlie sex differences in drinking patterns, with men reporting greater social enhancement from alcohol than women, and all-male social drinking contexts being associated with particularly high rates of hazardous drinking. But experimental evidence for sex differences in social-emotional response to alcohol has heretofore been lacking. Research using larger sample sizes, a social context, and more sensitive measures of alcohol's rewarding effects may be necessary to better understand sex differences in the etiology of AUD. This study explored the acute effects of alcohol during social exchange on speech volume---an objective measure of social-emotional experience that was reliably captured at the group level. Social drinkers (360 male; 360 female) consumed alcohol (.82 g/kg males; .74 g/kg females), placebo, or a no-alcohol control beverage in groups of 3 over 36-min. Within each of the 3 beverage conditions, equal numbers of groups consisted of all males, all females, 2 females and 1 male, and 1 female and 2 males. Speech volume was monitored continuously throughout the drink period, and group volume emerged as a robust correlate of self-report and facial indexes of social reward. Notably, alcohol-related increases in group volume were observed selectively in all-male groups but not in groups containing any females. Results point to social enhancement as a promising direction for research exploring factors underlying sex differences in problem drinking.\n@article{fairbairn2015,\n  title = {Speech Volume Indexes Sex Differences in the Social-Emotional Effects of Alcohol},\n  author = {Catharine E. Fairbairn and Michael A. Sayette and Marlissa C. Amole and John D. Dimoff and Jeffrey F. Cohn and Jeffrey M. Girard},\n  year = {2015},\n  journal = {Experimental and Clinical Psychopharmacology},\n  volume = {23},\n  number = {4},\n  pages = {255--264},\n  doi = {10.1037/pha0000021}\n}\n\n\n\n\nSpontaneous Facial Expression in Unscripted Social Interactions Can Be Measured Automatically\n\n\nGirard, Cohn, Jeni, Sayette & De la Torre\n\nBehavior Research Methods\n\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nMethods to assess individual facial actions have potential to shed light on important behavioral phenomena ranging from emotion and social interaction to psychological disorders and health. However, manual coding of such actions is labor intensive and requires extensive training. To date, establishing reliable automated coding of unscripted facial actions has been a daunting challenge impeding development of psychological theories and applications requiring facial expression assessment. It is therefore essential that automated coding systems be developed with enough precision and robustness to ease the burden of manual coding in challenging data involving variation in participant gender, ethnicity, head pose, speech, and occlusion. We report a major advance in automated coding of spontaneous facial actions during an unscripted social interaction involving three strangers. For each participant (n = 80, 47 \\% women, 15 \\% Nonwhite), 25 facial action units (AUs) were manually coded from video using the Facial Action Coding System. Twelve AUs occurred more than 3 \\% of the time and were processed using automated FACS coding. Automated coding showed very strong reliability for the proportion of time that each AU occurred (mean intraclass correlation = 0.89), and the more stringent criterion of frame-by-frame reliability was moderate to strong (mean Matthew's correlation = 0.61). With few exceptions, differences in AU detection related to gender, ethnicity, pose, and average pixel intensity were small. Fewer than 6 \\% of frames could be coded manually but not automatically. These findings suggest automated FACS coding has progressed sufficiently to be applied to observational research in emotion and related areas of study.\n@article{girard2015d,\n  title = {Spontaneous Facial Expression in Unscripted Social Interactions Can Be Measured Automatically},\n  author = {Jeffrey M. Girard and Jeffrey F. Cohn and L{\\a'a}szl{\\a'o} A. Jeni and Michael A. Sayette and Fernando {De la Torre}},\n  year = {2015},\n  journal = {Behavior Research Methods},\n  volume = {47},\n  number = {4},\n  pages = {1136--1147},\n  doi = {10.3758/s13428-014-0536-1}\n}\n\n\n\n\n\n2014\n\n\n\nBP4D-spontaneous: A High-Resolution Spontaneous 3D Dynamic Facial Expression Database\n\n\nZhang, Yin, Cohn, Canavan, Reale, Horowitz, Liu & Girard\n\nImage and Vision Computing\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nFacial expression is central to human experience. Its efficiency and valid measurement are challenges that automated facial image analysis seeks to address. Most publically available databases are limited to 2D static images or video of posed facial behavior. Because posed and un-posed (aka ``spontaneous'') facial expressions differ along several dimensions including complexity and timing,well-annotated video of un-posed facial behavior is needed. Moreover, because the face is a three-dimensional deformable object, 2D video may be insufficient, and therefore 3D video archives are required.We present a newly developed 3D video database of spontaneous facial expressions in a diverse group of young adults. Well-validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication. Frame-level ground-truth for facial actions was obtained using the Facial Action Coding System. Facial features were tracked in both 2D and 3D domains. To the best of our knowledge, this newdatabase is the first of its kind for the public. Thework promotes the exploration of 3D spatiotemporal features in subtle facial expression, better understanding of the relation between pose and motion dynamics in facial action units, and deeper understanding of naturally occurring facial action.\n@article{zhang2014,\n  title = {{{BP4D-spontaneous}}: A High-Resolution Spontaneous {{3D}} Dynamic Facial Expression Database},\n  author = {Xing Zhang and Lijun Yin and Jeffrey F. Cohn and Shaun Canavan and Michael Reale and Andy Horowitz and Peng Liu and Jeffrey M. Girard},\n  year = {2014},\n  journal = {Image and Vision Computing},\n  volume = {32},\n  number = {10},\n  pages = {692--706},\n  doi = {10.1016/j.imavis.2014.06.002}\n}\n\n\n\n\nCARMA: Software for Continuous Affect Rating and Media Annotation\n\n\nGirard\n\nJournal of Open Research Software\n\n\nAbstract\nCitation\nBibTeX\nPreprint\nMaterials\n\n\nCopy\nCopied!\n—\n\n\n\nCARMA is a media annotation program that collects continuous ratings while displaying audio and video files. It is designed to be highly user-friendly and easily customizable. Based on Gottman and Levenson's affect rating dial, CARMA enables researchers and study participants to provide moment-by-moment ratings of multimedia files using a computer mouse or keyboard. The rating scale can be configured on a number of parameters including the labels for its upper and lower bounds, its numerical range, and its visual representation. Annotations can be displayed alongside the multimedia file and saved for easy import into statistical analysis software. CARMA provides a tool for researchers in affective computing, human-computer interaction, and the social sciences who need to capture the unfolding of subjective experience and observable behavior over time.\n@article{girard2014b,\n  title = {{{CARMA}}: Software for Continuous Affect Rating and Media Annotation},\n  author = {Jeffrey M. Girard},\n  year = {2014},\n  journal = {Journal of Open Research Software},\n  volume = {2},\n  number = {1},\n  pages = {e5-e5},\n  doi = {10.5334/jors.ar}\n}\n\n\n\n\nNonverbal Social Withdrawal in Depression: Evidence from Manual and Automatic Analyses\n\n\nGirard, Cohn, Mahoor, Mavadati, Hammal & Rosenwald\n\nImage and Vision Computing\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nThe relationship between nonverbal behavior and severity of depression was investigated by following depressed participants over the course of treatment and video recording a series of clinical interviews. Facial expressions and head pose were analyzed from video using manual and automatic systems. Both systems were highly consistent for FACS action units (AUs) and showed similar effects for change over time in depression severity. When symptom severity was high, participants made fewer affiliative facial expressions (AUs 12 and 15) and more non-affiliative facial expressions (AU 14). Participants also exhibited diminished head motion (i.e., amplitude and velocity) when symptom severity was high. These results are consistent with the Social Withdrawal hypothesis: that depressed individuals use nonverbal behavior to maintain or increase interpersonal distance. As individuals recover, they send more signals indicating a willingness to affiliate. The finding that automatic facial expression analysis was both consistent with manual coding and revealed the same pattern of findings suggests that automatic facial expression analysis may be ready to relieve the burden of manual coding in behavioral and clinical science.\n@article{girard2014,\n  title = {Nonverbal Social Withdrawal in Depression: {{Evidence}} from Manual and Automatic Analyses},\n  author = {Jeffrey M. Girard and Jeffrey F. Cohn and Mohammad H. Mahoor and S. Mohammad Mavadati and Zakia Hammal and Dean P. Rosenwald},\n  year = {2014},\n  journal = {Image and Vision Computing},\n  volume = {32},\n  number = {10},\n  pages = {641--647},\n  doi = {10.1016/j.imavis.2013.12.007}\n}\n\n\n\n\nPerceptions of Interpersonal Behavior Are Influenced by Gender, Facial Expression Intensity, and Head Pose\n\n\nGirard\n\nProceedings of the 2014 International Conference on Multimodal Interaction\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nAcross multiple channels, nonverbal behavior communicates information about affective states and interpersonal intentions. Researchers interested in understanding how these nonverbal messages are transmitted and interpreted have examined the relationship between behavior and ratings of interpersonal motives using dimensions such as agency and communion. However, previous work has focused on images of posed behavior and it is unclear how well these results will generalize to more dynamic representations of real-world behavior. The current study proposes to extend the current literature by examining how gender, facial expression intensity, and head pose influence interpersonal ratings in videos of spontaneous nonverbal behavior.\n@inproceedings{girard2014a,\n  title = {Perceptions of Interpersonal Behavior Are Influenced by Gender, Facial Expression Intensity, and Head Pose},\n  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Multimodal Interaction}}},\n  author = {Jeffrey M. Girard},\n  year = {2014},\n  pages = {394--398},\n  doi = {10.1145/2663204.2667575}\n}\n\n\n\n\n\n2013\n\n\n\nContinuous AU Intensity Estimation Using Localized, Sparse Facial Feature Space\n\n\nJeni, Girard, Cohn & De la Torre\n\nProceedings of the 10th IEEE International Conference on Automated Face and Gesture Recognition (FG)\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nMost work in automatic facial expression analysis seeks to detect discrete facial actions. Yet, the meaning and function of facial actions often depends in part on their intensity. We propose a part-based, sparse representation for automated measurement of continuous variation in AU intensity. We evaluated its effectiveness in two publically available databases, CK+ and the soon to be released Binghamton high-resolution spontaneous 3D dyadic facial expression database. The former consists of posed facial expressions and ordinal level intensity (absent, low, and high). The latter consists of spontaneous facial expression in response to diverse, well-validated emotion inductions, and 6 ordinal levels of AU intensity. In a preliminary test, we started from discrete emotion labels and ordinal-scale intensity annotation in the CK+ dataset. The algorithm achieved state-of-the-art performance. These preliminary results supported the utility of the part-based, sparse representation. Second, we applied the algorithm to the more demanding task of continuous AU intensity estimation in spontaneous facial behavior in the Binghamton database. Manual 6-point ordinal coding and continuous measurement were highly consistent. Visual analysis of the overlay of continuous measurement by the algorithm and manual ordinal coding strongly supported the representational power of the proposed method to smoothly interpolate across the full range of AU intensity.\n@inproceedings{jeni2013,\n  title = {Continuous {{AU}} Intensity Estimation Using Localized, Sparse Facial Feature Space},\n  booktitle = {Proceedings of the 10th {{IEEE International Conference}} on {{Automated Face}} and {{Gesture Recognition}} ({{FG}})},\n  author = {L{\\a'a}szl{\\a'o} A Jeni and Jeffrey M. Girard and Jeffrey F. Cohn and Fernando {De la Torre}},\n  year = {2013},\n  pages = {1--7},\n  publisher = {IEEE},\n  address = {Shanghai, China},\n  doi = {10.1109/fg.2013.6553808}\n}\n\n\n\n\nSocial Risk and Depression: Evidence from Manual and Automatic Facial Expression Analysis\n\n\nGirard, Cohn, Mahoor, Mavadati & Rosenwald\n\nProceedings of the 10th IEEE International Conference on Automatic Face and Gesture Recognition (FG)\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nInvestigated the relationship between change over time in severity of depression symptoms and facial expression. Depressed participants were followed over the course of treatment and video recorded during a series of clinical interviews. Facial expressions were analyzed from the video using both manual and automatic systems. Automatic and manual coding were highly consistent for FACS action units, and showed similar effects for change over time in depression severity. For both systems, when symptom severity was high, participants made more facial expressions associated with contempt, smiled less, and those smiles that occurred were more likely to be accompanied by facial actions associated with contempt. These results are consistent with the ``social risk hypothesis'' of depression. According to this hypothesis, when symptoms are severe, depressed participants withdraw from other people in order to protect themselves from anticipated rejection, scorn, and social exclusion. As their symptoms fade, participants send more signals indicating a willingness to affiliate. The finding that automatic facial expression analysis was both consistent with manual coding and produced the same pattern of depression effects suggests that automatic facial expression analysis may be ready for use in behavioral and clinical science.\n@inproceedings{girard2013,\n  title = {Social Risk and Depression: {{Evidence}} from Manual and Automatic Facial Expression Analysis},\n  booktitle = {Proceedings of the 10th {{IEEE International Conference}} on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},\n  author = {Jeffrey M. Girard and Jeffrey F. Cohn and Mohammad H. Mahoor and S Mohammad Mavadati and Dean P. Rosenwald},\n  year = {2013},\n  pages = {1--8},\n  publisher = {IEEE},\n  address = {Shanghai, China},\n  doi = {10.1109/fg.2013.6553748}\n}\n\n\n\n\n\n2011\n\n\n\nCriteria and Metrics for Thresholded AU Detection\n\n\nGirard & Cohn\n\nProceedings of the IEEE International Conference on Computer Vision Workshops (ICCV Workshops)\n\n\nAbstract\nCitation\nBibTeX\n\n\nCopy\nCopied!\n—\n\n\n\nImplementing a computerized facial expression analysis system for automatic coding requires that a threshold for the system's classifier outputs be selected. However, there are many potential ways to select a threshold. How do different criteria and metrics compare? Manually FACS coded video of 45 clinical interviews (Spectrum dataset) were processed using person-specific active appearance models (AAM). Support vector machine (SVM) classifiers were trained using an independent dataset (RU-FACS). Spectrum sessions were randomly assigned to training (n=32) and testing sets (n=13). Six different threshold selection criteria were compared for automatic A U coding. Three major findings emerged: 1) Thresholds that attempt to balance the confusion matrix (using kappa, F1, or MCC) performed significantly better on all metrics than thresholds that select arbitrary error or accuracy rates (such as TPR, FPR, or EER). 2) AU detection scores for kappa, F1, and MCC were highly intercorrelated; accuracy was uncorrelated with the others. And 3) Kappa, MCC, and Fl were all positively correlated with base rate. They increased with increases in AU base rates. Accuracy, by contrast, showed the opposite pattern. It was strongly negatively correlated with base rate. These findings suggest that better automatic coding can be obtained by using threshold-selection criteria that balance the confusion matrix and benefit from increased AU base rates in the training data.\n@inproceedings{girard2011,\n  title = {Criteria and Metrics for Thresholded {{AU}} Detection},\n  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision Workshops}} ({{ICCV Workshops}})},\n  author = {Jeffrey M. Girard and Jeffrey F. Cohn},\n  year = {2011},\n  pages = {2191--2197},\n  publisher = {IEEE},\n  address = {Barcelona, Spain},\n  doi = {10.1109/iccvw.2011.6130519}\n}\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nAafjes-van Doorn, K., & Girard, J. M. (2025). From intuition to innovation: Empirical illustrations of multimodal measurement in psychotherapy research. Psychotherapy Research, 35(2), 171–173. https://doi.org/10/g824tc\n\n\nAdaryukov, J., Biernat, M., Girard, J. M., Villicana, A. J., & Pleskac, T. J. (2025). Worth the weight: An examination of unstructured and structured data in graduate admissions. Decision, 12(1), 4–30. https://doi.org/10.1037/dec0000251\n\n\nAgrawal, V., Akinyemi, A., Alvero, K., Behrooz, M., Buffalini, J., Carlucci, F. M., Chen, J., Chen, J., Chen, Z., Cheng, S., Chowdary, P., Chuang, J., D’Avirro, A., Daly, J., Dong, N., Duppenthaler, M., Gao, C., Girard, J., Gleize, M., … Zollhoefer, M. (2025). Seamless interaction: Dyadic audiovisual motion modeling and large-scale dataset. arXiv:2506.22554 [cs.CV]. https://doi.org/10.48550/arXiv.2506.22554\n\n\nBaber, G. R., Hamilton, N. A., Girard, J. M., Cohen, J. M., Gratton, M. K. P., Ellis, S., & Hemmer, E. (2024). It’s the sentiment that counts: Comparing sentiment analysis tools for estimating affective valence in dream reports. Sleep, 47(12), zsae210. https://doi.org/10.1093/sleep/zsae210\n\n\nBowdring, M. A., Sayette, M. A., Girard, J. M., & Woods, W. C. (2021). In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions. Journal of Nonverbal Behavior, 45(2), 241–259. https://doi.org/10.1007/s10919-020-00350-2\n\n\nButler, R. M., Christian, C., Girard, J. M., Vanzhula, I. A., & Levinson, C. A. (2024). Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of exposure for eating disorders. Behaviour Research and Therapy, 180, 104577. https://doi.org/10.1016/j.brat.2024.104577\n\n\nCampbell, C., Girard, J. M., McDuff, D., & Rosengren, S. (2025). EXPRESS: How influencers grow: An empirical study and future research agenda. Journal of Interactive Marketing, 10949968251360683. https://doi.org/10.1177/10949968251360683\n\n\nCaumiant, E. P., Kang, D., Girard, J. M., & Fairbairn, C. E. (2025). Alcohol and emotion: Analyzing convergence between facially expressed and self-reported indices of emotion under alcohol intoxication. Psychology of Addictive Behaviors. https://doi.org/10.1037/adb0001053\n\n\nChung, Y., Girard, J. M., Ravichandran, C., Öngür, D., Cohen, B. M., & Baker, J. T. (2025). Transdiagnostic modeling of clinician-rated symptoms in affective and nonaffective psychotic disorders. Journal of Psychopathology and Clinical Science, 134(1), 81–96. https://doi.org/10/g8pwmn\n\n\nCohn, J. F., Ertugrul, I. O., Chu, W.-S., Girard, J. M., & Hammal, Z. (2018). Affective facial computing: Generalizability across domains. In X. Alameda-Pineda, E. Ricci, & N. Sebe (Eds.), Multimodal behavior analysis in the wild: Advances and challenges (pp. 407–441). Academic Press.\n\n\nCreswell, K. G., Wright, A. G. C., Sayette, M. A., Girard, J. M., Lyons, G., & Smyth, J. M. (2025). The effects of alcohol in groups of heavy-drinking young adults: A multimodal investigation of alcohol responses in a laboratory social setting. Clinical Psychological Science, 21677026251333784. https://doi.org/10.1177/21677026251333784\n\n\nEdershile, E. A., Girard, J. M., Woods, W. C., Williams, T. F., Simms, L. J., & Wright, A. G. C. (2025). Narcissism from every angle: An interpersonal analysis of narcissism in young adults. Assessment, 10731911251356150. https://doi.org/10.1177/10731911251356150\n\n\nFairbairn, C. E., Sayette, M. A., Amole, M. C., Dimoff, J. D., Cohn, J. F., & Girard, J. M. (2015). Speech volume indexes sex differences in the social-emotional effects of alcohol. Experimental and Clinical Psychopharmacology, 23(4), 255–264. https://doi.org/10.1037/pha0000021\n\n\nGerber, A. H., Girard, J. M., Scott, S. B., & Lerner, M. D. (2019). Alexithymia – not autism – is associated with frequency of social interactions in adults. Behaviour Research and Therapy, 123, 103477. https://doi.org/10.1016/j.brat.2019.103477\n\n\nGirard, J. M. (2014a). CARMA: Software for continuous affect rating and media annotation. Journal of Open Research Software, 2(1), e5–e5. https://doi.org/10.5334/jors.ar\n\n\nGirard, J. M. (2014b). Perceptions of interpersonal behavior are influenced by gender, facial expression intensity, and head pose. Proceedings of the 2014 International Conference on Multimodal Interaction, 394–398. https://doi.org/10.1145/2663204.2667575\n\n\nGirard, J. M. (2017). Open-source software for continuous measurement and media annotation. Proceedings of the 12th IEEE International Conference on Automated Face and Gesture Recognition (FG), 31, 995–995. https://doi.org/10.1109/fg.2017.151\n\n\nGirard, J. M. (2025). Building open science into graduate training in clinical psychology. Clinical Psychological Science. https://doi.org/osf.io/preprints/psyarxiv/jeuyk\n\n\nGirard, J. M., Chu, W.-S., Jeni, L. A., Cohn, J. F., De La Torre, F., & Sayette, M. A. (2017). Sayette group formation task (GFT) spontaneous facial expression database. Proceedings of the 12th IEEE International Conference on Automatic Face and Gesture Recognition (FG), 581–588. https://doi.org/10.1109/fg.2017.144\n\n\nGirard, J. M., & Cohn, J. F. (2011). Criteria and metrics for thresholded AU detection. Proceedings of the IEEE International Conference on Computer Vision Workshops (ICCV Workshops), 2191–2197. https://doi.org/10.1109/iccvw.2011.6130519\n\n\nGirard, J. M., & Cohn, J. F. (2015). Automated audiovisual depression analysis. Current Opinion in Psychology, 4, 75–79. https://doi.org/10.1016/j.copsyc.2014.12.010\n\n\nGirard, J. M., & Cohn, J. F. (2016). A primer on observational measurement. Assessment, 23(4), 404–413. https://doi.org/10.1177/1073191116635807\n\n\nGirard, J. M., Cohn, J. F., & De la Torre, F. (2015). Estimating smile intensity: A better way. Pattern Recognition Letters, 66, 13–21. https://doi.org/10/f7tkjg\n\n\nGirard, J. M., Cohn, J. F., Jeni, L. A., Lucey, S., & De la Torre, F. (2015). How much training data for facial action unit detection? Proceedings of the 11th IEEE International Conference on Automatic Face and Gesture Recognition (FG), 1–8. https://doi.org/10.1109/fg.2015.7163106\n\n\nGirard, J. M., Cohn, J. F., Jeni, L. A., Sayette, M. A., & De la Torre, F. (2015). Spontaneous facial expression in unscripted social interactions can be measured automatically. Behavior Research Methods, 47(4), 1136–1147. https://doi.org/10.3758/s13428-014-0536-1\n\n\nGirard, J. M., Cohn, J. F., Mahoor, M. H., Mavadati, S. M., Hammal, Z., & Rosenwald, D. P. (2014). Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses. Image and Vision Computing, 32(10), 641–647. https://doi.org/10.1016/j.imavis.2013.12.007\n\n\nGirard, J. M., Cohn, J. F., Mahoor, M. H., Mavadati, S. M., & Rosenwald, D. P. (2013). Social risk and depression: Evidence from manual and automatic facial expression analysis. Proceedings of the 10th IEEE International Conference on Automatic Face and Gesture Recognition (FG), 1–8. https://doi.org/10.1109/fg.2013.6553748\n\n\nGirard, J. M., Cohn, J. F., Yin, L., & Morency, L.-P. (2021). Reconsidering the Duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion. Affective Science, 2(1), 32–47. https://doi.org/10.1007/s42761-020-00030-w\n\n\nGirard, J. M., & McDuff, D. (2017). Historical heterogeneity predicts smiling: Evidence from large-scale observational analyses. Proceedings of the 12th IEEE International Conference on Automatic Face and Gesture Recognition (FG), 719–726. https://doi.org/10.1109/fg.2017.135\n\n\nGirard, J. M., Shandar, G., Liu, Z., Cohn, J. F., Yin, L., & Morency, L.-P. (2019). Reconsidering the duchenne smile: Indicator of positive emotion or artifact of smile intensity? Proceedings of the 8th International Conference on Affective Computing and Intelligent Interaction (ACII), 594–599. https://doi.org/10.1109/acii.2019.8925535\n\n\nGirard, J. M., Tie, Y., & Liebenthal, E. (2023). DynAMoS: The dynamic affective movie clip database for subjectivity analysis. Proceedings of the 11th International Conference on Affective Computing and Intelligent Interaction (ACII), 1–8. https://doi.org/10.1109/ACII59096.2023.10388135\n\n\nGirard, J. M., Vail, A. K., Liebenthal, E., Brown, K., Kilciksiz, C. M., Pennant, L., Liebson, E., Öngür, D., Morency, L.-P., & Baker, J. T. (2022). Computational analysis of spoken language in acute psychosis and mania. Schizophrenia Research, 245, 97–115. https://doi.org/10.1016/j.schres.2021.06.040\n\n\nGirard, J. M., & Wright, A. G. C. (2018). DARMA: Software for dual axis rating and media annotation. Behavior Research Methods, 50(3), 902–909. https://doi.org/10.3758/s13428-017-0915-5\n\n\nGirard, J. M., Wright, A. G. C., Beeney, J. E., Lazarus, S. A., Scott, L. N., Stepp, S. D., & Pilkonis, P. A. (2017). Interpersonal problems across levels of the psychopathology hierarchy. Comprehensive Psychiatry, 79, 53–69. https://doi.org/10.1016/j.comppsych.2017.06.014\n\n\nGirard, J. M., Yermol, D. A., Bylsma, L. M., Cohn, J. F., Fournier, J. C., Morency, L.-P., & Swartz, H. A. (2025). Dynamic and dyadic relationships between facial behavior, working alliance, and treatment outcomes during depression therapy. Journal of Consulting and Clinical Psychology, 93(11), 749–760. https://doi.org/10.1037/ccp0000980\n\n\nGirard, J. M., Yermol, D. A., Salah, A. A., & Cohn, J. F. (2025). Computational analysis of expressive behavior in clinical assessment. Annual Review of Clinical Psychology. https://doi.org/10.1146/annurev-clinpsy-081423-024140\n\n\nGrove, J. L., Smith, T. W., Girard, J. M., & Wright, A. G. (2019). Narcissistic admiration and rivalry: An interpersonal approach to construct validation. Journal of Personality Disorders, 33(6), 751–775. https://doi.org/10.1521/pedi_2019_33_374\n\n\nHopwood, C. J., Harrison, A. L., Amole, M. C., Girard, J. M., Wright, A. G. C., Thomas, K. M., Sadler, P., Ansell, E. B., Chaplin, T. M., Morey, L. C., Crowley, M. J., Durbin, C. E., & Kashy, D. A. (2020). Properties of the continuous assessment of interpersonal dynamics across sex, level of familiarity, and interpersonal conflict. Assessment, 27(1), 40–56. https://doi.org/10.1177/1073191118798916\n\n\nJeni, L. A., Girard, J. M., Cohn, J. F., & De la Torre, F. (2013). Continuous AU intensity estimation using localized, sparse facial feature space. Proceedings of the 10th IEEE International Conference on Automated Face and Gesture Recognition (FG), 1–7. https://doi.org/10.1109/fg.2013.6553808\n\n\nJeni, L. A., Girard, J. M., Cohn, J. F., & Kanade, T. (2015). Real-time dense 3D face alignment from 2D video with automatic facial action unit coding. IEEE International Conference and Workshops on Automatic Face and Gesture Recognition. https://doi.org/10.1109/fg.2015.7163165\n\n\nJun, D., Girard, J. M., Martin, C. K., & Fazzino, T. L. (2025). The role of hyper-palatable foods in energy intake measured using mobile food photography methodology. Eating Behaviors, 57, 101983. https://doi.org/10.1016/j.eatbeh.2025.101983\n\n\nKebe, G. Y., Birlikci, M. D., Boudin, A., Ishii, R., Girard, J. M., & Morency, L.-P. (2024). GeSTICS: A multimodal corpus for studying gesture synthesis in two-party interactions with contextualized speech. Proceedings of the 24th ACM International Conference on Intelligent Virtual Agents, 1–10. https://doi.org/10.1145/3652988.3673917\n\n\nKim, H., Küster, D., Girard, J. M., & Krumhuber, E. G. (2023). Human and machine recognition of dynamic and static facial expressions: Prototypicality, ambiguity, and complexity. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1221081\n\n\nL’Insalata, A. M., Girard, J. M., & Fazzino, T. L. (2024). Sources of environmental reinforcement and engagement in health risk behaviors among a general population sample of US adults. International Journal of Environmental Research and Public Health, 21(11), 1390. https://doi.org/10.3390/ijerph21111390\n\n\nLin, V., Girard, J. M., & Morency, L.-P. (2020). Context-dependent models for predicting and characterizing facial expressiveness. Proceedings of the 3rd Workshop on Affective Content Analysis Co-Located with the 34th AAAI Conference on Artificial Intelligence, 2614, 11–28.\n\n\nLin, V., Girard, J. M., Sayette, M. A., & Morency, L.-P. (2020). Toward multimodal modeling of emotional expressiveness. Proceedings of the 22nd International Conference on Multimodal Interaction, 548–557. https://doi.org/10.1145/3382507.3418887\n\n\nMcDuff, D., & Girard, J. M. (2019). Democratizing psychological insights from analysis of nonverbal behavior. Proceedings of the 8th International Conference on Affective Computing and Intelligent Interaction (ACII), 220–226. https://doi.org/10.1109/acii.2019.8925503\n\n\nMcDuff, D., Girard, J. M., & El Kaliouby, R. (2017). Large-scale observational evidence of cross-cultural differences in facial behavior. Journal of Nonverbal Behavior, 41(1), 1–19. https://doi.org/10/f92t52\n\n\nMuszynski, M., Zelazny, J., Girard, J. M., & Morency, L.-P. (2020). Depression severity assessment for adolescents at high risk of mental disorders. Proceedings of the 22nd International Conference on Multimodal Interaction, 70–78. https://doi.org/10.1145/3382507.3418859\n\n\nPacella, M. L., Girard, J. M., Wright, A. G. C., Suffoletto, B., & Callaway, C. W. (2018). The association between daily posttraumatic stress symptoms and pain over the first 14 days after injury: An experience sampling study. Academic Emergency Medicine, 25(8), 844–855. https://doi.org/10.1111/acem.13406\n\n\nRincon Caicedo, M., Girard, J. M., Punt, S. E., Giovanetti, A. K., & Ilardi, S. S. (2025). Depressive symptoms among hispanic americans: Investigating the interplay of acculturation and demographics. Journal of Latinx Psychology, 13(1), 68–84. https://doi.org/10.1037/lat0000266\n\n\nRoss, J. M., Girard, J. M., Wright, A. G. C., Beeney, J. E., Scott, L. N., Hallquist, M. N., Lazarus, S. A., Stepp, S. D., & Pilkonis, P. A. (2017). Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment. Psychological Assessment, 29(2), 123–134. https://doi.org/10.1037/pas0000338\n\n\nSetiawan, D., Wiranto, Y., Girard, J. M., Watts, A., & Ashourvan, A. (2025). Individualized machine-learning-based clinical assessment recommendation system. PLOS Digital Health, 4(9), e0001022. https://doi.org/10.1371/journal.pdig.0001022\n\n\nSewall, C. J. R., Girard, J. M., Merranko, J., Hafeman, D., Goldstein, B. I., Strober, M., Hower, H., Weinstock, L. M., Yen, S., Ryan, N. D., Keller, M. B., Liao, F., Diler, R. S., Gill, M. K., Axelson, D., Birmaher, B., & Goldstein, T. R. (2021). A Bayesian multilevel analysis of the longitudinal associations between relationship quality and suicidal ideation and attempts among youth with bipolar disorder. The Journal of Child Psychology and Psychiatry, 62(7), 905–9115. https://doi.org/10.1111/jcpp.13343\n\n\nShepherd, L. M., Sly, K. F., & Girard, J. M. (2017). Comparison of comprehensive and abstinence-only sexuality education in young african american adolescents. Journal of Adolescence, 61, 50–63. https://doi.org/10.1016/j.adolescence.2017.09.006\n\n\nSprunger, J. G., Girard, J. M., & Chard, K. M. (2024). Associations between transdiagnostic traits of psychopathology and hybrid posttraumatic stress disorder factors in a trauma-exposed community sample. Journal of Traumatic Stress, 37(3), 384–396. https://doi.org/10.1002/jts.23023\n\n\nSwartz, H. A., Bylsma, L. M., Fournier, J. C., Girard, J. M., Spotts, C., Cohn, J. F., & Morency, L.-P. (2023). Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth. Journal of Affective Disorders, 333, 543–552. https://doi.org/10.1016/j.jad.2023.04.092\n\n\nVail, A. K., Girard, J. M., Bylsma, L. M., Cohn, J. F., Fournier, J., Swartz, H. A., & Morency, L.-P. (2022). Toward causal understanding of therapist-client relationships: A study of language modality and social entrainment. Proceedings of the 24th ACM International Conference on Multimodal Interaction, 487–494. https://doi.org/10/gt3bwj\n\n\nVail, A. K., Girard, J. M., Bylsma, L. M., Fournier, J., Swartz, H. A., Cohn, J. F., & Morency, L.-P. (2023). Representation learning for interpersonal and multimodal behavior dynamics: A multiview extension of latent change score models. Proceedings of the 25th International Conference on Multimodal Interaction, 517–526. https://doi.org/10.1145/3577190.3614118\n\n\nVail, A. K., Girard, J. M., Bylsma, L., Cohn, J. F., Fournier, J., Swartz, H. A., & Morency, L.-P. (2021). Goals, tasks, and bonds: Toward the computational assessment of therapist versus client perception of working alliance. Proceedings of the 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG), 1–8. https://doi.org/10/gpfjrn\n\n\nValstar, M. F., Almaev, T., Girard, J. M., McKeown, G., Mehu, M., Yin, L., Pantic, M., & Cohn, J. F. (2015). FERA 2015 - second facial expression recognition and analysis challenge. Proceedings of the 11th IEEE International Conference on Automatic Face and Gesture Recognition (FG), 1–8. https://doi.org/10.1109/fg.2015.7284874\n\n\nValstar, M. F., Sanchez-Lozano, E., Cohn, J. F., Jeni, L. A., Girard, J. M., Zhang, Z., Yin, L., & Pantic, M. (2017). FERA 2017 - addressing head pose in the third facial expression recognition and analysis challenge. Proceedings of the 12th IEEE Conference on Automatic Face and Gesture Recognition (FG), 839–847. https://doi.org/10.1109/fg.2017.107\n\n\nvan Oest, R., & Girard, J. M. (2022). Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement. Psychological Methods, 27(6), 1069–1088. https://doi.org/10.1037/met0000412\n\n\nWolfert, P., Girard, J. M., Kucherenko, T., & Belpaeme, T. (2021). To rate or not to rate: Investigating evaluation methods for generated co-speech gestures. Proceedings of the 23rd International Conference on Multimodal Interaction, 494–502. https://doi.org/10.1145/3462244.3479889\n\n\nZhang, X., Yin, L., Cohn, J. F., Canavan, S., Reale, M., Horowitz, A., Liu, P., & Girard, J. M. (2014). BP4D-spontaneous: A high-resolution spontaneous 3D dynamic facial expression database. Image and Vision Computing, 32(10), 692–706. https://doi.org/10.1016/j.imavis.2014.06.002\n\n\nZhang, Z., Girard, J. M., Wu, Y., Zhang, X., Liu, P., Ciftci, U., Canavan, S., Reale, M., Horowitz, A., Yang, H., Cohn, J. F., Ji, Q., & Yin, L. (2016). Multimodal spontaneous emotion corpus for human behavior analysis. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3438–3446. https://doi.org/10.1109/cvpr.2016.374"
  },
  {
    "objectID": "posts/whisper2024c/index.html",
    "href": "posts/whisper2024c/index.html",
    "title": "AI Transcription from R using Whisper: Part 3",
    "section": "",
    "text": "Note\n\n\n\nYou can view this blog post as a replacement for the previous one (unless you are interested in all the nitty-gritty details)."
  },
  {
    "objectID": "posts/whisper2024c/index.html#introduction",
    "href": "posts/whisper2024c/index.html#introduction",
    "title": "AI Transcription from R using Whisper: Part 3",
    "section": "Introduction",
    "text": "Introduction\nIn a previous blog post on AI Transcription, I discussed the performance benefits of moving to WSL2 for CUDA support on Windows. However, the process of setting up and using the virtual machine was very technical, which may be a barrier for many users. In this blog post, I will discuss the use of a technology called Docker to ease the setup and use of WSL2.\nDocker is a tool that makes it easy to package and run applications in a “container,” which is a lightweight, standalone environment that includes everything an application needs to run. This way, the application will work the same way on any computer without worrying about setup or compatibility issues.\nThere are three main components to Docker: a Dockerfile, a Docker image, and a Docker container. To explain these, I will use a pizza analogy. Just like a chef might create a recipe that contains step-by-step instructions for preparing a pizza, a power user might create a Dockerfile that contains detailed and reproducible instructions for building and configuring a software environment. This recipe might then be followed in a food processing plant to produce many identical pizzas, which are then frozen for re-sale. Similarly, the Dockerfile instructions might be followed to “build” a Docker image, which contains all the resulting files and settings. Finally, a customer might purchase a frozen pizza, bring it home, and reheat it in their own oven for consumption. In much the same way, a user might download or “pull” a Docker image from the internet and “run” it as a Docker container on their own computer to use the desired application. The benefit of this approach is that the customer/user doesn’t need to understand or even know about the recipe/Dockerfile instructions to enjoy the pizza/container!\nI have played the role of chef or power user here and condensed the instructions from my previous blog post into a Dockerfile and built it into a Docker image that you can easily pull and run.\nBefore we dive into things, I’ll provide a brief overview of all the steps.\n\nCheck that our computer’s hardware supports CUDA\nInstall/update the NVIDIA graphics driver on Windows\nInstall Docker Desktop on Windows\nPull the preconfigured Docker image\nRun the Docker image with GPU support\nLog into RStudio Server and use audio.whisper"
  },
  {
    "objectID": "posts/whisper2024c/index.html#check-for-cuda-support",
    "href": "posts/whisper2024c/index.html#check-for-cuda-support",
    "title": "AI Transcription from R using Whisper: Part 3",
    "section": "Check for CUDA Support",
    "text": "Check for CUDA Support\nThis post assumes that you are using the Windows operating system and that your computer’s graphics card supports CUDA. To check that this is the case, first look up your graphics card’s model number. An easy way to do this on Windows 10/11 is to click on the desktop search bar (bottom-left of the screen next to the windows icon) and type in “Device Manager.” Then click the arrow next to “Display adapters” and find your graphics card’s model name. On my computer, it says “NVIDIA GeForce RTX 2060.” Then go to this link and click the “CUDA-Enabled NVIDIA Quadro and NVIDIA RTX” and “CUDA-Enabled GeForce and TITAN Products” blocks to open their accordions. Then search for your graphics card’s model number (the left tables are for desktop cards and the right tables are for notebook cards). I found “GeForce RTX 2060” on the list under GeForce and TITAN Products with a compute capability of 7.5. Thus, my card is supported!"
  },
  {
    "objectID": "posts/whisper2024c/index.html#install-the-latest-nvidia-graphics-driver",
    "href": "posts/whisper2024c/index.html#install-the-latest-nvidia-graphics-driver",
    "title": "AI Transcription from R using Whisper: Part 3",
    "section": "Install the Latest NVIDIA Graphics Driver",
    "text": "Install the Latest NVIDIA Graphics Driver\nDownload and install the latest graphics driver for your card from NVIDIA. The previous step described how to find your graphics card model name, which you’ll need to navigate to. You should choose the Game Ready version. Note that you should not install the CUDA toolkit on Windows as doing so may confuse things and lead to issues later on (as Docker will install the CUDA toolkit for WSL)."
  },
  {
    "objectID": "posts/whisper2024c/index.html#install-the-latest-version-of-docker-desktop",
    "href": "posts/whisper2024c/index.html#install-the-latest-version-of-docker-desktop",
    "title": "AI Transcription from R using Whisper: Part 3",
    "section": "Install the Latest Version of Docker Desktop",
    "text": "Install the Latest Version of Docker Desktop\nDownload and install the latest version of Docker Desktop for Windows. If you are unsure of whether you have AMD64 or ARM64, open a Command Prompt window and enter echo %PROCESSOR_ARCHITECTURE%. If the installer asks you whether you want to install or use WSL2 integration, select Yes."
  },
  {
    "objectID": "posts/whisper2024c/index.html#pull-the-wsl-cuda-whisper-docker-image",
    "href": "posts/whisper2024c/index.html#pull-the-wsl-cuda-whisper-docker-image",
    "title": "AI Transcription from R using Whisper: Part 3",
    "section": "Pull the wsl-cuda-whisper Docker image",
    "text": "Pull the wsl-cuda-whisper Docker image\nOpen the Docker Desktop application and click the Terminal button on the bottom. If it asks you to confirm/enable this, click yes.\nIn the Docker terminal, enter docker pull jmgirard/wsl-whisper.\nThis will take some time and disk space to download. If you are concerned at all about security, you can see the Dockerfile instructions here; it just installs rocker/tidyverse (i.e., Ubuntu, R, RStudio Server, and the tidyverse R packages), and then ffmpeg, CUDA Toolkit, and the audio.* R packages."
  },
  {
    "objectID": "posts/whisper2024c/index.html#run-the-wsl-cuda-whisper-docker-image",
    "href": "posts/whisper2024c/index.html#run-the-wsl-cuda-whisper-docker-image",
    "title": "AI Transcription from R using Whisper: Part 3",
    "section": "Run the wsl-cuda-whisper Docker image",
    "text": "Run the wsl-cuda-whisper Docker image\nOnce the download is complete, you can run the image to access RStudio with audio.whisper and CUDA support. To run an image, we can use a command like docker run [options] image-name. We will use jmgirard/wsl-whisper as our image name, but we need to learn several options to get the most out of this.\n\n--gpus all tells Docker to grant the container access to our NVIDIA graphics card, which is necessary for us to make use of CUDA.\n-p 8787:8787 tells Docker to host the container’s RStudio Server on network port 8787, which will let us access it from a browser on Windows.\n-e PASSWORD=[password] tells Docker to set the password for the RStudio Server to whatever we replace [password] with (e.g., abc). The username will be rstudio.\n-v \"[winpath]:/win\" will make whatever folder we replace [winpath] with (e.g., C:\\Users\\jeffg) accessible to the container as /win (or whatever we put at the end).\n--rm tells Docker to delete the container after it is closed, which can help save space in the long run\n-it tells Docker to run the container “interactively” so that messages from the container will be shown in the Docker terminal, which can be helpful to tell when the RStudio server is ready for use\n\nPutting this all together, we can enter this into the Docker terminal:\ndocker run --gpus all -p 8787:8787 -e PASSWORD=abc -v \"C:\\Users\\jeffg:/win\" --rm -it jmgirard/wsl-whisper\nThe Docker terminal will show the progress of R installing the audio.whisper package (unfortunately this can’t be done ahead of time). You will know the server is ready for use when the terminal says “TTY detected.”"
  },
  {
    "objectID": "posts/whisper2024c/index.html#open-the-containers-rstudio-server-from-windows",
    "href": "posts/whisper2024c/index.html#open-the-containers-rstudio-server-from-windows",
    "title": "AI Transcription from R using Whisper: Part 3",
    "section": "Open the Container’s RStudio Server from Windows",
    "text": "Open the Container’s RStudio Server from Windows\nIn Docker Desktop, click the Containers tab on the left and click the “8787:8787” link next to your new container under the Ports column. This will open a web browser on Windows and direct it to http://localhost:8787 (or you could also just paste this into your favorite browser manually or even make a desktop shortcut for it). Enter rstudio as the Username and whatever you set as the Password above (e.g., abc in the example code)."
  },
  {
    "objectID": "posts/whisper2024c/index.html#use-rstudio-server-to-run-whisper",
    "href": "posts/whisper2024c/index.html#use-rstudio-server-to-run-whisper",
    "title": "AI Transcription from R using Whisper: Part 3",
    "section": "Use RStudio Server to Run Whisper",
    "text": "Use RStudio Server to Run Whisper\nIn the R console, load the audio.whisper package and try it out on the JFK clip that took so long to process in the previous blog post. Note that there will be one important change to the commands from before. This time, when we load the model using the whisper() function, we will add the use_gpu = TRUE argument.\n# Load the package from library\nlibrary(audio.whisper)\n\n# Download or load from file the desired model (with GPU support)\nmodel &lt;- whisper(\"base\", use_gpu = TRUE)\n\n# Construct file path to example audio file in package data\njfk &lt;- system.file(package = \"audio.whisper\", \"samples\", \"jfk.wav\")\n\n# Run English transcription using the downloaded whisper model\nout &lt;- predict(model, newdata = jfk, language = \"en\")\n\n# Print transcript\nout$data\n\n\n\n\n\nsegment\nsegment_offset\nfrom\nto\ntext\n\n\n\n\n1\n0\n00:00:00.000\n00:00:07.600\nAnd so my fellow Americans, ask not what your country can do for you,\n\n\n2\n0\n00:00:07.600\n00:00:10.600\nask what you can do for your country.\n\n\n\n\n\nThe results look good/the same as before, but check out the timing!\nInstead of 20.988 minutes, it took 0.007 minutes…\n\nout$timing\n## $transcription_start\n## [1] \"2024-11-10 12:34:19 CST\"\n## \n## $transcription_end\n## [1] \"2024-11-10 12:34:20 CST\"\n## \n## $transcription_duration\n## Time difference of 0.006911568 mins\n\nIf we wanted to save the transcript back to Windows, we could use the following approach. To save the entire list object generated by predict() for later use in R, we could use saveRDS(out, file = \"/win/jfk.rds\"). Or, to save just the transcript for human consumption, we could use write.csv(out$data, file = \"jfk.csv\")."
  },
  {
    "objectID": "posts/whisper2024c/index.html#wrap-up",
    "href": "posts/whisper2024c/index.html#wrap-up",
    "title": "AI Transcription from R using Whisper: Part 3",
    "section": "Wrap-up",
    "text": "Wrap-up\nThat wraps up this blog post. In the next part, I will discuss more practical aspects of using this technology. For example, I’ll talk about how to generate a list of audio/videos files on your hard drive (or elsewhere) and then iterate over them to create transcripts from many files all at once.\nPart 4 coming soon…"
  },
  {
    "objectID": "posts/whisper2024/index.html",
    "href": "posts/whisper2024/index.html",
    "title": "AI Transcription from R using Whisper: Part 1",
    "section": "",
    "text": "In much of my work, I study how people communicate through verbal and nonverbal behavior. To study verbal behavior, it is often necessary to generate transcripts, which are written records of the words that were spoken. Transcriptions can be done manually (i.e., by a person) and assisted through the use of behavioral annotation software like ELAN or ANVIL or subtitle generation and editing software like Aegisub or Subtitld. However, new tools based on artificial intelligence (AI) can be much more efficient and scalable, albeit at some cost to accuracy.\nIn this blog post, I will provide a tutorial on how to set up and use OpenAI’s free Whisper model to generate automatic transcriptions of audio files (either recorded originally as audio or extracted from video files). I will first show you how to quickly install the audio.whisper R package and transcribe an example file. However, the processing will be very slow and we can do much, much better if we offload some of the work to a dedicated graphics card, such as an Nvidia card with CUDA. Enabling this takes some technical work, especially on Windows, but is worth the investment if you plan to process a lot of files. This technical work will be described in Part 2.\n\n\n\n\n\n\nNote\n\n\n\nAlthough the Whisper model comes from OpenAI, the approach described here will actually run it locally, which means your audio files will not need to be sent to any third parties. This makes it usable for private and sensitive (e.g., patient) data!"
  },
  {
    "objectID": "posts/whisper2024/index.html#introduction",
    "href": "posts/whisper2024/index.html#introduction",
    "title": "AI Transcription from R using Whisper: Part 1",
    "section": "",
    "text": "In much of my work, I study how people communicate through verbal and nonverbal behavior. To study verbal behavior, it is often necessary to generate transcripts, which are written records of the words that were spoken. Transcriptions can be done manually (i.e., by a person) and assisted through the use of behavioral annotation software like ELAN or ANVIL or subtitle generation and editing software like Aegisub or Subtitld. However, new tools based on artificial intelligence (AI) can be much more efficient and scalable, albeit at some cost to accuracy.\nIn this blog post, I will provide a tutorial on how to set up and use OpenAI’s free Whisper model to generate automatic transcriptions of audio files (either recorded originally as audio or extracted from video files). I will first show you how to quickly install the audio.whisper R package and transcribe an example file. However, the processing will be very slow and we can do much, much better if we offload some of the work to a dedicated graphics card, such as an Nvidia card with CUDA. Enabling this takes some technical work, especially on Windows, but is worth the investment if you plan to process a lot of files. This technical work will be described in Part 2.\n\n\n\n\n\n\nNote\n\n\n\nAlthough the Whisper model comes from OpenAI, the approach described here will actually run it locally, which means your audio files will not need to be sent to any third parties. This makes it usable for private and sensitive (e.g., patient) data!"
  },
  {
    "objectID": "posts/whisper2024/index.html#quickstart-easy-setup-slow-processing",
    "href": "posts/whisper2024/index.html#quickstart-easy-setup-slow-processing",
    "title": "AI Transcription from R using Whisper: Part 1",
    "section": "Quickstart (easy setup, slow processing)",
    "text": "Quickstart (easy setup, slow processing)\n\nInstall dependencies\nI assume you already have R (and probably an IDE like RStudio) installed. Open this up and install the development version of the audio.whisper package from github.\n\n# Install remotes if you don't have it already\n# install.packages(\"remotes\") \n\n# Install audio.whisper from github\nremotes::install_github(\"bnosac/audio.whisper\")\n\n\n\nDownload whisper model\nLoad this new package and download one of the whisper models: \"tiny\", \"base\", \"small\", \"medium\", or \"large-v3\". Earlier entries on that list are smaller (to download and hold in RAM), faster, and less accurate whereas later entries are larger, slower, and more accurate. There are also English-only versions of all but the large model, which end in \".en\" as in \"base.en\", and these may be more efficient if you know that all speech will be in English. You can learn more about these models via ?whisper_download_model. For this tutorial, we will go with the \"base\" model.\n\n# Load package from library\nlibrary(audio.whisper)\n\n\n# Download or load from file the desired whisper model\nmodel &lt;- whisper(\"base\")\n## whisper_init_from_file_with_params_no_state: loading model from 'C:/GitHub/affcomlab/posts/whisper2024/ggml-base.bin'\n## whisper_model_load: loading model\n## whisper_model_load: n_vocab       = 51865\n## whisper_model_load: n_audio_ctx   = 1500\n## whisper_model_load: n_audio_state = 512\n## whisper_model_load: n_audio_head  = 8\n## whisper_model_load: n_audio_layer = 6\n## whisper_model_load: n_text_ctx    = 448\n## whisper_model_load: n_text_state  = 512\n## whisper_model_load: n_text_head   = 8\n## whisper_model_load: n_text_layer  = 6\n## whisper_model_load: n_mels        = 80\n## whisper_model_load: ftype         = 1\n## whisper_model_load: qntvr         = 0\n## whisper_model_load: type          = 2 (base)\n## whisper_model_load: adding 1608 extra tokens\n## whisper_model_load: n_langs       = 99\n## whisper_model_load:      CPU buffer size =   147.46 MB\n## whisper_model_load: model size    =  147.37 MB\n## whisper_init_state: kv self size  =   16.52 MB\n## whisper_init_state: kv cross size =   18.43 MB\n## whisper_init_state: compute buffer (conv)   =   14.86 MB\n## whisper_init_state: compute buffer (encode) =   85.99 MB\n## whisper_init_state: compute buffer (cross)  =    4.78 MB\n## whisper_init_state: compute buffer (decode) =   96.48 MB\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the larger models may take a while to download, so if you get an error that the download took longer than permitted, you can temporarily allow more time via: options(timeout = 300).\n\n\n\n\nTranscribe example file\nThe package comes with an example audio file in the proper format, which contains 11 seconds of a speech by John F. Kennedy Jr. Let’s load it from file using system.file() and then transcribe it using predict().\n\n# Construct file path to example audio file in package data\njfk &lt;- system.file(package = \"audio.whisper\", \"samples\", \"jfk.wav\")\n\n# Run English transcription using the downloaded whisper model\nout &lt;- predict(model, newdata = jfk, language = \"en\")\n\n# Print transcript\nout$data\n\n\n\n\n\n\nsegment\nsegment_offset\nfrom\nto\ntext\n\n\n\n\n1\n0\n00:00:00.000\n00:00:11.000\nAnd so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.\n\n\n\n\n\nThe results look good! But we can see how long this took by digging into the output object.\n\n# Examine the time elapsed to process this audio\nout$timing\n## $transcription_start\n## [1] \"2024-08-15 12:30:48 CDT\"\n## \n## $transcription_end\n## [1] \"2024-08-15 12:51:48 CDT\"\n## \n## $transcription_duration\n## Time difference of 20.98786 mins\n\nYikes, 21 minutes to process just 11 seconds of audio. That’s motivation to work on the CUDA version to speed things up. But before we move on to that, I’ll first show you how to extract audio from a video file and convert it to the format that Whisper wants.\n\n\nExtract and format audio\nDownload the example mlk.mp4 video file, which contains 12 seconds of a speech by Martin Luther King, Jr. This video contains an audio stream in AAC format with a sampling rate of 44.1 kHz. However, whisper requires audio files in WAV format with a sampling rate of 16 kHz. We can extract and convert it in one step using the av_audio_convert() function from the av package.\n\n# Install av package if you don't have it already\n# install.packages(\"av\")\n\n# Load package from library\nlibrary(av)\n\n# Extract and convert audio\nav_audio_convert(\n  \"mlk.mp4\", \n  output = \"mlk.wav\", \n  format = \"wav\", \n  sample_rate = 16000\n)\n## [1] \"C:\\\\GitHub\\\\affcomlab\\\\posts\\\\whisper2024\\\\mlk.wav\"\n\nNote that the process would have been identical if this had been an audio file in a different format rather than a video file - you would just replace the .mp4 file with the audio file (e.g., .mp3). Now let’s transcribe this and verify that our conversion worked.\n\n# Run English transcription using the downloaded whisper model\nout2 &lt;- predict(model, newdata = \"mlk.wav\", language = \"en\")\n\n# Print transcript\nout2$data\n\n\n\n\n\n\nsegment\nsegment_offset\nfrom\nto\ntext\n\n\n\n\n1\n0\n00:00:00.000\n00:00:02.000\nI have a dream.\n\n\n2\n0\n00:00:02.000\n00:00:12.000\nBut one day, this nation will rise up, live up the true meaning of its creed.\n\n\n\n\n\nNot perfect (swapped “that” for “but” and omitted an “and”) but pretty good. And this only the base model - it might do better with a larger model, but for time’s sake I’ll leave that until after we get CUDA working in Part 2.\n\nContinue to Part 2  »"
  },
  {
    "objectID": "posts/tips2024/index.html",
    "href": "posts/tips2024/index.html",
    "title": "TIPS 2024 Conference",
    "section": "",
    "text": "Daiil and I are attending the 2024 TIPS Conference in Phoenix this weekend. His presentation is below. See you there!\n\nJun, D., Girard, J. M., Chung, Y., Liebenthal, E., & Baker, J. T. (2024). Longitudinal analysis of speaking rate in patients with serious mental illness using daily diaries and dynamic structural equation modeling. In J. M. Girard & E. Liebenthal (Chairs), Computational analysis and modeling of transdiagnostic psychiatric speech samples [Symposium]. Technology in Psychiatry Summit (TIPS), Phoenix, AZ."
  },
  {
    "objectID": "posts/sas2024/index.html#additional-figures-below",
    "href": "posts/sas2024/index.html#additional-figures-below",
    "title": "SAS 2024 Conference",
    "section": "Additional Figures Below",
    "text": "Additional Figures Below\n\nHistograms of Variables:"
  },
  {
    "objectID": "posts/sas2024/index.html#figures-including-all-sessions-and-patients-without-clustering",
    "href": "posts/sas2024/index.html#figures-including-all-sessions-and-patients-without-clustering",
    "title": "SAS 2024 Conference",
    "section": "Figures including all sessions and patients (without clustering):",
    "text": "Figures including all sessions and patients (without clustering):\n\n\nShow the code\ndata %&gt;% \n  ggplot(mapping = aes(x = fisher_z, y = wai)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, fullrange = TRUE) +\n  labs(\n    x = \"Smile Synchrony\",\n    y = \"Working Alliance Scores\",\n    title = \"Working Alliance by Smile Synchrony\"\n  )\n\n\n\n\n\n\n\n\n\n\nIn this figure, each dot represents the relationship between working alliance scores and smile synchrony (including all sessions from patients)"
  },
  {
    "objectID": "posts/sas2024/index.html#section",
    "href": "posts/sas2024/index.html#section",
    "title": "SAS 2024 Conference",
    "section": "",
    "text": "Show the code\ndata %&gt;% \n  ggplot(mapping = aes(x = fisher_z, y = wai, colour = factor(patient_id))) +\n  geom_point() +\n  geom_smooth(mapping = aes(group = patient_id), method = \"lm\", se = FALSE, fullrange = TRUE) +\n  labs(\n    colour = \"Patient ID\",\n    x = \"Smile Synchrony\",\n    y = \"Working Alliance Scores\",\n    title = \"Working Alliance by Smile Synchrony\"\n    )\n\n\n\n\n\n\n\n\n\n\nIn this figure, each line represents the relationship between working alliance scores and smile synchrony for each patient. This figure includes data from all sessions."
  },
  {
    "objectID": "posts/sas2024/index.html#figures-split-by-individual-patients",
    "href": "posts/sas2024/index.html#figures-split-by-individual-patients",
    "title": "SAS 2024 Conference",
    "section": "Figures split by individual patients:",
    "text": "Figures split by individual patients:\n\n\nShow the code\ndata %&gt;% \n  group_by(patient_id) %&gt;%\n  ggplot(mapping = aes(x = fisher_z, y = wai)) + \n  geom_point() +\n  geom_smooth(mapping = aes(group = patient_id), method = \"lm\", se = FALSE, fullrange = TRUE) +\n  labs(title = \"Working Alliance by Smile Synchrony for each Patient\",\n       x = \"Smile Synchrony\",\n       y = \"Working Alliance Scores\") +\n  scale_x_continuous(breaks=seq(from=0.2,to=0.4,by=0.1)) +\n  coord_cartesian(xlim = c(0.2, 0.4), ylim = c(0, 70)) +\n  theme_gray(base_size = 12) +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  facet_wrap(~patient_id, ncol = 8) +\n  theme(panel.spacing = grid::unit(1, \"lines\"))\n\n\n\n\n\n\n\n\n\n\nEach subfigure represents the relationship between working alliance scores and smile synchrony for each patient. This figure includes data from all sessions, but we have some missing data (e.g., patient #1025, patient #1072)."
  },
  {
    "objectID": "posts/sas2024/index.html#section-2",
    "href": "posts/sas2024/index.html#section-2",
    "title": "SAS 2024 Conference",
    "section": "",
    "text": "Show the code\ndata %&gt;% \n  group_by(session) %&gt;%\n  ggplot(mapping = aes(x = session, y = wai)) + \n  coord_cartesian(ylim = c(20, 65)) +\n  geom_point() +\n  geom_smooth(mapping = aes(group = patient_id), method = \"lm\", se = FALSE, fullrange = TRUE) +\n  labs(title = \"Working Alliance Scores across Sessions\",\n       x = \"Session\",\n       y = \"Working Alliance Scores\") +\n  theme_gray(base_size = 12) +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  facet_wrap(~patient_id)"
  },
  {
    "objectID": "posts/sas2024/index.html#section-3",
    "href": "posts/sas2024/index.html#section-3",
    "title": "SAS 2024 Conference",
    "section": "",
    "text": "Show the code\ndata %&gt;% \n  group_by(session) %&gt;%\n  ggplot(mapping = aes(x = session, y = fisher_z)) +\n  geom_point() +\n  geom_smooth(mapping = aes(group = patient_id), method = \"lm\", se = FALSE, fullrange = TRUE) +\n  labs(title = \"Smile Synchrony across Sessions\",\n       x = \"Session\",\n       y = \"Smile Synchrony\") +\n  scale_y_continuous(breaks=seq(from=0.2,to=0.4,by=0.1)) +\n  theme_gray(base_size = 12) +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  facet_wrap(~patient_id)"
  },
  {
    "objectID": "posts/rowwise2023/index.html",
    "href": "posts/rowwise2023/index.html",
    "title": "Row-wise means in dplyr",
    "section": "",
    "text": "For those looking for a quick answer, here is an example of my recommended approach, which calculates a new variable, the mean fuel efficiency (mfe) of each car, as the row-wise mean of two existing variables, highway fuel efficiency (hwy) and city fuel efficiency (cty):\n\nlibrary(tidyverse) # needs dplyr version 1.1.0+\n\n# Calculate mean fuel efficiency (mfe) from highway (hwy) and city (cty)\nmpg |&gt; mutate(mfe = rowMeans(pick(hwy, cty)))"
  },
  {
    "objectID": "posts/rowwise2023/index.html#tldr",
    "href": "posts/rowwise2023/index.html#tldr",
    "title": "Row-wise means in dplyr",
    "section": "",
    "text": "For those looking for a quick answer, here is an example of my recommended approach, which calculates a new variable, the mean fuel efficiency (mfe) of each car, as the row-wise mean of two existing variables, highway fuel efficiency (hwy) and city fuel efficiency (cty):\n\nlibrary(tidyverse) # needs dplyr version 1.1.0+\n\n# Calculate mean fuel efficiency (mfe) from highway (hwy) and city (cty)\nmpg |&gt; mutate(mfe = rowMeans(pick(hwy, cty)))"
  },
  {
    "objectID": "posts/rowwise2023/index.html#introduction",
    "href": "posts/rowwise2023/index.html#introduction",
    "title": "Row-wise means in dplyr",
    "section": "Introduction",
    "text": "Introduction\ndplyr is an amazing tool for data wrangling and I use it daily. However, there is one type of operation that I frequently do that has historically caused me some confusion and frustration: row-wise means. Once I figured out what was going on, I wanted to share what I learned through this brief blog post. It will focus on how to avoid some common issues I ran into and how to speed up rowwise operations with large data frames. I hope some find it helpful.\n\n# Load packages used in this post\nlibrary(tidyverse)\nlibrary(microbenchmark)\n\n# Set random seed for reproduciblity\nset.seed(2023)"
  },
  {
    "objectID": "posts/rowwise2023/index.html#simulated-example-data",
    "href": "posts/rowwise2023/index.html#simulated-example-data",
    "title": "Row-wise means in dplyr",
    "section": "Simulated Example Data",
    "text": "Simulated Example Data\nLet’s say we have a tibble (or data frame) containing 10 observations and 4 numerical variables: y, x1, x2, and x3. We can simulate this quickly using rnorm() to sample from different normal distributions.\n\n# Simulate data\ny  &lt;- rnorm(n = 10, mean = 100, sd = 15)\nx1 &lt;- rnorm(n = 10, mean =   0, sd =  1)\nx2 &lt;- rnorm(n = 10, mean =  10, sd = 10)\nx3 &lt;- rnorm(n = 10, mean =  20, sd =  5)\nn10 &lt;- tibble(y, x1, x2, x3)"
  },
  {
    "objectID": "posts/rowwise2023/index.html#doing-it-by-hand",
    "href": "posts/rowwise2023/index.html#doing-it-by-hand",
    "title": "Row-wise means in dplyr",
    "section": "Doing it “by-hand”",
    "text": "Doing it “by-hand”\nNow let’s say we want to add a new variable xmean to the tibble containing each observation’s mean of x1, x2, and x3.\nWe can use mutate() and math to achieve this:\n\n# Example 0 (works but inconvenient)\nn10 |&gt; \n  mutate(xmean = (x1 + x2 + x3) / 3)\n\n# A tibble: 10 × 5\n       y     x1    x2    x3 xmean\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  98.7  0.327  5.88  18.6  8.28\n 2  85.3 -0.413  7.06  26.4 11.0 \n 3  71.9  0.562 22.2   15.9 12.9 \n 4  97.2  0.663 12.4   19.8 11.0 \n 5  90.5 -0.603  5.55  16.8  7.25\n 6 116.   0.698 -8.48  17.8  3.34\n 7  86.3  0.596  3.71  22.5  8.95\n 8 115.   0.452  1.39  25.6  9.16\n 9  94.0  0.897 25.1   24.9 17.0 \n10  93.0  0.572 37.4   19.4 19.1"
  },
  {
    "objectID": "posts/rowwise2023/index.html#a-failed-attempt",
    "href": "posts/rowwise2023/index.html#a-failed-attempt",
    "title": "Row-wise means in dplyr",
    "section": "A Failed Attempt",
    "text": "A Failed Attempt\nBut this approach will be a hassle if you have any missing values or if you have lots of variables to average. Instead, if you are just learning {dplyr}, you would probably try to combine the mean() and mutate() functions as below.\n\n# Example 1 (doesn't work)\nn10 |&gt; \n  mutate(xmean = mean(c(x1, x2, x3)))\n\n# A tibble: 10 × 5\n       y     x1    x2    x3 xmean\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  98.7  0.327  5.88  18.6  10.8\n 2  85.3 -0.413  7.06  26.4  10.8\n 3  71.9  0.562 22.2   15.9  10.8\n 4  97.2  0.663 12.4   19.8  10.8\n 5  90.5 -0.603  5.55  16.8  10.8\n 6 116.   0.698 -8.48  17.8  10.8\n 7  86.3  0.596  3.71  22.5  10.8\n 8 115.   0.452  1.39  25.6  10.8\n 9  94.0  0.897 25.1   24.9  10.8\n10  93.0  0.572 37.4   19.4  10.8\n\n\nHowever, you’ll notice in the output above that the new xmean variable contains repetitions of a constant value. What is going on here? Basically, what mutate() did was take all the numbers in x1, x2, and x3, combine them into one long vector of 30 numbers, and send that vector to the mean() function. The mean() function then returns a single value—the mean of all 30 numbers—and tries to put that into the new column xmean. But because the column needs to be a vector of 10 numbers to fit into the tibble, that single value gets recycled (i.e., repeated 10 times). To verify this is what happened, we can do the operation by hand and see that we get the same number:\n\nmean(c(x1, x2, x3))\n\n[1] 10.79233\n\n\nSo, clearly mutate() is not doing what we intended it to do."
  },
  {
    "objectID": "posts/rowwise2023/index.html#the-across-approach",
    "href": "posts/rowwise2023/index.html#the-across-approach",
    "title": "Row-wise means in dplyr",
    "section": "The across() approach",
    "text": "The across() approach\nLuckily, dplyr 1.0.0 added some great features for doing operations within rows. The simplest version simply adds a call to the rowwise() function to our pipeline before the mutate().\n\n# Example 2 (works but slow)\nn10 |&gt; \n  rowwise() |&gt;  \n  mutate(xmean = mean(c(x1, x2, x3)))\n\n# A tibble: 10 × 5\n# Rowwise: \n       y     x1    x2    x3 xmean\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  98.7  0.327  5.88  18.6  8.28\n 2  85.3 -0.413  7.06  26.4 11.0 \n 3  71.9  0.562 22.2   15.9 12.9 \n 4  97.2  0.663 12.4   19.8 11.0 \n 5  90.5 -0.603  5.55  16.8  7.25\n 6 116.   0.698 -8.48  17.8  3.34\n 7  86.3  0.596  3.71  22.5  8.95\n 8 115.   0.452  1.39  25.6  9.16\n 9  94.0  0.897 25.1   24.9 17.0 \n10  93.0  0.572 37.4   19.4 19.1 \n\n\nThis did what we wanted it to do, despite the actual mutate() call being identical to what is was before! Pretty cool. We can even save some time by selecting the variables to include in the mean() operation automatically, instead of listing them out in the c() function. This isn’t such a time-savings in this case with only three variables, but in settings with more variables it can really add up. To do so, we just need to use a tidy selection function; in this case, all the variables we want to include start with the letter “x” so let’s use starts_with().\n\n# Example 3 (doesn't work)\nn10 |&gt; \n  rowwise()|&gt; \n  mutate(xmean = mean(starts_with(\"x\")))\n\nError in `mutate()`:\nℹ In argument: `xmean = mean(starts_with(\"x\"))`.\nℹ In row 1.\nCaused by error:\n! `starts_with()` must be used within a *selecting* function.\nℹ See &lt;https://tidyselect.r-lib.org/reference/faq-selection-context.html&gt; for\n  details.\n\n\nShoot, that didn’t work. But why not? Basically, the problem is that mutate() doesn’t know what do to with selection functions like starts_with(). The error message basically says that we are in the “wrong context” for a selection function."
  },
  {
    "objectID": "posts/rowwise2023/index.html#tidy-selection-with-c_across",
    "href": "posts/rowwise2023/index.html#tidy-selection-with-c_across",
    "title": "Row-wise means in dplyr",
    "section": "Tidy selection with c_across()",
    "text": "Tidy selection with c_across()\nLuckily, dplyr 1.0.0 also added the c_across() function, which will allow us to change the context to one that does allow selection functions. The code below now works as intended, first selecting all the variables starting with “x” and then computing their row-wise means.\n\n# Example 4 (works but slow)\nn10 |&gt; \n  rowwise() |&gt; \n  mutate(xmean = mean(c_across(starts_with(\"x\"))))\n\n# A tibble: 10 × 5\n# Rowwise: \n       y     x1    x2    x3 xmean\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  98.7  0.327  5.88  18.6  8.28\n 2  85.3 -0.413  7.06  26.4 11.0 \n 3  71.9  0.562 22.2   15.9 12.9 \n 4  97.2  0.663 12.4   19.8 11.0 \n 5  90.5 -0.603  5.55  16.8  7.25\n 6 116.   0.698 -8.48  17.8  3.34\n 7  86.3  0.596  3.71  22.5  8.95\n 8 115.   0.452  1.39  25.6  9.16\n 9  94.0  0.897 25.1   24.9 17.0 \n10  93.0  0.572 37.4   19.4 19.1"
  },
  {
    "objectID": "posts/rowwise2023/index.html#remember-to-ungroup",
    "href": "posts/rowwise2023/index.html#remember-to-ungroup",
    "title": "Row-wise means in dplyr",
    "section": "Remember to ungroup()",
    "text": "Remember to ungroup()\nThere are two things to note about rowwise(), however. First, it transformed our tibble into an implicitly “grouped” tibble, which is what allowed our mutate() function to calculate row-wise means instead of overall means (basically, it is treating each row as a separate group and calculating the means per group/row). However, after that mutate() call, the tibble remains grouped. This is handy if we want to continue doing row-wise operations, but how do we tell it to stop once we are done with row-wise operations and want to return to “normal” behavior? Let’s see when this could be a problem; one example is if we want to calculate the maximum row-wise mean xmean_max.\n\n# Example 5 (doesn't work)\nn10 |&gt; \n  rowwise() |&gt; \n  mutate(xmean = mean(c_across(starts_with(\"x\")))) |&gt; \n  summarize(xmean_max = max(xmean))\n\n# A tibble: 10 × 1\n   xmean_max\n       &lt;dbl&gt;\n 1      8.28\n 2     11.0 \n 3     12.9 \n 4     11.0 \n 5      7.25\n 6      3.34\n 7      8.95\n 8      9.16\n 9     17.0 \n10     19.1 \n\n\nHere we wanted to summarize over all values of xmean and expected a single maximum value. Instead we got the same 10 values back. What happened? Basically, our tibble was still implicitly grouped by row and the summarize() function respected that, calculating the maximum of each group/row. To avoid this behavior, we can add the ungroup() function to our pipeline (reverting the tibble back to a standard one without implicit grouping).\n\n# Example 6 (works but slow)\nn10 |&gt; \n  rowwise() |&gt; \n  mutate(xmean = mean(c_across(starts_with(\"x\")))) |&gt; \n  ungroup() |&gt;\n  summarize(xmean_max = max(xmean))\n\n# A tibble: 1 × 1\n  xmean_max\n      &lt;dbl&gt;\n1      19.1\n\n\nNow we get the desired behavior, and so I am usually very careful to add ungroup() to my pipeline as soon as I am done with row-wise operations (otherwise you might end up with some unexpected problems)."
  },
  {
    "objectID": "posts/rowwise2023/index.html#faster-means-with-rowmeans",
    "href": "posts/rowwise2023/index.html#faster-means-with-rowmeans",
    "title": "Row-wise means in dplyr",
    "section": "Faster means with rowMeans()",
    "text": "Faster means with rowMeans()\nThe other thing to note about rowwise() is that it can be slow. With a small tibble like this, it doesn’t matter much, but the difference could be meaningful for larger and more complex data. In these cases, you have some alternatives. This blog post describes some of them, but does not address the specific case of means, which is what I want to do the most in practice. A faster alternative in this case is to use the rowMeans() function. As you might imagine, this function takes in a numeric matrix or dataframe and returns the mean of each row.\n\nrowMeans(n10)\n\n [1] 30.89482 29.57077 27.64172 32.52192 28.06156 31.59763 28.28442 35.62799\n [9] 36.23378 37.56994\n\n\nBut we want to exclude the y variable and append it to the n10 tibble. How can we do so? We might reasonably try to put it into mutate() like we did with mean():\n\n# Example 7 (doesn't work)\nn10 |&gt; \n  mutate(xmean = rowMeans(c(x1, x2, x3)))\n\nError in `mutate()`:\nℹ In argument: `xmean = rowMeans(c(x1, x2, x3))`.\nCaused by error in `rowMeans()`:\n! 'x' must be an array of at least two dimensions\n\n\nBut here we run into a problem. rowMeans() is expecting a numeric matrix or data frame, but is being provided with a vector of 30 numbers again (as in Example 1). Thus, it doesn’t have rows to calculate means within and returns an error. We can solve this by transforming the vector to a matrix:\n\n# Example 8 (works but inconvenient)\nn10 |&gt; \n  mutate(xmean = rowMeans(matrix(c(x1, x2, x3), ncol = 3)))\n\n# A tibble: 10 × 5\n       y     x1    x2    x3 xmean\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  98.7  0.327  5.88  18.6  8.28\n 2  85.3 -0.413  7.06  26.4 11.0 \n 3  71.9  0.562 22.2   15.9 12.9 \n 4  97.2  0.663 12.4   19.8 11.0 \n 5  90.5 -0.603  5.55  16.8  7.25\n 6 116.   0.698 -8.48  17.8  3.34\n 7  86.3  0.596  3.71  22.5  8.95\n 8 115.   0.452  1.39  25.6  9.16\n 9  94.0  0.897 25.1   24.9 17.0 \n10  93.0  0.572 37.4   19.4 19.1 \n\n\nBut this is inconvenient and error prone. Luckily, dplyr 1.1.0 added a way to streamline this kind of thing. We can use the new pick() function to create this context. Note that we are using pick() here instead of c_across() because the latter is for working within rows (in combination with rowwise()) and here we want the row-wise operations to be handled by rowMeans().\n\n# Example 9 (works and convenient and fast)\nn10 |&gt; \n  mutate(xmean = rowMeans(pick(starts_with(\"x\"))))\n\n# A tibble: 10 × 5\n       y     x1    x2    x3 xmean\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  98.7  0.327  5.88  18.6  8.28\n 2  85.3 -0.413  7.06  26.4 11.0 \n 3  71.9  0.562 22.2   15.9 12.9 \n 4  97.2  0.663 12.4   19.8 11.0 \n 5  90.5 -0.603  5.55  16.8  7.25\n 6 116.   0.698 -8.48  17.8  3.34\n 7  86.3  0.596  3.71  22.5  8.95\n 8 115.   0.452  1.39  25.6  9.16\n 9  94.0  0.897 25.1   24.9 17.0 \n10  93.0  0.572 37.4   19.4 19.1 \n\n\nTo test the speed of each approach, we can use the microbenchmark package, which will precisely time each approach over multiple iterations (in this case, 100). Let’s wrap up by testing the speed of each approach with increasingly large data sets.\n\n# Create function wrappers so the microbenchmark output is prettier\nA_rowwise &lt;- function(.data) { \n  .data |&gt; \n    rowwise() |&gt; \n    mutate(xmean = mean(c_across(starts_with(\"x\")))) |&gt; \n    ungroup()\n}\n\nB_rowMeans &lt;- function(.data) { \n  .data |&gt; \n    mutate(xmean = rowMeans(pick(starts_with(\"x\"))))\n}\n\n# Simulate larger datasets\nn100 &lt;- n10 |&gt; slice(rep(1:n(), times = 10))\nn1000 &lt;- n100 |&gt; slice(rep(1:n(), times = 10))\nn10000 &lt;- n1000 |&gt; slice(rep(1:n(), times = 10))\n\n# Perform microbenchmarking\nspeedtest &lt;- \n  microbenchmark(\n    A_rowwise(n10),\n    A_rowwise(n100),\n    A_rowwise(n1000),\n    A_rowwise(n10000),\n    B_rowMeans(n10),\n    B_rowMeans(n100), \n    B_rowMeans(n1000),\n    B_rowMeans(n10000),\n    times = 100L\n  )\n\n\n\n\n\n\nApproach\nmean_time_ms\nsd_time_ms\nmin_time_ms\nmax_time_ms\n\n\n\n\nA_rowwise(n10)\n7.502\n0.880\n6.765\n10.523\n\n\nA_rowwise(n100)\n53.755\n9.072\n49.137\n115.875\n\n\nA_rowwise(n1000)\n505.254\n15.616\n485.415\n572.448\n\n\nA_rowwise(n10000)\n5066.136\n110.166\n4913.116\n5758.591\n\n\nB_rowMeans(n10)\n1.471\n0.466\n1.265\n4.153\n\n\nB_rowMeans(n100)\n1.505\n0.390\n1.274\n4.014\n\n\nB_rowMeans(n1000)\n1.511\n0.528\n1.274\n4.815\n\n\nB_rowMeans(n10000)\n1.643\n0.614\n1.370\n5.813\n\n\n\n\n\n\nautoplot(speedtest)\n\n\n\n\n\n\n\n\nThe speed difference between approaches grows with the number of observations and becomes quite noticeable at 1000 observations or more. So, although I think it is worth learning the new rowwise() and c_across() functions, in settings where observations are many and speed in paramount, it may be worthwhile to learn “parallel” functions such as rowMeans(), rowSums(), pmin(), pmax(), and paste()."
  },
  {
    "objectID": "posts/rowwise2023/index.html#session-info",
    "href": "posts/rowwise2023/index.html#session-info",
    "title": "Row-wise means in dplyr",
    "section": "Session Info",
    "text": "Session Info\n\n\n\nClick here for session info\n\n\nsessionInfo()\n\nR version 4.4.3 (2025-02-28 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] kableExtra_1.4.0     knitr_1.49           microbenchmark_1.5.0\n [4] lubridate_1.9.3      forcats_1.0.0        stringr_1.5.1       \n [7] dplyr_1.1.4          purrr_1.0.2          readr_2.1.5         \n[10] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n[13] tidyverse_2.0.0     \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6      jsonlite_1.8.9    compiler_4.4.3    renv_1.0.11      \n [5] tidyselect_1.2.1  xml2_1.3.6        systemfonts_1.1.0 scales_1.3.0     \n [9] yaml_2.3.10       fastmap_1.2.0     R6_2.5.1          generics_0.1.3   \n[13] htmlwidgets_1.6.4 munsell_0.5.1     svglite_2.1.3     pillar_1.9.0     \n[17] tzdb_0.4.0        rlang_1.1.4       utf8_1.2.4        stringi_1.8.4    \n[21] xfun_0.49         viridisLite_0.4.2 timechange_0.3.0  cli_3.6.3        \n[25] withr_3.0.2       magrittr_2.0.3    digest_0.6.37     grid_4.4.3       \n[29] rstudioapi_0.17.1 hms_1.1.3         lifecycle_1.0.4   vctrs_0.6.5      \n[33] evaluate_1.0.1    glue_1.8.0        farver_2.1.2      fansi_1.0.6      \n[37] colorspace_2.1-1  rmarkdown_2.29    tools_4.4.3       pkgconfig_2.0.3  \n[41] htmltools_0.5.8.1"
  },
  {
    "objectID": "posts/rowwise2023/index.html#notes-references",
    "href": "posts/rowwise2023/index.html#notes-references",
    "title": "Row-wise means in dplyr",
    "section": "Notes / References",
    "text": "Notes / References\n\n[1] dplyr vignette: Row-wise operations\n[2] dplyr 1.1.0: pick(), reframe(), arrange()\n[3] dplyr 1.0.0: working within rows\n[4] dplyr issue #4544"
  },
  {
    "objectID": "posts/recruitment2023/index.html",
    "href": "posts/recruitment2023/index.html",
    "title": "Recruitment 2023",
    "section": "",
    "text": "Are you accepting applications for new graduate students?\nYes! I am planning to review applications for graduate students to start in Fall 2023 and may admit one or two Psychology PhD students across the Clinical Psychological Science (CPS) and Brain, Behavior, and Quantitative Science (BBQ) programs. The application deadline for both programs is December 1, 2022.\n\n\nWhat does your lab study?\nThe AffCom lab develops and uses technology to advance scientific understanding of how emotional, social, and mental health-related information is communicated through verbal and nonverbal behavior. Our work is eclectic and spans basic science on emotion and behavior; clinical science on assessment and treatment; and methodological science on the collection, processing, analysis, and visualization of research data.\n\n\nWhat are your research plans for the near future?\nIn the next few years, I plan to study (1) how therapists and patients with depression communicate during psychotherapy sessions, (2) how individuals with and without a history of trauma communicate their experiences and construct narratives, (3) how psychotic symptoms affect the production of communicative behavior, and (4) how technology can be used to more precisely measure emotional expressiveness in both healthy and clinical populations. I am also planning to work on software related to inter-rater agreement, hierarchical latent variable modeling, and web-based video annotation. See the research page for more information.\n\n\nShould I apply to work with you at KU?\nI welcome applications from all qualified students regardless of background. However, admissions are competitive, and applications are not free; thus, I want to give applicants a better idea of what I am looking for so that they can make an informed decision about whether to invest their time and money.\nFirst, note that I currently only advise PhD students in the CPS and BBQ programs. The psychology department does not offer a terminal Masters degree and I am not affiliated with the Child Clinical Psychology program. Second, note that my lab does not specialize in neuroscience or do much brain-focused research (i.e., we skew more towards the behavior and quantitative side of the BBQ program).\nThe primary criteria I use to evaluate applications are (1) evidence of the applicant’s ability to succeed in a research-focused PhD program, (2) a match between the applicant’s academic interests/experiences and my own areas of interest/expertise, and (3) a match between the applicant’s interpersonal style and my own.\nTo be competitive for admission, applicants should have (1) a strong academic record with a Bachelor’s or Master’s degree in psychology or a related field, (2) previous experience working/volunteering in one or more research labs, (3) a high level of maturity and professionalism conducive to collaborative and/or clinical work, and (4) a direct connection to my area of research such as through previous coursework, research experience, or independent study.\n\n\nWhat is the funding situation for graduate students?\nI would not take on a graduate student if I did not believe that I could provide them full funding (tuition remission, stipend, and benefits) for four to five years. Funding comes primarily from teaching assistantships, although research assistantships and scholarships are also possibilities. Supplemental summer funding is also often possible depending on our success in securing research grants.\n\n\nWhat is your stance on scientific reform?\nI am committed to making my science more transparent, open, reproducible, and accountable though the (1) sharing of research preprints, materials, and data, (2) transparent description of hypotheses and methods, and (3) use and development of free and open-source software.\n\n\nCan potential applicants meet with you before applying?\nDue to the high number of inquiries I receive each year, it is unfortunately not possible for me to meet with potential applicants. However, I will carefully review every application that lists my name and those applicants who are invited to interview will have the opportunity to speak with me one-on-one.\n\n\nCan potential applicants speak to students in the lab?\nSimilar to the question above, applicants who are invited to interview will have the opportunity to speak to graduate students and research assistants in the lab (as well as various other members of the department).\n\n\nHow do I apply?\nAdmission information and links can be found on the Psychology Graduate Program webpage. Scroll down and open the “Admission” accordion or click on one of the “Application Instructions” links. You will need to submit a curriculum vitae, statement of purpose, three letters of recommendations, and transcripts."
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#explanatory-latent-variables",
    "href": "posts/mvac2023/Presentation.html#explanatory-latent-variables",
    "title": "Measurement Validation in Affective Computing",
    "section": "Explanatory Latent Variables",
    "text": "Explanatory Latent Variables\n\nTo understand and predict events in the world, it often helps to hypothesize explanatory latent variables\nThese explanatory variables aren’t directly observable but rather are inferred from observations they explain\nExamples of explanatory latent variables:\n\nMotivation explains people acting in pursuit of goals\nEngagement explains students paying close attention\nIrritability explains people being quick to anger"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#constructs-and-indicators",
    "href": "posts/mvac2023/Presentation.html#constructs-and-indicators",
    "title": "Measurement Validation in Affective Computing",
    "section": "Constructs and Indicators",
    "text": "Constructs and Indicators\n\nHypothesized latent variables are called constructs\nThe observations that they explain are called indicators\nMost constructs are estimated using “effect” indicators"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#psychological-constructs",
    "href": "posts/mvac2023/Presentation.html#psychological-constructs",
    "title": "Measurement Validation in Affective Computing",
    "section": "Psychological Constructs",
    "text": "Psychological Constructs\n\nPsychological constructs explain…\n\nInternally experienced thoughts and feelings\nExternally observable abilities and behaviors"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#hierarchies-of-constructs",
    "href": "posts/mvac2023/Presentation.html#hierarchies-of-constructs",
    "title": "Measurement Validation in Affective Computing",
    "section": "Hierarchies of Constructs",
    "text": "Hierarchies of Constructs\n\nConstructs are often correlated with other constructs\n\nThey may share similar mechanisms and indicators\n\nThese relationships often from a construct hierarchy\n\nHigher-level constructs are more broad, general, abstract\nLower-level constructs are more narrow, specific, concrete\n\nConstruct hierarchies are very common and useful\n\nCan improve annotation, modeling, and prediction"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#hierarchies-of-constructs-1",
    "href": "posts/mvac2023/Presentation.html#hierarchies-of-constructs-1",
    "title": "Measurement Validation in Affective Computing",
    "section": "Hierarchies of Constructs",
    "text": "Hierarchies of Constructs"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#levels-of-abstraction",
    "href": "posts/mvac2023/Presentation.html#levels-of-abstraction",
    "title": "Measurement Validation in Affective Computing",
    "section": "Levels of Abstraction",
    "text": "Levels of Abstraction"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#what-is-measurement",
    "href": "posts/mvac2023/Presentation.html#what-is-measurement",
    "title": "Measurement Validation in Affective Computing",
    "section": "What is measurement?",
    "text": "What is measurement?\n\nWe often want to know an individual, group, or object’s “standing” or score on a psychological construct\n\nHow neurotic is a person in their day-to-day life?\nHow engaged was the audience during the movie?\nHow persuasive is this argument (to most people)?\nIs the person in the photograph smiling or not?\n\nThis means assigning a numerical score to it\nWe call this construct estimation or measurement"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#what-is-the-measurement-process",
    "href": "posts/mvac2023/Presentation.html#what-is-the-measurement-process",
    "title": "Measurement Validation in Affective Computing",
    "section": "What is the measurement process?",
    "text": "What is the measurement process?\n\nBy definition, constructs cannot be directly measured\nSo we must infer their scores from measured indicators\nEstimating construct scores thus proceeds as follows:\n\nSelect a set of indicators to represent the construct\nMeasure each selected indicator for objects of interest\nAggregate measures into estimated construct scores"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#common-measurement-strategies",
    "href": "posts/mvac2023/Presentation.html#common-measurement-strategies",
    "title": "Measurement Validation in Affective Computing",
    "section": "Common measurement strategies",
    "text": "Common measurement strategies\n\nItems-as-Indicators\n\nOne rater completes many items of the same construct\nCommonly used in self-report questionnaires\n\nRaters-as-Indicators\n\nMany raters complete a single item of a construct\nCommonly used in observer rating scales\n\nHybrid Approaches (Advanced)"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#self-report-questionnaires",
    "href": "posts/mvac2023/Presentation.html#self-report-questionnaires",
    "title": "Measurement Validation in Affective Computing",
    "section": "Self-report questionnaires",
    "text": "Self-report questionnaires\n\nSelf-report questionnaires ask participants to indicate their own standing on one or more constructs\nQuestionnaires are composed of multiple items, each of which is responded to using a numerical rating scale\nEach item is meant to measure a single indicator\nConstruct scores are often estimated by summing or averaging all items that correspond to that construct (though sophisticated methods include PCA, SEM, and IRT)"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#self-report-questionnaires-1",
    "href": "posts/mvac2023/Presentation.html#self-report-questionnaires-1",
    "title": "Measurement Validation in Affective Computing",
    "section": "Self-report questionnaires",
    "text": "Self-report questionnaires"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#self-report-questionnaires-2",
    "href": "posts/mvac2023/Presentation.html#self-report-questionnaires-2",
    "title": "Measurement Validation in Affective Computing",
    "section": "Self-report questionnaires",
    "text": "Self-report questionnaires\n\n\nAssumptions of numerical questionnaires\n\nAll items are measuring the same construct*\nAll items are equally important/central to the construct*\nAll items were correctly understood by the participants\n\nPros and cons of self-report questionnaires\n\nCommon, efficient, and good for internal phenomena\nCan be subjective and influenced by self-report biases\n\n\n\n*These assumptions can be tested and even relaxed with advanced methods."
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#observer-rating-scales",
    "href": "posts/mvac2023/Presentation.html#observer-rating-scales",
    "title": "Measurement Validation in Affective Computing",
    "section": "Observer rating scales",
    "text": "Observer rating scales\n\nObservers or raters are individuals who view stimuli (e.g., multimedia) and provide scores on various constructs\nThese measurements are standardized using an instrument (e.g., coding scheme or rating scale)\nSuch instruments tell observers what to focus on and help them to make consistent measurements\nThe goal is to take out any unwanted subjectivity"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#example-coding-scheme",
    "href": "posts/mvac2023/Presentation.html#example-coding-scheme",
    "title": "Measurement Validation in Affective Computing",
    "section": "Example Coding Scheme",
    "text": "Example Coding Scheme\nA smile is a facial movement that pulls the mouth corners upwards and toward the ears (see examples below). You will view several images; please examine each image carefully and determine whether the image would be best described as either Smiling or Not Smiling."
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#example-rating-scale",
    "href": "posts/mvac2023/Presentation.html#example-rating-scale",
    "title": "Measurement Validation in Affective Computing",
    "section": "Example Rating Scale",
    "text": "Example Rating Scale\nPersuasiveness is the capability of a person or argument to convince or persuade someone to accept a desired way of thinking. You will read several brief movie reviews in which a reviewer will argue that a movie is either worth watching or not worth watching. Use the following scale to indicate how persuasive you found each movie review to be overall."
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#observer-ratings",
    "href": "posts/mvac2023/Presentation.html#observer-ratings",
    "title": "Measurement Validation in Affective Computing",
    "section": "Observer Ratings",
    "text": "Observer Ratings\n\nAssumptions of observer ratings\n\nThe stimulus is rich enough to inform measurement\nThe instructions are sufficient to standardize\nObservers can generalize to new examples\n\nPros and cons of observer ratings\n\nAvoids many biases introduced by self-report methods\nCan be very inefficient and subjective (when done poorly)\nCan’t access internal states of the stimulus participants"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#what-are-measurement-errors",
    "href": "posts/mvac2023/Presentation.html#what-are-measurement-errors",
    "title": "Measurement Validation in Affective Computing",
    "section": "What are measurement errors?",
    "text": "What are measurement errors?\n\nMeasurement errors are differences between an object’s estimated score and its “true” standing on the construct\nMeasurement errors come from two main sources:\n\nConstruct under-representation\n\nFailing to represent all aspects of the construct\n\nConstruct-irrelevant variance\n\nFailing to be influenced by only the construct"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#construct-under-representation",
    "href": "posts/mvac2023/Presentation.html#construct-under-representation",
    "title": "Measurement Validation in Affective Computing",
    "section": "Construct under-representation",
    "text": "Construct under-representation\n\nThe depression construct has many aspects\n\nAffective symptoms (e.g., low mood, anhedonia, irritability)\nCognitive symptoms (e.g., problems in attention and concentration)\nSomatic symptoms (e.g., changes in sleep, appetite, energy, activity)\nSymptom duration (at least 2 weeks)\n\nExamples of under-representation\n\nA depression questionnaire omits items about cognitive symptoms\nAn observer rates someone’s depression without symptom information\nA depression diagnosis algorithm only measures body motion/activity\nA depression diagnosis algorithm omits symptom duration information"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#construct-irrelevant-variance",
    "href": "posts/mvac2023/Presentation.html#construct-irrelevant-variance",
    "title": "Measurement Validation in Affective Computing",
    "section": "Construct-irrelevant variance",
    "text": "Construct-irrelevant variance\n\nExamples from Psychology\n\nA patient’s sleep problems (from a newborn) are attributed to depression\nOne doctor thinks a patient is depressed but another doctor doesn’t\nA participant reports feeling “frustrated” but not “angry”\nParticipants report higher life satisfaction on Fridays than on Mondays\nParticipants behave differently in the research lab than they do at home\n\nExamples from Affective Computing\n\nA facial recognition algorithm confuses two men with beards\nAn emotion recognition algorithm mistakes a shouting athlete as angry\nA “drowsy driver” algorithm under-predicts drowsiness during the day\nA depression diagnosis algorithm confuses an accent with slurred speech"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#what-are-validity-and-validation",
    "href": "posts/mvac2023/Presentation.html#what-are-validity-and-validation",
    "title": "Measurement Validation in Affective Computing",
    "section": "What are validity and validation?",
    "text": "What are validity and validation?\n\nValidity is the degree to which scores on an appropriately administered instrument support inferences about variation in the characteristic it was designed to measure.\nValidation is the ongoing process of gathering, summarizing, and evaluating relevant evidence concerning the degree to which that evidence supports the intended meaning of scores yielded by an instrument and inferences about standing on the characteristic it was designed to measure."
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#contemporary-validity-theory",
    "href": "posts/mvac2023/Presentation.html#contemporary-validity-theory",
    "title": "Measurement Validation in Affective Computing",
    "section": "Contemporary Validity Theory",
    "text": "Contemporary Validity Theory\n\nValidity applies to inferences, not instruments\nValidity varies across populations and contexts\nValidity is integrative and singular\nValidity exists on a continuum\nValidation is an ongoing process\nValidation has three main phases:(substantive, structural, and external)"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#why-does-validation-matter",
    "href": "posts/mvac2023/Presentation.html#why-does-validation-matter",
    "title": "Measurement Validation in Affective Computing",
    "section": "Why does validation matter?",
    "text": "Why does validation matter?\n\nGinsberg et al. (2009) used ML to predict flu pandemics from Google searches faster than traditional CDC methods, but Lazer et al. (2014) later found that it was merely predicting seasonality and the model completely missed nonseasonal influenza\nLiu et al. (2015) found that positive emotional expressions online were not related to self-reported life satisfaction, but it is difficult to argue against past theories because the inconsistency could be due to measurement errors\nRibeiro et al. (2016) found that an ML model learned to distinguish between images of Huskies and Wolves by merely looking for the presence of snow yet many participants trusted the model before learning about this relationship\nBleidorn & Hopwood (2019) and Tay et al. (2020) find that validity issues may be holding back ML approaches to personality assessment\nJacobucci & Grimm (2020) found that predictors/features with low reliability (an aspect of validity) attenuate predictive performance, especially for ML algorithms"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#substantive-phase-1",
    "href": "posts/mvac2023/Presentation.html#substantive-phase-1",
    "title": "Measurement Validation in Affective Computing",
    "section": "Substantive Phase",
    "text": "Substantive Phase\n\nDoes the construct definition make sense?\nDo the selected indicators represent the construct well?\n\nShow that there is not construct under-representation\nShow that there is not construct-irrelevant variance\n\nAre the items/scales being understood/used as intended?\nIs the instrument being administered properly?"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#literature-review",
    "href": "posts/mvac2023/Presentation.html#literature-review",
    "title": "Measurement Validation in Affective Computing",
    "section": "Literature Review",
    "text": "Literature Review\n\nHow has this construct been defined before?\nWhat are the main aspects of this construct?\nWhat are the main indicators of this construct?\nHow is it related to and distinct from other constructs?\nHow has it been measured in previous work?\nWhat theories are relevant to this construct?\nWhat support is there for these theories?"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#construct-conceptualization",
    "href": "posts/mvac2023/Presentation.html#construct-conceptualization",
    "title": "Measurement Validation in Affective Computing",
    "section": "Construct Conceptualization",
    "text": "Construct Conceptualization\n\nProvide a precise definition of the construct\nCreate a list of indicators for the construct\nCreate a list of related constructs / hierarchies\nCreate a Venn Diagram of related constructs\n\nDraw partially overlapping circles for each construct\nIn the overlap, describe what makes them similar\nIn the non-overlap, describe what makes them distinct"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#examples-of-related-constructs",
    "href": "posts/mvac2023/Presentation.html#examples-of-related-constructs",
    "title": "Measurement Validation in Affective Computing",
    "section": "Examples of related constructs",
    "text": "Examples of related constructs\n\nMain Construct: Depression\n\nRelated Constructs: stress, anxiety, sadness\n\nMain Construct: Engagement\n\nRelated Constructs: attention, participation, understanding\n\nMain Construct: Emotional Support\n\nRelated Constructs: empathy, advice, financial support"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#item-development-and-mapping",
    "href": "posts/mvac2023/Presentation.html#item-development-and-mapping",
    "title": "Measurement Validation in Affective Computing",
    "section": "Item development and mapping",
    "text": "Item development and mapping\n\nFor each aspect of the construct, generate a list of indicators\nCreate multiple “candidate” items/scales for each indicator\nHave experts review the list of aspects, indicators, and items\nPilot test the items/scales and refine based on feedback\nSelect the best items/scales based on pilot testing\nEnsure representation of all aspects in final selection"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#response-processess",
    "href": "posts/mvac2023/Presentation.html#response-processess",
    "title": "Measurement Validation in Affective Computing",
    "section": "Response Processess",
    "text": "Response Processess\nConsider how participants will respond to items/scales\n\nAre participants understanding the questions?\nAre participants understanding the response options?\nAre participants choosing responses as you intended?\nCognitive interviewing and think-aloud techniques\n\nAsk participants to rephrase each question\nAsk participants why they chose each answer\nAsk participants to think out loud during test"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#test-administration",
    "href": "posts/mvac2023/Presentation.html#test-administration",
    "title": "Measurement Validation in Affective Computing",
    "section": "Test Administration",
    "text": "Test Administration\nConsider where/how the instrument will be completed\n\nAre there any distractions in the environment?\nAre there any biasing factors in the environment?\nAre there any sources of error in the procedure?\nAre there any test security issues (e.g., cheating)?"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#inter-item-reliability",
    "href": "posts/mvac2023/Presentation.html#inter-item-reliability",
    "title": "Measurement Validation in Affective Computing",
    "section": "Inter-Item Reliability",
    "text": "Inter-Item Reliability\nAre scores consistent across items?\n\nAverage inter-item correlation (simplistic)\nCongeneric CFA (confirmatory factor analysis)\n\nEach item \\(x_j\\) is influenced by one construct \\(f\\)\nThe strength of this influence \\(\\lambda_j\\) varies per item\nEach item also has unique/error variance \\(e_j\\)\n\n\n\n\\[x_j = \\lambda_jf + e_j\\]"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#inter-item-reliability-1",
    "href": "posts/mvac2023/Presentation.html#inter-item-reliability-1",
    "title": "Measurement Validation in Affective Computing",
    "section": "Inter-Item Reliability",
    "text": "Inter-Item Reliability"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#inter-item-reliability-2",
    "href": "posts/mvac2023/Presentation.html#inter-item-reliability-2",
    "title": "Measurement Validation in Affective Computing",
    "section": "Inter-Item Reliability",
    "text": "Inter-Item Reliability\nMcDonald’s omega\n\nReliability of the factor scores \\(f\\) is captured by \\(\\omega_u\\)\nHow much of the score \\((x)\\) variance is explained by \\(f\\)?\n\n\n\\[\\omega_u = \\frac{(\\Sigma_j\\lambda_j)^2}{\\sigma_x^2}\\]\n\n\n\\(\\omega_u \\in (0, 1)\\), higher is better"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#inter-rater-and-test-retest-reliability",
    "href": "posts/mvac2023/Presentation.html#inter-rater-and-test-retest-reliability",
    "title": "Measurement Validation in Affective Computing",
    "section": "Inter-rater and test-retest reliability",
    "text": "Inter-rater and test-retest reliability\nAre scores consistent across raters or time?\n\nCategorical Ratings:\n\nChance-adjusted agreement (kappas)\n\nContinuous Ratings:\n\nIntraclass correlations (ICCs)"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#overview-of-kappas",
    "href": "posts/mvac2023/Presentation.html#overview-of-kappas",
    "title": "Measurement Validation in Affective Computing",
    "section": "Overview of kappas",
    "text": "Overview of kappas\n\nHow much agreement was observed?\n\nDid raters assign the same items to the same categories?\n\nCan be generalized (Gwet, 2021, Vol. I)\n\nAny number of categories and raters\nOrdered categories (via weights)\nMissing/unbalanced rating data"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#generalized-observed-agreement",
    "href": "posts/mvac2023/Presentation.html#generalized-observed-agreement",
    "title": "Measurement Validation in Affective Computing",
    "section": "Generalized observed agreement",
    "text": "Generalized observed agreement\nWeighting scheme (partial credit for close errors)\n\\[r_{ik}^\\star = \\sum_{l=1}^{q}w_{kl}r_{il}\\]\n\nObserved agreement\n\\[p_o = \\frac{1}{n'}\\sum_{i=1}^{n'}\\sum_{k=1}^{q}\\frac{r_{ik}(r_{ik}^\\star-1)}{r_i(r_i-1)}\\]"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#generalized-chance-agreement",
    "href": "posts/mvac2023/Presentation.html#generalized-chance-agreement",
    "title": "Measurement Validation in Affective Computing",
    "section": "Generalized chance agreement",
    "text": "Generalized chance agreement\n\nHow much agreement would we expect from chance?\n\nCategory-based (e.g., Bennett et al.’s \\(S\\)) \\[p_c^S=\\frac{1}{q}\\sum_{k,l}w_{kl}\\]\nDistribution-based (e.g., Cohen’s \\(\\kappa\\), Scott’s \\(\\pi\\), Kripp.’s \\(\\alpha\\)) \\[p_c^\\pi=\\sum_{k,l}^qw_{kl}\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{r_{ik}}{r_i}\\right)\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{r_{il}}{r_i}\\right)\\]"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#overview-of-kappas-1",
    "href": "posts/mvac2023/Presentation.html#overview-of-kappas-1",
    "title": "Measurement Validation in Affective Computing",
    "section": "Overview of kappas",
    "text": "Overview of kappas\n\nHow much possible non-chance agreement was achieved? \\[\\kappa = \\frac{p_o-p_c}{1-p_c}=\\frac{\\text{Observed Non-Chance Agreement}}{\\text{Possible Non-Chance Agreement}}\\]\n\n\\(\\kappa\\in(-1,+1)\\), higher is better\n\\(\\kappa\\ge.60\\) is acceptable, \\(\\kappa\\ge.80\\) is good\nInterval estimates are recommended\nHypothesis testing \\(\\kappa\\) vs. zero is not"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#overview-of-iccs",
    "href": "posts/mvac2023/Presentation.html#overview-of-iccs",
    "title": "Measurement Validation in Affective Computing",
    "section": "Overview of ICCs",
    "text": "Overview of ICCs\n\nHow much score variance is due to items vs. raters?\n\nIf the raters are consistent, that variance should be low\nThus, most of the variance should be due to items\n\nVariance components can be estimated many ways\n\ne.g., from ANOVA, MLM, or SEM\nten Hove et al. (2022) extended to missing data\n\n\\(ICC\\in(-1,+1)\\) where higher is better\n\\(ICC\\ge0.75\\) is acceptable, \\(ICC\\ge0.90\\) is good"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#important-icc-questions",
    "href": "posts/mvac2023/Presentation.html#important-icc-questions",
    "title": "Measurement Validation in Affective Computing",
    "section": "Important ICC Questions",
    "text": "Important ICC Questions\n\nOne-way or two-way?\n\nOne-way: different raters each time\nTwo-way: same raters multiple times\n\nSingle-measures or Average-measures?\n\nSingle: a single rater’s scores will be used\nAverage: the average of \\(k\\) raters’ scores will be used\n\nAgreement or consistency? (two-way only)\n\nAgreement: raters must agree exactly (absolute error)\nConsistency: raters can have different means (relative error)"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#one-way-iccs",
    "href": "posts/mvac2023/Presentation.html#one-way-iccs",
    "title": "Measurement Validation in Affective Computing",
    "section": "One-way ICCs",
    "text": "One-way ICCs\n\nOne-way Single-Measures ICC \\[ICC(1)=\\frac{\\sigma_i^2}{\\sigma_i^2+\\sigma_{r:i}^2}\\]\nOne-way Average-Measures ICC \\[ICC(k)=\\frac{\\sigma_i^2}{\\sigma_i^2 + \\sigma_{r:i}^2/k}\\]"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#two-way-agreement-iccs",
    "href": "posts/mvac2023/Presentation.html#two-way-agreement-iccs",
    "title": "Measurement Validation in Affective Computing",
    "section": "Two-way Agreement ICCs",
    "text": "Two-way Agreement ICCs\n\nTwo-way Single-Measures Agreement ICC \\[ICC(A,1) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\sigma_r^2 + \\sigma_{ir}^2}\\]\nTwo-way Average-Measures Agreement ICC \\[ICC(A,k) = \\frac{\\sigma_i^2}{\\sigma_i^2 + (\\sigma_r^2 + \\sigma_{ir}^2)/k}\\]"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#two-way-consistency-iccs",
    "href": "posts/mvac2023/Presentation.html#two-way-consistency-iccs",
    "title": "Measurement Validation in Affective Computing",
    "section": "Two-way Consistency ICCs",
    "text": "Two-way Consistency ICCs\n\nTwo-way Single-Measures Consistency ICC \\[ICC(C,1) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\sigma_{ir}^2}\\]\nTwo-way Average-Measures Consistency ICC \\[ICC(C,k) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\sigma_{ir}^2/k}\\]"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#criterion-variables",
    "href": "posts/mvac2023/Presentation.html#criterion-variables",
    "title": "Measurement Validation in Affective Computing",
    "section": "Criterion Variables",
    "text": "Criterion Variables\n\nA good score of X should be highly correlated with other, trusted measures of X (called criterion variables)\nDo our scores correlate with criterion variables?\n\nExample: Depression scores\n\nSelf-report scales (PHQ-9, BDI-2, QIDS-SR)\nClinical interviews (SCID-5, HRDS-17, MADRS)"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#nomological-network",
    "href": "posts/mvac2023/Presentation.html#nomological-network",
    "title": "Measurement Validation in Affective Computing",
    "section": "Nomological Network",
    "text": "Nomological Network\n\nA good score of X should correlate positively with A…\nA good score of X should correlate negatively with B…\nA good score of X should be uncorrelated with C…\nDo our scores correlate with others as expected?\n\nExample: Depression scores (Kotov et al., 2010)\n\nPositive with neuroticism\nNegative with extraversion\nUnrelated to agreeableness"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#related-groups",
    "href": "posts/mvac2023/Presentation.html#related-groups",
    "title": "Measurement Validation in Affective Computing",
    "section": "Related Groups",
    "text": "Related Groups\n\nA good score of X should differ between groups A and B…\nDo our scores differ between known groups?\n\nExample: Depression scores\n\nBetween patient and community groups\n\nExample: Content Mastery scores\n\nBetween trainee and instructor groups"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#smiling-around-the-world",
    "href": "posts/mvac2023/Presentation.html#smiling-around-the-world",
    "title": "Measurement Validation in Affective Computing",
    "section": "Smiling Around the World",
    "text": "Smiling Around the World\n\nGathered ~290,000 facial images of celebrities\nUsed OpenFace 2.0 to measure smiling\n\nSmiling defined as activation of FACS AU12\n\nCompared smiling between genders and countries\nHow should we go about validating our measure?\n\n\nMcDuff & Girard (ACII 2019)"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#smiling-around-the-world-1",
    "href": "posts/mvac2023/Presentation.html#smiling-around-the-world-1",
    "title": "Measurement Validation in Affective Computing",
    "section": "Smiling Around the World",
    "text": "Smiling Around the World\n\nSelected ~300 images (balanced gender, country, smile)\nEach coded by 1 of 3 certified FACS coders for AU12 intensity\nEach coded by 5 untrained crowdworkers\n\nRated on positive emotion in image (0 to 5)\nRated on smile intensity in image (0 to 5)\n\nCorrelations between openface, FACS coding, and ratings\n\n\nMcDuff & Girard (ACII 2019)"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#smiling-around-the-world-2",
    "href": "posts/mvac2023/Presentation.html#smiling-around-the-world-2",
    "title": "Measurement Validation in Affective Computing",
    "section": "Smiling Around the World",
    "text": "Smiling Around the World\n\n\nMcDuff & Girard (ACII 2019)"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#perceived-emotional-expressivity",
    "href": "posts/mvac2023/Presentation.html#perceived-emotional-expressivity",
    "title": "Measurement Validation in Affective Computing",
    "section": "Perceived Emotional Expressivity",
    "text": "Perceived Emotional Expressivity\n\nRecorded 96 participants interacting in 3-person groups\nWant to predict “perceived emotional expressivity”\nDeveloped a four-item rating scale for our construct\nCollected ratings of each video from 8 crowdworkers\nHow to validate these measures?\n\n\nLin, Girard, et al. (ICMI 2020)"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#substantive-phase-2",
    "href": "posts/mvac2023/Presentation.html#substantive-phase-2",
    "title": "Measurement Validation in Affective Computing",
    "section": "Substantive Phase",
    "text": "Substantive Phase\n\nHow expressive was the person in this video (use your own understanding of what it means to be expressive)?\nHow much did the person in this video show their emotions (through their words and nonverbal behavior)?\nHow animated (lively, energetic, or active) was the person in this video?\nHow much did the person in this video react to the other people (through their words and nonverbal behavior)?\n\n\nLin, Girard, et al. (ICMI 2020)"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#structural-phase-1",
    "href": "posts/mvac2023/Presentation.html#structural-phase-1",
    "title": "Measurement Validation in Affective Computing",
    "section": "Structural Phase",
    "text": "Structural Phase\nInter-Rater Reliability: \\(ICC(C,8)\\)\n\n\nLin, Girard, et al. (ICMI 2020)"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#structural-phase-2",
    "href": "posts/mvac2023/Presentation.html#structural-phase-2",
    "title": "Measurement Validation in Affective Computing",
    "section": "Structural Phase",
    "text": "Structural Phase\nInter-Item Reliability\n\n\n\n\n \\[\\omega_u=0.966\\]"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#perceived-emotional-expressivity-1",
    "href": "posts/mvac2023/Presentation.html#perceived-emotional-expressivity-1",
    "title": "Measurement Validation in Affective Computing",
    "section": "Perceived Emotional Expressivity",
    "text": "Perceived Emotional Expressivity\nExternal Phase: Nomological Network\n\nExpect positive correlation with extraversion (0.2 to 0.4)\n\n\\(r=0.26\\) (great!)\n\nExpect positive correlation with agreeableness (0.0 to 0.2)\n\n\\(r=0.28\\) (good but a little high)\n\nExpect weak correlation with neuroticism (–0.1 to 0.1)\n\n\\(r=-0.07\\) (great)\n\n\n\nLin, Girard, et al. (ICMI 2020)"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#activities-round-1",
    "href": "posts/mvac2023/Presentation.html#activities-round-1",
    "title": "Measurement Validation in Affective Computing",
    "section": "Activities Round 1",
    "text": "Activities Round 1\n\nAssess an applied paper in your area\n\nIs their construct conceptualization clear?\nIs their measurement well justified?\n\nAssess a measurement instrument in your area\n\nHow much validity evidence did they provide?\nHow could their validation be improved?"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html#activities-round-2",
    "href": "posts/mvac2023/Presentation.html#activities-round-2",
    "title": "Measurement Validation in Affective Computing",
    "section": "Activities Round 2",
    "text": "Activities Round 2\n\nWork on a construct conceptualization\n\nWrite a definition and list indicators\nCompare and contrast related constructs\n\nPlan a measurement validation study\n\nPlan your validation sample and design\nPlan the three phases of validation"
  },
  {
    "objectID": "posts/mmm2024/index.html",
    "href": "posts/mmm2024/index.html",
    "title": "MMM 2024 Conference",
    "section": "",
    "text": "Aaron and I attended the MMM2024 conference in Storrs last week. Our presentations are provided below.\n\nGirard, J. M. (2024). A communications-focused approach to building path diagrams for multilevel models. Paper presented at the Modern Modeling Methods Conference, Storrs, CT.\nGirard, J. M. (2024). Intensive longitudinal modeling of big social media data. Paper presented at the Modern Modeling Methods Conference, Storrs, CT.\nSimmons, A. M. & Girard, J. M. (2024). An illustration of advanced intraclass correlations for inter-rater reliability. Poster presented at the Modern Modeling Methods Conference, Storrs, CT."
  },
  {
    "objectID": "posts/hitop2025/index.html",
    "href": "posts/hitop2025/index.html",
    "title": "HiTOP 2025 Conference",
    "section": "",
    "text": "Daiil, Kassy, Jimin, and I will be attending the 2025 HiTOP Conference in Denver next week. Our presentations are below. See you there!\n\nJun, D., Girard, J. M., Chung, Y., Liebenthal, E., & Baker, J. T. (2025). Exploring and connecting the within-person factor structures of psychosis-related EMA and clinician-rated symptoms. Paper presented at the Annual Meeting of the Hierarchical Taxonomy of Psychopathology society.\n\n\n\nGray, K., Girard, J. M., & HiTOP Software and Web Development Workgroup. (2025). Toward open-source software for collecting and visualizing HiTOP data. Poster presented at the Annual Meeting of the Hierarchical Taxonomy of Psychopathology society.\n\nClick here for the feedback link\n\n\n\n\nYoo, J., Yermol, D. A., Burke, T., Liu, R., & Girard, J. M. (2025). Exploring internalizing and externalizing symptoms and guardian-adolescent communication in high-risk adolescents. Poster presented at the Annual Meeting of the Hierarchical Taxonomy of Psychopathology society."
  },
  {
    "objectID": "posts/gpsp2023/index.html",
    "href": "posts/gpsp2023/index.html",
    "title": "GPSP 2023 Convention",
    "section": "",
    "text": "Kassy and I attended the Great Plains Student Psychology Convention last weekend, hosted at Emporia State University. I gave an invited keynote address on “Reconsidering the Duchenne Smile,” and Kassy presented her first AffCom lab poster on “Adult attachment and the working alliance during brief psychotherapy for depression.” She was also awarded an Outstanding Graduate Poster award. Congratulations, Kassy!"
  },
  {
    "objectID": "posts/firacode/index.html",
    "href": "posts/firacode/index.html",
    "title": "Using Fira Code Ligatures in RStudio",
    "section": "",
    "text": "To quote the Fira Code README:\n\nProblem\nProgrammers use a lot of symbols, often encoded with several characters. For the human brain, sequences like -&gt;, &lt;= or := are single logical tokens, even if they take two or three characters on the screen. Your eye spends a non-zero amount of energy to scan, parse and join multiple characters into a single logical one. Ideally, all programming languages should be designed with full-fledged Unicode symbols for operators, but that’s not the case yet.\nSolution\nFira Code is a free monospaced font containing ligatures for common programming multi-character combinations. This is just a font rendering feature: underlying code remains ASCII-compatible. This helps to read and understand code faster. For some frequent sequences like .. or //, ligatures allow us to correct spacing."
  },
  {
    "objectID": "posts/firacode/index.html#what-are-ligature-fonts",
    "href": "posts/firacode/index.html#what-are-ligature-fonts",
    "title": "Using Fira Code Ligatures in RStudio",
    "section": "",
    "text": "To quote the Fira Code README:\n\nProblem\nProgrammers use a lot of symbols, often encoded with several characters. For the human brain, sequences like -&gt;, &lt;= or := are single logical tokens, even if they take two or three characters on the screen. Your eye spends a non-zero amount of energy to scan, parse and join multiple characters into a single logical one. Ideally, all programming languages should be designed with full-fledged Unicode symbols for operators, but that’s not the case yet.\nSolution\nFira Code is a free monospaced font containing ligatures for common programming multi-character combinations. This is just a font rendering feature: underlying code remains ASCII-compatible. This helps to read and understand code faster. For some frequent sequences like .. or //, ligatures allow us to correct spacing."
  },
  {
    "objectID": "posts/firacode/index.html#step-1-install-the-fira-code-font-family",
    "href": "posts/firacode/index.html#step-1-install-the-fira-code-font-family",
    "title": "Using Fira Code Ligatures in RStudio",
    "section": "Step 1: Install the Fira Code Font Family",
    "text": "Step 1: Install the Fira Code Font Family\n\nWindows\n\nGo to https://github.com/tonsky/FiraCode/releases/latest\nDownload the Fira_Code_vX.X.zip file\nExtract all the zipped files to a folder on your computer\nOpen the extracted folder and open the ttf subfolder\nHighlight all files in the tff subfolder\nRight-click on one of these highlighted files to open the context menu\nSelect “Install” from the context menu\n\n\n\n\nAnimation showing how to install the Fira Code font family on Windows\n\n\n\n\nMacOS\n\nFollow the same instructions 1(a) to 1(f) from Windows\nSelect “Open” or “Open with Font Book” from the context menu\nSelect “Install Font”\n\n\n\nLinux\n\nSee Installing with a package manager"
  },
  {
    "objectID": "posts/firacode/index.html#step-2-change-the-rstudio-editor-font",
    "href": "posts/firacode/index.html#step-2-change-the-rstudio-editor-font",
    "title": "Using Fira Code Ligatures in RStudio",
    "section": "Step 2: Change the RStudio Editor Font",
    "text": "Step 2: Change the RStudio Editor Font\n\nIn RStudio, open the “Tools” menu and select “Global Options…”\nSelect the “Appearance” tab and change the “Editor Font” to Fira Code.\nClick the “OK” button.\n\n\n\n\nAnimation showing how to change the RStudio Editor Font to Fira Code"
  },
  {
    "objectID": "posts/firacode/index.html#step-3-using-ligatures-in-rstudio",
    "href": "posts/firacode/index.html#step-3-using-ligatures-in-rstudio",
    "title": "Using Fira Code Ligatures in RStudio",
    "section": "Step 3: Using Ligatures in RStudio",
    "text": "Step 3: Using Ligatures in RStudio\nNow, certain character combinations will automatically be transformed into beautiful ligatures as you type them. The best part is that, although the ligatures may appear to be one character, they are still represented in your files as the individual characters and thus can still be edited without problem.\n\n\n\nAnimation showing the use of ligatures in RStudio\n\n\nHere are some of my favorite Fira Code ligatures for R.\n\nAssignment operators\nx &lt;- 10 becomes \n23 -&gt; y becomes \n\n\nRelational operators\nx &lt;= y becomes \nx &gt;= y becomes \nx == y becomes \nx != y becomes \n\n\nNative pipe operator\nx |&gt; sqrt() becomes"
  },
  {
    "objectID": "posts/dischargeR01/index.html",
    "href": "posts/dischargeR01/index.html",
    "title": "NIMH R01 Grant",
    "section": "",
    "text": "I am very pleased to announce that we have been awarded an R01 (Research Project) Grant from the National Institute of Mental Health (R01-MH125740) to study the use of multimodal information and machine learning to estimate the discharge readiness of psychiatric inpatients with severe mental illness. The project period is from 04/2021 through 02/2025 with a total funding of $1,160,953."
  },
  {
    "objectID": "posts/dischargeR01/index.html#project-team",
    "href": "posts/dischargeR01/index.html#project-team",
    "title": "NIMH R01 Grant",
    "section": "Project Team",
    "text": "Project Team\n\nMPI: Justin Baker, McLean Hospital, Harvard Medical School\nMPI: Louis-Philippe Morency, Carnegie Mellon University\nCo-I: Jeffrey Girard, University of Kansas"
  },
  {
    "objectID": "posts/dischargeR01/index.html#project-summary",
    "href": "posts/dischargeR01/index.html#project-summary",
    "title": "NIMH R01 Grant",
    "section": "Project Summary",
    "text": "Project Summary\nWhich psychiatric symptoms and behaviors are the most important to assess and manage during critical points in psychiatric healthcare, such as the time leading up to hospital discharge? At present, psychiatry lacks objective tests that could inform this and other clinically challenging–and potentially costly–decisions. Establishing valid objective markers of psychiatric disease processes is especially challenging compared with the development of biomarkers in other fields. One key challenge is lack of available data from psychiatrically ill patients during key periods in their care trajectory, which the present project seeks to address. A second major challenge, also addressed as a core feature in this project, is the complex, context-dependence of human behavioral expression, which greatly complicates efforts to establish robust, objective measures that reflect underlying mental health disease processes. This project will address both barriers, introducing a new computational framework, named Context-Adaptive Multimodal Informatics, to identify and evaluate behavioral biomarkers related to discharge-readiness and symptoms in severe mental illness. The project aims to address five fundamental research challenges: (1) Acquire a multimodal psychiatric discharge-planning dataset of 400 inpatients with severe mental illness; (2) Create self-aware linear and neural models to identify multimodal behavioral biomarkers; (3) Develop context-sensitive linear and neural models to contextualize behavioral biomarkers and quantify the influence of context on behavior; (4) Build a new adaptive assessment planning framework which creates a personalized patient analysis to rank contexts and modalities for the next assessment session; (5) Assess the trustworthiness and generalizability of our measurements, models, and insights. This research will improve basic understanding of social context and behavioral biomarkers, build objective measures for mental health assessment, and more broadly, pave the way for a restructured care-delivery system in which resources are allocated intelligently to ensure assessments are informative with respect to desired clinical objectives."
  },
  {
    "objectID": "posts/dischargeR01/index.html#public-health-relevance",
    "href": "posts/dischargeR01/index.html#public-health-relevance",
    "title": "NIMH R01 Grant",
    "section": "Public Health Relevance",
    "text": "Public Health Relevance\nCharting mental illness trajectories to determine when, where, and how to intervene is a key objective of the NIMH mission to transform the understanding and treatment of mental illness. This project will: (1) deepen our understanding of the trajectory leading to hospitalization discharge with knowledge about behavioral biomarkers and their relationship to symptoms and discharge-readiness; (2) contribute to the knowledge on how social context impacts patient’s behaviors since some symptoms or predictive biomarkers may only be seen in a specific context such as social or solitary interactions; and (3) contribute a credible, objective framework for behavioral measurement by defining interpretable biomarkers and latent factors for symptoms and discharge assessment. The project will also enable a judicious use of remote sensing and non-direct clinical encounters (e.g., telemedicine), which are increasingly viewed as essential capabilities in the medical fields to reduce costs while maintaining or elevating care standards."
  },
  {
    "objectID": "posts/acii2024/index.html",
    "href": "posts/acii2024/index.html",
    "title": "ACII 2024 Conference",
    "section": "",
    "text": "I co-organized a full-day workshop at ACII 2024 on “Embracing Ambiguity and Subjectivity in Emotion Research” (EASE). Please visit the workshop website for more information. I also gave an invited talk, described below.\n\nGirard, J. M. (2024). Theoretical foundations of ambiguity and subjectivity. Invited talk presented at the Embracing Ambiguity and Subjectivity in Emotion Research Workshop, AAAC International Conference on Affective Computing and Intelligent Interaction (ACII), Glasgow, UK.\n\n\nSlides\n\n\n\nRecording"
  },
  {
    "objectID": "people/staff/yermol_dasha.html",
    "href": "people/staff/yermol_dasha.html",
    "title": "Dasha Yermol",
    "section": "",
    "text": "Home\n    People\n    Graduate Students\n    Dasha Yermol"
  },
  {
    "objectID": "people/staff/yermol_dasha.html#biography",
    "href": "people/staff/yermol_dasha.html#biography",
    "title": "Dasha Yermol",
    "section": "Biography",
    "text": "Biography\nDasha Yermol studies how emotion perception is influenced by individual differences (e.g., mental health and personality), the functions of affective synchrony between interacting dyads, and differences in emotion expression due to social factors (e.g., context, social roles).\nDasha obtained her B.A. in psychology with comprehensive honors from the University of Wisconsin-Madison in 2021. After graduating from UW-Madison, she was a lab manager at a child development research lab for two years, studying how children learn language using eye-tracking methods.\nDasha is currently a PhD Student in the Brain, Behavior, and Quantitative Science (BBQ) program. She is also a graduate teaching assistant for undergraduate psychology courses. Outside of the lab, Dasha enjoys drinking great coffee, being outdoors (biking, swimming, hiking), and tending to her many houseplants."
  },
  {
    "objectID": "people/staff/yermol_dasha.html#education",
    "href": "people/staff/yermol_dasha.html#education",
    "title": "Dasha Yermol",
    "section": "Education",
    "text": "Education\n\nUniversity of Kansas | Lawrence, KS, USA PhD Student (Brain, Behavior, and Quantitative Science), 2023–Current\nUniversity of Wisconsin-Madison | Madison, WI, USA BS in Psychology | 2021"
  },
  {
    "objectID": "people/staff/simmons_aaron.html",
    "href": "people/staff/simmons_aaron.html",
    "title": "Aaron Simmons",
    "section": "",
    "text": "Home\n    People\n    Graduate Students\n    Aaron Simmons"
  },
  {
    "objectID": "people/staff/simmons_aaron.html#biography",
    "href": "people/staff/simmons_aaron.html#biography",
    "title": "Aaron Simmons",
    "section": "Biography",
    "text": "Biography\nAaron Matthew Simmons’s main research interests include Bayesian statistics and modeling, latent variable modeling, scale development, and how new modeling frameworks in general can be applied and leveraged to study a wide range of cognitive and behavioral phenomena. Further interests include the use and development of neuroscientific methods, especially EEG/ERP and eye-tracking methods, to study cognition and behavior.\nHe completed his undergraduate Bachelor of Science degree at the University of California, Davis in 2015. After graduating, he spent many years working at the Center for Mind and Brain at UC Davis with Dr. Steven J. Luck. Currently, he is a student in the Brain, Behavior, and Quantitative Science (BBQ) program in the Department of Psychology at the University of Kansas, and a member of the Affective Communication and Computing lab under Dr. Jeffrey Girard. Personal interests include coffee, philosophy, chess, and BBQ (the kind you eat)."
  },
  {
    "objectID": "people/staff/simmons_aaron.html#education",
    "href": "people/staff/simmons_aaron.html#education",
    "title": "Aaron Simmons",
    "section": "Education",
    "text": "Education\n\nUniversity of Kansas | Lawrence, KS, USA PhD Student (Brain, Behavior, and Quantitative Science), 2023–Current\nUniversity of California: Davis | Davis, CA, USA BS in Psychology (Mathematics Emphasis) | 2015"
  },
  {
    "objectID": "people/staff/gray_kassandra.html",
    "href": "people/staff/gray_kassandra.html",
    "title": "Kassandra Gray",
    "section": "",
    "text": "Home\n    People\n    Graduate Students\n    Kassandra Gray"
  },
  {
    "objectID": "people/staff/gray_kassandra.html#biography",
    "href": "people/staff/gray_kassandra.html#biography",
    "title": "Kassandra Gray",
    "section": "Biography",
    "text": "Biography\nKassandra Gray’s research interests include interpersonal interactions, emotional expression and communication, individual differences, personality, anxiety, and depression. She is interested in how emotional expression and communication manifest in the therapeutic relationship and the influential role of individual differences (e.g., personality, anxiety, and depression).\nShe completed her undergraduate Bachelor of Science degree at Southern Nazarene University in May 2022 and is currently a doctoral student in the Clinical Psychological Science program in the Department of Psychology at the University of Kansas. She is a graduate teaching assistant (GTA) for undergraduate psychology courses and is also a member of the Affective Communication and Computing lab."
  },
  {
    "objectID": "people/staff/gray_kassandra.html#education",
    "href": "people/staff/gray_kassandra.html#education",
    "title": "Kassandra Gray",
    "section": "Education",
    "text": "Education\n\nUniversity of Kansas | Lawrence, KS, USA PhD Student (Clinical Psychological Science), 2022–Current\nSouthern Nazarene University | Bethany, OK, USA BS in Psychology | 2018–2022"
  },
  {
    "objectID": "people/alumni.html",
    "href": "people/alumni.html",
    "title": "AffCom Lab",
    "section": "",
    "text": "Home\n    People\n    Alumni"
  },
  {
    "objectID": "people/alumni.html#alumni",
    "href": "people/alumni.html#alumni",
    "title": "AffCom Lab",
    "section": "Alumni",
    "text": "Alumni"
  },
  {
    "objectID": "news.html",
    "href": "news.html",
    "title": "AffCom Lab",
    "section": "",
    "text": "Home\n    News\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTenure Talk\n\n\n\npresentation\n\n\n\nReflecting on 5 Years at KU\n\n\n\n\n\nNov 17, 2025\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nISTSS 2025 Conference\n\n\n\nannouncement\n\nconference\n\n\n\nInternational Society for Traumatic Stress Studies\n\n\n\n\n\nSep 18, 2025\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Fira Code Ligatures in RStudio\n\n\n\nteaching\n\ndata science\n\n\n\nA guide to my favorite font for data science\n\n\n\n\n\nJun 9, 2025\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nHiTOP 2025 Conference\n\n\n\nannouncement\n\nconference\n\n\n\nHierarchical Taxonomy of Psychopathology\n\n\n\n\n\nMar 19, 2025\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nTIPS 2024 Conference\n\n\n\nannouncement\n\nconference\n\n\n\nTechnology in Psychiatry Summit\n\n\n\n\n\nDec 7, 2024\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nAI Transcription from R using Whisper: Part 3\n\n\n\nteaching\n\naudio\n\nAI\n\n\n\nEasing the WSL2 Setup with Docker\n\n\n\n\n\nNov 10, 2024\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nSAI 2024 Workshop\n\n\n\nannouncement\n\nconference\n\n\n\nSocial Artificial Intelligence Workshop\n\n\n\n\n\nSep 20, 2024\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nACII 2024 Conference\n\n\n\nannouncement\n\nconference\n\n\n\nAffective Computing and Intelligent Interaction\n\n\n\n\n\nSep 15, 2024\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nAI Transcription from R using Whisper: Part 2\n\n\n\nteaching\n\naudio\n\nAI\n\n\n\nSpeedup on Windows using WSL2 and CUDA\n\n\n\n\n\nAug 16, 2024\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nAI Transcription from R using Whisper: Part 1\n\n\n\nteaching\n\naudio\n\nAI\n\n\n\nTutorial on Using AI Transcription\n\n\n\n\n\nAug 14, 2024\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nMMM 2024 Conference\n\n\n\nannouncement\n\nconference\n\n\n\nModern Modeling Methods\n\n\n\n\n\nJul 1, 2024\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nAPS 2024 Conference\n\n\n\nannouncement\n\nconference\n\n\n\nAssociation for Psychological Science\n\n\n\n\n\nMay 26, 2024\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nHiTOP 2024 Conference\n\n\n\nannouncement\n\nconference\n\n\n\nHierarchical Taxonomy of Psychopathology\n\n\n\n\n\nMar 13, 2024\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nSAS 2024 Conference\n\n\n\nannouncement\n\nconference\n\n\n\nSociety for Affective Science\n\n\n\n\n\nMar 2, 2024\n\n\nDasha Yermol\n\n\n\n\n\n\n\n\n\n\n\n\nPostdoctoral Position 2024\n\n\n\nannouncement\n\nrecruitment\n\n\n\nInformation for applicants to the psychology graduate program to start in Fall 2024\n\n\n\n\n\nDec 11, 2023\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nRecruitment 2024\n\n\n\nannouncement\n\nrecruitment\n\n\n\nInformation for applicants to the psychology graduate program to start in Fall 2024\n\n\n\n\n\nSep 28, 2023\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nDynAMoS Database\n\n\n\nconference\n\npresentation\n\n\n\nDynamic Affective Movie Clip Database for Subjectivity Analysis\n\n\n\n\n\nSep 13, 2023\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nMVAC Tutorial\n\n\n\nteaching\n\npresentation\n\n\n\nMeasurement Validation in Affective Computing\n\n\n\n\n\nSep 10, 2023\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nGPSP 2023 Convention\n\n\n\nannouncement\n\nconference\n\n\n\nGreat Plains Student Psychology Convention\n\n\n\n\n\nApr 11, 2023\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nRow-wise means in dplyr\n\n\n\nteaching\n\ndata science\n\n\n\nGuide to calculating mean scores in dplyr\n\n\n\n\n\nMar 18, 2023\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nAdventures in Moderation\n\n\n\nteaching\n\npresentation\n\n\n\nProbing Interactions in ANOVA and Regression\n\n\n\n\n\nNov 18, 2022\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nICMI 2022 Paper\n\n\n\nannouncement\n\npaper\n\n\n\nToward causal understanding of therapist-client relationships\n\n\n\n\n\nNov 7, 2022\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Research Grant\n\n\n\nannouncement\n\nfunding\n\n\n\nNovel Scalable Mental Health Screening Procedures on Ubiquitous Sensing Devices\n\n\n\n\n\nOct 28, 2022\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nNIMH R21 Grant\n\n\n\nannouncement\n\nfunding\n\n\n\nMultimodal Dynamics of Parent-Child Interactions and Suicide Risk\n\n\n\n\n\nSep 8, 2022\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nRecruitment 2023\n\n\n\nannouncement\n\nrecruitment\n\n\n\nInformation for applicants to the psychology graduate program to start in Fall 2023\n\n\n\n\n\nAug 30, 2022\n\n\nJeffrey Girard\n\n\n\n\n\n\n\n\n\n\n\n\nNIMH R01 Grant\n\n\n\nannouncement\n\nfunding\n\n\n\nContext-Adaptive Multimodal Informatics for Psychiatric Discharge Planning\n\n\n\n\n\nApr 15, 2021\n\n\nJeffrey Girard\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Affective Communication & Computing Lab",
    "section": "",
    "text": "We are an interdisciplinary research group based within the Department of Psychology at the University of Kansas. We develop, validate, and apply computational methods to advance the study of emotion, interpersonal communication, and mental health.\n\n\n\n\n\n\n\n\nNote\n\n\n\n2025-09-07: Many parts of the website have been redesigned. Take a look around!"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "AffCom Lab",
    "section": "",
    "text": "Home\n    Contact"
  },
  {
    "objectID": "contact.html#visit-us",
    "href": "contact.html#visit-us",
    "title": "AffCom Lab",
    "section": "Visit Us",
    "text": "Visit Us\nThe AffCom Lab is located in Fraser Hall Room #455 at the University of Kansas.\n\nStreet Address: 1415 Jayhawk Blvd, Lawrence, KS 66044\nVisitor Parking: 1218 Mississippi St, Lawrence, KS 66045"
  },
  {
    "objectID": "contact.html#email-us",
    "href": "contact.html#email-us",
    "title": "AffCom Lab",
    "section": "Email Us",
    "text": "Email Us"
  },
  {
    "objectID": "girard/index.html",
    "href": "girard/index.html",
    "title": "Girard Redirect",
    "section": "",
    "text": "Please follow this link."
  },
  {
    "objectID": "mvac/index.html",
    "href": "mvac/index.html",
    "title": "MVAC Redirect",
    "section": "",
    "text": "Please follow this link."
  },
  {
    "objectID": "opportunities.html",
    "href": "opportunities.html",
    "title": "AffCom Lab",
    "section": "",
    "text": "Home\n    Opportunities"
  },
  {
    "objectID": "opportunities.html#for-undergraduate-students",
    "href": "opportunities.html#for-undergraduate-students",
    "title": "AffCom Lab",
    "section": "For Undergraduate Students",
    "text": "For Undergraduate Students\nThe lab is frequently home to undergraduate students from the Psychology and Behavioral Neuroscience departments (and occasionally others).\nUndergraduate students work as research assistants, learning about the scientific method and assisting in the collection and processing of research data. Many enroll in PSYC 480 and receive credit hours for this work. Research assistants attend weekly lab meetings and can expect a letter of recommendation from Dr. Girard describing their work in and contributions to the lab (please give one month of advanced notice). Opportunities to earn co-authorship on research products (e.g., conference presentations) are also occasionally offered to advanced research assistants.\nHonors students will also occasionally design and complete an independent research project (i.e., honors thesis) under the supervision of Dr. Girard over the course of their senior year. These students typically apply for (and have often won) research awards from the Center for Undergraduate Research and make oral or poster presentations about their work at the Undergraduate Research Symposium.\nApply to be a Research Assistant"
  },
  {
    "objectID": "opportunities.html#for-prospective-graduate-students",
    "href": "opportunities.html#for-prospective-graduate-students",
    "title": "AffCom Lab",
    "section": "For Prospective Graduate Students",
    "text": "For Prospective Graduate Students\nThe KU Psychology Department grants doctoral degrees (i.e., PhDs) in three concentrations: Brain, Behavior, and Quantitative Science (BBQ); Clinical Psychological Science (CPS); and Social Psychology; note that the department does not offer a terminal Masters degree and that the CPS concentration is focused on adult psychopathology rather than child psychopathology.\nDr. Girard advises graduate students in the BBQ and CPS concentration and is a particularly good match for students with interests that span both areas (e.g., CPS students interested in advanced methods or BBQ students interested in medical applications).\nGraduate students will complete coursework in the department and conduct research in the lab. They will receive a world-class education in the scientific method and have many opportunities to contribute to ongoing research projects. More information about each admissions cycle can be found on the News page.\nLearn About Graduate Admissions"
  },
  {
    "objectID": "opportunities.html#for-prospective-postdocs-and-staff",
    "href": "opportunities.html#for-prospective-postdocs-and-staff",
    "title": "AffCom Lab",
    "section": "For Prospective Postdocs and Staff",
    "text": "For Prospective Postdocs and Staff\nPostdoctoral fellowships and other staff positions in the lab will be made available as funding permits. Please check the News page for more information."
  },
  {
    "objectID": "people/staff/girard_jeffrey.html",
    "href": "people/staff/girard_jeffrey.html",
    "title": "Jeffrey Girard",
    "section": "",
    "text": "Home\n    People\n    Research Staff\n    Jeffrey Girard"
  },
  {
    "objectID": "people/staff/girard_jeffrey.html#biography",
    "href": "people/staff/girard_jeffrey.html#biography",
    "title": "Jeffrey Girard",
    "section": "Biography",
    "text": "Biography\nDr. Jeffrey Girard studies how emotions are expressed through verbal and nonverbal behavior, as well as how interpersonal communication is influenced by individual differences (e.g., personality and mental health) and social factors (e.g., culture and context). This work is deeply interdisciplinary and draws insights and tools from various areas of social science, computer science, statistics, and medicine.\nHe completed his doctoral training under the mentorship of Dr. Jeffrey Cohn (nonverbal behavior, machine learning, depression) and Dr. Aidan Wright (interpersonal functioning, statistics, personality). He then completed his predoctoral clinical internship at the University of Mississippi Medical Center (inpatient psychiatry, trauma treatment, neuropsychology) and a two-year postdoctoral research fellowship at Carnegie Mellon University under the mentorship of Dr. Louis-Philippe Morency (verbal behavior, machine learning, computer science).\nHe is now an Associate Professor in the department of Psychology at the University of Kansas, where he directs the Affective Communication and Computing lab and the Brain, Behavior, and Quantitative Science (BBQ) program. He teaches undergraduate and graduate courses on psychological assessment, data science, and statistics within the department and is a co-founder of SMaRT Workshops, which offers numerous methodological training workshops to faculty and students. He is also the developer and maintainer of numerous open-source research software packages (see Resources)."
  },
  {
    "objectID": "people/staff/girard_jeffrey.html#experience",
    "href": "people/staff/girard_jeffrey.html#experience",
    "title": "Jeffrey Girard",
    "section": "Experience",
    "text": "Experience\n\nUniversity of Kansas | Lawrence, KS, USA Associate Professor of Psychology | 2025–Current Assistant Professor of Psychology | 2020–2025 Wright Faculty Scholar | 2020–2025 Director of the BBQ Doctoral Program | 2022–Current  Co-Director of the Kansas Data Science Consortium | 2023-Current\nCarnegie Mellon University | Pittsburgh, PA, USA Postdoctoral Researcher, School of Computer Science | 2018–2020\nUniversity of Mississippi Medical Center | Jackson, MS, USA Clinical Intern, Department of Psychiatry | 2017–2018"
  },
  {
    "objectID": "people/staff/girard_jeffrey.html#journal-editing",
    "href": "people/staff/girard_jeffrey.html#journal-editing",
    "title": "Jeffrey Girard",
    "section": "Journal Editing",
    "text": "Journal Editing\n\nNPP Digital Psychiatry and Neuroscience  Consulting Editor | 2023–Current\nJournal of Psychopathology and Clinical Science  Consulting Editor | 2023–Current\nIEEE Transactions on Affective Computing  Associate Editor | 2022–2025\nCollabra Psychology  Associate Editor | 2021–Current\nClinical Psychological Science  Consulting Editor | 2021–Current\nPsychological Assessment  Consulting Editor | 2020–2022"
  },
  {
    "objectID": "people/staff/girard_jeffrey.html#education",
    "href": "people/staff/girard_jeffrey.html#education",
    "title": "Jeffrey Girard",
    "section": "Education",
    "text": "Education\n\nUniversity of Pittsburgh | Pittsburgh, PA, USA PhD in Psychology (Clinical) | 2013–2018\nUniversity of Pittsburgh | Pittsburgh, PA, USA MS in Psychology (Clinical) | 2010–2013\nUniversity of Washington | Seattle, WA, USA BA in Psychology & Philosophy (Honors) | 2005–2008"
  },
  {
    "objectID": "people/staff/jun_daiil.html",
    "href": "people/staff/jun_daiil.html",
    "title": "Daiil Jun",
    "section": "",
    "text": "Home\n    People\n    Graduate Students\n    Daiil Jun"
  },
  {
    "objectID": "people/staff/jun_daiil.html#biography",
    "href": "people/staff/jun_daiil.html#biography",
    "title": "Daiil Jun",
    "section": "Biography",
    "text": "Biography\nDaiil Jun’s research interests include the heterogeneity of psychopathology, particularly the intersection between addictive behaviors and serious mental illness. He is especially interested in understanding transdiagnostic mechanisms of reward dysfunction that connect maladaptive eating, substance use, and serious mental illness. He is also an enthusiastic learner of advanced statistical methods and computational approaches.\nHe completed his Bachelor of Arts degree at Handong Global University in May 2020 and is currently a doctoral student in the Clinical Psychological Science program in the Department of Psychology at the University of Kansas. He serves as a graduate research assistant (GRA) for the Kansas Data Science Consortium. He is also a member of the Affective Communication and Computing Lab under Dr. Girard’s mentorship and works jointly with the Health Behavior and Technology Lab under Dr. Fazzino’s mentorship."
  },
  {
    "objectID": "people/staff/jun_daiil.html#education",
    "href": "people/staff/jun_daiil.html#education",
    "title": "Daiil Jun",
    "section": "Education",
    "text": "Education\n\nUniversity of Kansas | Lawrence, KS, USA PhD Student (Clinical Psychological Science), 2021–Current\nHandong Global University | Pohang, South Korea BA in Counseling Psychology | 2013–2020"
  },
  {
    "objectID": "people/staff/yang_yuanyuan.html",
    "href": "people/staff/yang_yuanyuan.html",
    "title": "Yuanyuan Yang",
    "section": "",
    "text": "Home\n    People\n    Graduate Students\n    Yuanyuan Yang"
  },
  {
    "objectID": "people/staff/yang_yuanyuan.html#biography",
    "href": "people/staff/yang_yuanyuan.html#biography",
    "title": "Yuanyuan Yang",
    "section": "Biography",
    "text": "Biography\nYuanyuan is interested in exploring the structure of psychopathology by exploring the connections between symptoms and verbal and non-verbal behaviors (e.g., language processing, vocal, facial recognition). Further, Yuanyuan hopes to leverage these insights to enhance the treatment efficacy and accessibility.\nYuanyuan obtained B.S. in Psychology and Applied Mathematics and Statistics, and M.A. in General Psychology at Stony Brook University. Currently, she is a first year PhD student in the Clinical Psychological Science program in the Department of Psychology at the University of Kansas. She is a graduate teaching assistant (GTA) for undergraduate psychology courses and is also a member of the Affective Communication and Computing lab.\nOutside of lab, she takes care of bunch of plants and two cats (Pepper&Ronny). She likes running, cooking, and playing the piano."
  },
  {
    "objectID": "people/staff/yang_yuanyuan.html#education",
    "href": "people/staff/yang_yuanyuan.html#education",
    "title": "Yuanyuan Yang",
    "section": "Education",
    "text": "Education\n\nUniversity of Kansas | Lawrence, KS, USA PhD Student (Clinical Psychological Science), 2023–Current\nSUNY University at Stony Brook | Stony Brook, NY, USA MS in Psychology | 2023\nSUNY University at Stony Brook | Stony Brook, NY, USA BS in Psychology, Applied Mathematics & Statistics | 2021"
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "AffCom Lab",
    "section": "",
    "text": "Home\n    People"
  },
  {
    "objectID": "people.html#research-staff",
    "href": "people.html#research-staff",
    "title": "AffCom Lab",
    "section": "Research Staff",
    "text": "Research Staff\n\n\n\n\n\n\n\n\n\n\nJeffrey Girard\n\n\nLab Director\n\n\n\n\n\n\n\n\n\n\n\n\nKassandra Gray\n\n\nPhD Student\n\n\n\n\n\n\n\n\n\n\n\n\nDaiil Jun\n\n\nPhD Student\n\n\n\n\n\n\n\n\n\n\n\n\nAaron Simmons\n\n\nPhD Student\n\n\n\n\n\n\n\n\n\n\n\n\nYuanyuan Yang\n\n\nPhD Student\n\n\n\n\n\n\n\n\n\n\n\n\nDasha Yermol\n\n\nPhD Student\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people.html#research-assistants",
    "href": "people.html#research-assistants",
    "title": "AffCom Lab",
    "section": "Research Assistants",
    "text": "Research Assistants"
  },
  {
    "objectID": "people.html#alumni",
    "href": "people.html#alumni",
    "title": "AffCom Lab",
    "section": "Alumni",
    "text": "Alumni\nClick to view a list of former research staff and assistants."
  },
  {
    "objectID": "posts/aps2024/index.html",
    "href": "posts/aps2024/index.html",
    "title": "APS 2024 Conference",
    "section": "",
    "text": "Kassy and I will be attending the 2024 APS Convention in San Francisco this weekend. Our presentations are below. See you there!\n\nGray, K. & Girard, J. M. (2024). Is personality related to social well-being? Associations between social health measures and personality pathology dimensions. Poster presented at the Association for Psychological Science Convention, San Francisco, CA.\nGirard, J. M. (2024). COVID-19’s effect on the rhythms of smiling on social media. Paper presented at the Association for Psychological Science Convention, San Francisco, CA."
  },
  {
    "objectID": "posts/dynamos/index.html",
    "href": "posts/dynamos/index.html",
    "title": "DynAMoS Database",
    "section": "",
    "text": "I gave this presentation about our conference paper at the Affective Computing and Intelligent Interaction (ACII 2023) conference in Boston, MA. Here are my slides:\n\n\nNote. To view the slideshow in full screen mode, click inside it and press the “f” key on your keyboard. You can also use the menu by clicking on the icon at the bottom-left of each slide.\nYou can learn more about the database at dynamos.mgb.org."
  },
  {
    "objectID": "posts/google2022/index.html",
    "href": "posts/google2022/index.html",
    "title": "Google Research Grant",
    "section": "",
    "text": "I am very pleased to announce that the AffCom Lab has been granted a $30,000 gift from Google LLC to support our work on novel, scalable mental health screening procedures! Specifically, we plan to use these funds to further integrate computer vision, natural language processing, ambulatory assessment, and passive sensing into new screening procedures for affective disorders. This is also an opportunity to deepen the lab’s collaborative ties with industry leaders in digital mental health. Stay tuned for more information in the next few months about a new project we plan to launch in this space!"
  },
  {
    "objectID": "posts/hitop2024/index.html",
    "href": "posts/hitop2024/index.html",
    "title": "HiTOP 2024 Conference",
    "section": "",
    "text": "Yuanyuan, Kassy, and I will be attending the 2024 HiTOP Conference in San Diego this weekend. Our presentations are below. See you there!\n\nYang, Y., Gray, K., Sprunger, J. G., & Girard, J. M. (2024). Exploring the structure of personality symptoms in a trauma-exposed community sample. Poster presented at the Hierarchical Taxonomy of Psychopathology (HiTOP) Conference, San Diego, CA.\nGray, K. & Girard, J. M. (2024). Is personality related to social well-being? Associations between social health measures and personality pathology dimensions. Poster presented at the Hierarchical Taxonomy of Psychopathology (HiTOP) Conference, San Diego, CA.\nGirard, J. M., Chung, Y., Ravichandran, C., Ongur, D., Cohen, B. M., & Baker, J. T. (2024). Transdiagnostic structure of affective and non-affective psychosis symptoms. Paper presented at the Hierarchical Taxonomy of Psychopathology (HiTOP) Conference, San Diego, CA."
  },
  {
    "objectID": "posts/istss2025/index.html",
    "href": "posts/istss2025/index.html",
    "title": "ISTSS 2025 Conference",
    "section": "",
    "text": "Yuanyuan and I will be attending the 2025 ISTSS Conference in Baltimore. Our presentations are below. See you there!\n\nYang, Y., Jun, D., Welch, B. M., Sprunger, J. G., & Girard, J. M. (2025). Computational Analysis of Multimodal Behavior in Telehealth Trauma Interviews. Paper presented at the Annual Meeting of the International Society for Traumatic Stress Studies, Baltimore, MD.\n\n\n\nSprunger, J. G., Welch, B. M., Chard, K., & Girard, J. M. (2025). Predicting PTSD Treatment Response in Cognitive Processing Therapy Using Computerized Patient Behavior Analysis of Baseline Trauma Interviews. Paper presented at the Annual Meeting of the International Society for Traumatic Stress Studies, Baltimore, MD."
  },
  {
    "objectID": "posts/moderation2022/index.html",
    "href": "posts/moderation2022/index.html",
    "title": "Adventures in Moderation",
    "section": "",
    "text": "I was invited to give a 75 minute presentation on testing, visualizing, and probing interactions in ANOVA and regression models at the Social Psychology Professional Seminar at the University of Kansas. Here are my slides:\n\n\nNote. To view the slideshow in full screen mode, click inside it and press the “f” key on your keyboard. You can also use the menu by clicking on the icon at the bottom-left of each slide."
  },
  {
    "objectID": "posts/mvac2023/index.html",
    "href": "posts/mvac2023/index.html",
    "title": "MVAC Tutorial",
    "section": "",
    "text": "I gave a three-hour tutorial on Measurement Validation in Affective Computing at the Affective Computing and Intelligent Interaction (ACII 2023) conference in Boston, MA. Here are my slides:\nNote. To view the slideshow in full screen mode, click inside it and press the “f” key on your keyboard. You can also use the menu by clicking on the icon at the bottom-left of each slide."
  },
  {
    "objectID": "posts/mvac2023/index.html#recommended-articles",
    "href": "posts/mvac2023/index.html#recommended-articles",
    "title": "MVAC Tutorial",
    "section": "Recommended Articles",
    "text": "Recommended Articles\n\nCizek, G. J. (2016). Validating test score meaning and defending test score use: Different aims, different methods. Assessment in Education: Principles, Policy & Practice, 23(2), 212–225. https://doi.org/10.1080/0969594x.2015.1063479\nFlake, J. K., & Fried, E. I. (2020). Measurement schmeasurement: Questionable measurement practices and how to avoid them. Advances in Methods and Practices in Psychological Science, 3(4), 456–465. https://doi.org/10/ghnbdg\nFlake, J. K., Pek, J., & Hehman, E. (2017). Construct validation in social and personality research: Current practice and recommendations. Social Psychological and Personality Science, 8(4), 370–378. https://doi.org/10.1177/1948550617693063\nFlora, D. B. (2020). Your coefficient alpha is probably wrong, but which coefficient omega is right? A tutorial on using R to obtain better reliability estimates. Advances in Methods and Practices in Psychological Science, 3(4), 484–501. https://doi.org/10.1177/2515245920951747\nGehlbach, H., & Brinkworth, M. E. (2011). Measure twice, cut down error: A process for enhancing the validity of survey scales. Review of General Psychology, 15(4), 380–387. https://doi.org/10/bnn2s3\nJacobucci, R., & Grimm, K. J. (2020). Machine Learning and Psychological Research: The Unexplored Effect of Measurement. Perspectives on Psychological Science, 15(3), 809–816. https://doi.org/10/ghdp3b\nMessick, S. (1995). Validity of psychological assessment: Validation of inferences from persons’ responses and performances as scientific inquiry into score meaning. American Psychologist, 50(9), 741–749. https://doi.org/10.1037/0003-066x.50.9.741\nQiu, L., Chan, S. H. M., & Chan, D. (2017). Big data in social and psychological science: Theoretical and methodological issues. Journal of Computational Social Science, 1(1), 59–66. https://doi.org/10.1007/s42001-017-0013-6\nten Hove, D., Jorgensen, T. D., & van der Ark, L. A. (2022). Updated guidelines on selecting an intraclass correlation coefficient for interrater reliability, with applications to incomplete observational designs. Psychological Methods. https://doi.org/10.1037/met0000516\nWeidman, A. C., Steckler, C. M., & Tracy, J. L. (2017). The jingle and jangle of emotion assessment: Imprecise measurement, casual scale usage, and conceptual fuzziness in emotion research. Emotion, 17(2), 267–295. https://doi.org/10.1037/emo0000226"
  },
  {
    "objectID": "posts/mvac2023/index.html#recommended-books",
    "href": "posts/mvac2023/index.html#recommended-books",
    "title": "MVAC Tutorial",
    "section": "Recommended Books",
    "text": "Recommended Books\n\nAERA, APA, & NCME. (2014). Standards for educational and psychological testing. American Educational Research Association. https://www.testingstandards.net/open-access-files.html\nBowden, S. C. (Ed.). (2017). Neuropsychological assessment in the age of evidence-based practice. Oxford University Press.\nBrennan, R. L. (2001). Generalizability theory. Springer.\nGwet, K. L. (2021). Handbook of inter-rater reliability: Chance-corrected agreement coefficients (5th ed., Vol. 1).\nGwet, K. L. (2021). Handbook of inter-rater reliability: Analysis of quantitative ratings (5th ed., Vol. 2).\nKline, R. (2015). Principles and practice of structural equation modeling (4th ed.). Guilford Press.\nRevelle, W. (2014). An introduction to psychometric theory with applications in R. https://www.personality-project.org/r/book/\nZumbo, B. D., & Hubley, A. M. (Eds.). (2017). Understanding and investigating response processes in validation research. Springer."
  },
  {
    "objectID": "posts/postdoc2024/index.html",
    "href": "posts/postdoc2024/index.html",
    "title": "Postdoctoral Position 2024",
    "section": "",
    "text": "Position Overview\nWe are seeking a highly motivated and innovative postdoctoral researcher to join the interdisciplinary Affective Communication and Computing research laboratory (https://affcom.ku.edu), which is housed within the University of Kansas Department of Psychology. The successful candidate will work closely with the lab’s director, Dr. Jeffrey Girard, to contribute to cutting-edge research projects that apply advanced quantitative and computational techniques to the study of affective and interpersonal communication, mental and social health, and clinical assessment. This position provides an excellent opportunity for professional development and collaboration within a vibrant research community.\nThe postdoctoral researcher will be involved in all parts of the research process including project planning, project management, data collection, data analysis, and dissemination of findings through publications and conference presentations. Additionally, the successful candidate will collaborate with interdisciplinary teams, mentor graduate and undergraduate students, and engage in ongoing professional development opportunities.\nThis position is limited to 1 year, with additional years being contingent on funding.\nThe successful candidate must have appropriate authorization to work in the U.S. before employment begins.\n\n\nWhere can I apply?\nhttps://employment.ku.edu/staff/26753BR\n\n\nJob Duties\n70% Independent and Collaborative Research\n\nDevelop and implement research projects related to personal and lab interests.\nConduct literature reviews to inform study design and hypothesis generation.\nCollect and analyze research data using a variety of methodologies.\nContribute to the preparation and submission of manuscripts for publication.\n\n30% Participation in Lab Activities\n\nAid in the preparation of IRB proposals and grant proposals (both internal and external).\nMentor graduate and undergraduate students involved in lab research activities.\nCommunicate regularly with lab director and members in formal and informal meetings.\nStay up to date on relevant theoretical and methodological advancements.\n\n\n\nRequired Qualifications\n\nPh.D. in Psychology, Computer Science, or a related field.\nDemonstrated ability to conduct independent research.\nExcellent written and oral communication skills as evidenced by application materials.\nRecord of scholarly publications in peer-reviewed journals/proceedings.\nAbility to work collaboratively in a team-oriented research environment.\n\nTo be appointed at the Postdoctoral Researcher title, it is necessary to have the PhD conferred at the time of hire. Appointments made without a diploma or certified transcript indicating an earned doctorate are conditional hires and are appointed on an interim basis not to exceed 6 months. Upon verification of degree the appointment will be extended to its full duration.\n\n\nPreferred Qualifications\n\nExpertise in advanced quantitative and/or computational techniques.\nExperience with grant writing and securing external research funding.\nTrack record of conference presentations at national or international meetings.\nStrong interpersonal skills and the ability to mentor and guide junior researchers.\nDemonstrated interest in affective communication and/or affective computing.\nFamiliarity with open science and/or team science techniques and initiatives.\nProficiency in analysis and/or software development using the R environment."
  },
  {
    "objectID": "posts/recruitment2024/index.html",
    "href": "posts/recruitment2024/index.html",
    "title": "Recruitment 2024",
    "section": "",
    "text": "Are you planning to accept new graduate students this cycle?\nProbably not. Because the lab admitted three students last cycle, we will likely not admit any new students this cycle. However, there is a small possibility that we would admit an outstanding applicant who is an exceptionally good fit with the lab, either to the Clinical Psychological Science (CPS) or Brain, Behavior, and Quantitative Science (BBQ) program. The application deadline for both programs is December 1, 2023."
  },
  {
    "objectID": "posts/sai2024/index.html",
    "href": "posts/sai2024/index.html",
    "title": "SAI 2024 Workshop",
    "section": "",
    "text": "I was invited to speak at the 2nd Social Artificial Intelligence Workshop. It was a lot of fun and I am very grateful to the Social AI Group at University of Glasgow! Please find my slides below.\n\nGirard, J. M. (2024). Contextualizing social artificial intelligence for gender, culture, and more. Invited talk presented at the 2nd Social Artificial Intelligence Workshop, University of Glasgow, Glasgow, UK.\n\n\nSlides"
  },
  {
    "objectID": "posts/suicideR21/index.html",
    "href": "posts/suicideR21/index.html",
    "title": "NIMH R21 Grant",
    "section": "",
    "text": "I am very pleased to announce that we have been awarded an R21 (Exploratory/Developmental Research) Grant from the National Institute of Mental Health (R21-MH130767) to study parent-child communication dynamics and suicide risk. The project period is from 09/2022 through 07/2024 with a total funding of $278,341."
  },
  {
    "objectID": "posts/suicideR21/index.html#project-team",
    "href": "posts/suicideR21/index.html#project-team",
    "title": "NIMH R21 Grant",
    "section": "Project Team",
    "text": "Project Team\n\nMPI: Richard Liu, Massachusetts General Hospital\nMPI: Taylor Burke, Massachusetts General Hospital\nCo-I: Jeffrey Girard, University of Kansas\nCo-I: Louis-Philippe Morency, Carnegie Mellon University"
  },
  {
    "objectID": "posts/suicideR21/index.html#project-summary",
    "href": "posts/suicideR21/index.html#project-summary",
    "title": "NIMH R21 Grant",
    "section": "Project Summary",
    "text": "Project Summary\nSuicide continues to be a growing public health concern. Indeed, suicide rates have increased 33% since 1999. This concern has been particularly pronounced in the case of adolescents, as the suicide rates in this age group have tripled over the last 10 years. Clarifying potential processes of risk for suicidal behavior in this population therefore remains a pressing priority. Increasing theoretical and empirical focus has been devoted to the conceptualization of acute suicidal risk as being a period of elevated arousal. Stress within parent-child relationship dynamics may hold particular importance for understanding adolescents’ proximal risk for suicidal behavior. However, all past studies examining parent-child relationship stress and suicidal behavior have used self-report methodologies and/or retrospective recall. Although studies employing these methodologies provide important knowledge about the association between life stress and mental health outcomes, they are not designed to characterize the interpersonal dynamics of these stressors as they unfold over time, which may be particularly relevant to short-term suicide risk. Recent advances in computational approaches to automatic sensing of acoustic and visual behaviors hold promise to address the need for clarifying indices of arousal in parent-child dynamics associated with adolescent suicide risk. A novel method to assess familial stress processes is through the automated assessment of synchrony, or the behavioral matching, of arousal between adolescents and their parents. In healthy parent-child dyads, parents and adolescents are responsive to each other’s behavioral and emotional cues. However, during high stress or conflict, behavioral matching may be a marker of a high-risk interaction pattern inasmuch as it may be indicative of high arousal maintenance or escalation. Focusing on the RDoC constructs of arousal and social processes, the current R21 proposal aims to leverage recent developments in automated sensing of acoustic and visual behavior to characterize synchrony within the dynamics of a parent-child conflict task in a sample of psychiatrically hospitalized adolescents (n = 100) and their parents. The current study aims to evaluate whether acoustic and visual behavioral markers of arousal synchrony, respectively, are associated with prospective suicidal ideation three months post-discharge. We also aim to pool acoustic and visual behavioral markers of parent-child arousal synchrony through computational modeling to create a multimodal prediction model for prospective suicidal ideation, thereby to advance beyond unimodal analysis to multimodal analysis in classifying suicide risk. This R21 is intended as an initial step toward automating the assessment of parent-child arousal synchrony within clinical contexts to inform clinical decision-making and interventions. This proposal has both scientific and clinical significance because it is the first study to employ computational automation of acoustic and visual markers of suicide risk at the dyadic level and has the potential directly to inform adolescent suicide risk assessment and intervention."
  },
  {
    "objectID": "posts/suicideR21/index.html#public-health-relevance",
    "href": "posts/suicideR21/index.html#public-health-relevance",
    "title": "NIMH R21 Grant",
    "section": "Public Health Relevance",
    "text": "Public Health Relevance\nThis application aims to examine the dynamics of parent-child stress using automatic sensing of acoustic and visual behaviors as markers of suicide risk in adolescents. This proposal has both scientific and clinical significance because it is the first study to employ computational automation of acoustic and visual markers of suicide risk at the dyadic level. This research has the potential to improve our identification of stress-related risk in suicidal adolescents in periods of high risk and to directly inform clinical decision-making and intervention in the context of adolescents’ psychiatric hospitalization."
  },
  {
    "objectID": "posts/vail2022/index.html",
    "href": "posts/vail2022/index.html",
    "title": "ICMI 2022 Paper",
    "section": "",
    "text": "I am very pleased to announce that a new conference paper from our team has been published in the Proceedings of the 2022 International Conference on Multimodal Interaction.\nThis paper, entitled “Toward causal understanding of therapist-client relationships: A study of language modality and social entrainment,” explores the associations between working alliance ratings and linguistic entrainment between therapist and patients with depression during brief psychotherapy.\nThis paper was authored by Vail, Girard, Bylsma, Cohn, Fournier, Swartz, and Morency and was supported in part by our “Dyadic Behavior Informatics for Psychotherapy Process and Outcome” grant from the National Science Foundation.\nClick here to access the paper"
  },
  {
    "objectID": "posts/whisper2024b/index.html",
    "href": "posts/whisper2024b/index.html",
    "title": "AI Transcription from R using Whisper: Part 2",
    "section": "",
    "text": "Warning\n\n\n\nThis post has been replaced by a new one, which makes things considerably easier. I am keeping this one up mostly for posterity."
  },
  {
    "objectID": "posts/whisper2024b/index.html#introduction",
    "href": "posts/whisper2024b/index.html#introduction",
    "title": "AI Transcription from R using Whisper: Part 2",
    "section": "Introduction",
    "text": "Introduction\nIn a previous blog post, I discussed using the audio.whisper R package to do local, AI-based audio transcription. It worked well but was prohibitively slow (e.g., ~1 minute to process each second of audio). In this blog post, I will discuss how to achieve considerable speed improvements on Windows through a combination of hardware and software. Parts will be more technical but hang in there and I’ll do my best to make it achievable.\nBefore we dive into things, I’ll provide a brief overview of all the steps.\n\nCheck that our computer’s hardware supports CUDA\nInstall/update the NVIDIA graphics driver on Windows\nInstall and update the Windows Subsystem for Linux (WSL2) on Windows\nInstall and setup the Ubuntu operating system via WSL2\nInstall the CUDA Toolkit for WSL on Ubuntu\nInstall R and dependency packages on Ubuntu\nInstall audio.whisper R package with CUDA support on Ubuntu\nTest and time the model"
  },
  {
    "objectID": "posts/whisper2024b/index.html#check-for-cuda-support",
    "href": "posts/whisper2024b/index.html#check-for-cuda-support",
    "title": "AI Transcription from R using Whisper: Part 2",
    "section": "Check for CUDA Support",
    "text": "Check for CUDA Support\nThis post assumes that you are using the Windows operating system and that your computer’s graphics card supports CUDA. To check that this is the case, first look up your graphics card’s model number. An easy way to do this on Windows 10/11 is to click on the desktop search bar (bottom-left of the screen next to the windows icon) and type in “Device Manager.” Then click the arrow next to “Display adapters” and find your graphics card’s model name. On my computer, it says “NVIDIA GeForce RTX 2060.” Then go to this link and click the “CUDA-Enabled NVIDIA Quadro and NVIDIA RTX” and “CUDA-Enabled GeForce and TITAN Products” blocks to open their accordions. Then search for your graphics card’s model number (the left tables are for desktop cards and the right tables are for notebook cards). I found “GeForce RTX 2060” on the list under GeForce and TITAN Products with a compute capability of 7.5. Thus, my card is supported!"
  },
  {
    "objectID": "posts/whisper2024b/index.html#install-the-newest-nvidia-graphics-driver",
    "href": "posts/whisper2024b/index.html#install-the-newest-nvidia-graphics-driver",
    "title": "AI Transcription from R using Whisper: Part 2",
    "section": "Install the Newest NVIDIA Graphics Driver",
    "text": "Install the Newest NVIDIA Graphics Driver\nDownload and install the newest graphics driver for your card from NVIDIA. You should choose the Game Ready version. Note that you should not install the CUDA toolkit on Windows as doing so may confuse things and lead to issues later on (as we will be installing the CUDA toolkit for WSL in a later step)."
  },
  {
    "objectID": "posts/whisper2024b/index.html#install-and-update-the-windows-subsystem-for-linux",
    "href": "posts/whisper2024b/index.html#install-and-update-the-windows-subsystem-for-linux",
    "title": "AI Transcription from R using Whisper: Part 2",
    "section": "Install and Update the Windows Subsystem for Linux",
    "text": "Install and Update the Windows Subsystem for Linux\nOpen the Microsoft Store app (e.g., using the desktop search bar) and search for the “Windows Subsystem for Linux.” If it doesn’t come up in the search results, you may already have it installed - you can check this by clicking the “Library” button on the left sidebar in the app and searching for it there. If it does come up, click on the Install button. If you can’t find it, then open the Command Prompt app (e.g., using the desktop search bar) and type or paste the following command: wsl --install. After it install using any method, it will ask you to restart your computer. Once restarted, open the Command Prompt app again and type or paste the following command: wsl --update. This will ensure that you have the most recent version of WSL2 installed."
  },
  {
    "objectID": "posts/whisper2024b/index.html#install-and-setup-ubuntu",
    "href": "posts/whisper2024b/index.html#install-and-setup-ubuntu",
    "title": "AI Transcription from R using Whisper: Part 2",
    "section": "Install and Setup Ubuntu",
    "text": "Install and Setup Ubuntu\nIn the Command Prompt app, type or paste the following command: wsl --install Ubuntu. This will install the Ubuntu Linux operating system over the course of several minutes. After installation, it will prompt you to create a UNIX username and password. Use whatever you want but don’t lose this information as you will need it again later."
  },
  {
    "objectID": "posts/whisper2024b/index.html#install-the-cuda-toolkit-for-wsl",
    "href": "posts/whisper2024b/index.html#install-the-cuda-toolkit-for-wsl",
    "title": "AI Transcription from R using Whisper: Part 2",
    "section": "Install the CUDA Toolkit for WSL",
    "text": "Install the CUDA Toolkit for WSL\nIn the Ubuntu console (which is opened automatically after Ubuntu is installed), enter or paste the following commands to install the CUDA Toolkit for WSL-Ubuntu. It will ask you to enter your password (created in the previous step) and may take several minutes to complete.\nwget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\nsudo apt-get update\nsudo apt-get -y install cuda-toolkit\n\n\n\n\n\n\nImportant\n\n\n\nDo not install any NVIDIA graphics drivers on Ubuntu directly (i.e., install cuda-toolkit and not cuda or cuda-drivers). Ubuntu will inherit the Windows drivers you installed in a previous step via WSL.\nIf you get timeout errors when trying to install things on WSL, check to make sure that you are not connected to a VPN on Windows as this can mess things up."
  },
  {
    "objectID": "posts/whisper2024b/index.html#install-r-on-ubuntu",
    "href": "posts/whisper2024b/index.html#install-r-on-ubuntu",
    "title": "AI Transcription from R using Whisper: Part 2",
    "section": "Install R on Ubuntu",
    "text": "Install R on Ubuntu\nIn the Ubuntu console, enter or paste the following commands to install R and other packages commonly used by R. You may have to hit ENTER and type Y several times when prompted to do so.\nwget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc\nsudo add-apt-repository \"deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/\"\nsudo apt install -y --no-install-recommends r-base\nsudo apt-get install -y libcurl4-openssl-dev libssl-dev libxml2-dev libudunits2-dev libgdal-dev cargo libfontconfig1-dev libcairo2-dev\nsudo add-apt-repository ppa:c2d4u.team/c2d4u4.0+\nsudo apt upgrade\nsudo apt install -y --no-install-recommends r-cran-devtools r-cran-av r-cran-tidyverse"
  },
  {
    "objectID": "posts/whisper2024b/index.html#install-audio.whisper-with-cuda-support-on-ubuntu",
    "href": "posts/whisper2024b/index.html#install-audio.whisper-with-cuda-support-on-ubuntu",
    "title": "AI Transcription from R using Whisper: Part 2",
    "section": "Install audio.whisper with CUDA Support on Ubuntu",
    "text": "Install audio.whisper with CUDA Support on Ubuntu\nIn the Ubuntu console, type or paste the following command: sudo R to open the R console. You will then need to set several environmental variables before installing the audio.whisper package from GitHub. Do so by entering or pasting the following commands into the R console:\nSys.setenv(PATH = sprintf(\"%s:/usr/local/cuda/bin\", Sys.getenv(\"PATH\")))\nSys.setenv(CUDA_PATH = \"/usr/local/cuda\")\nSys.setenv(WHISPER_CUBLAS = \"1\")\nremotes::install_github(\"bnosac/audio.whisper\")"
  },
  {
    "objectID": "posts/whisper2024b/index.html#test-and-time-the-model",
    "href": "posts/whisper2024b/index.html#test-and-time-the-model",
    "title": "AI Transcription from R using Whisper: Part 2",
    "section": "Test and Time the Model",
    "text": "Test and Time the Model\n\nBase Model and Short Audio Clip\nIn the R console, load the audio.whisper package and try it out on the JFK clip that took so long to process in the previous blog post. Note that there will be one important change to the commands from before. This time, when we load the model using the whisper() function, we will add the use_gpu = TRUE argument.\n# Load the package from library\nlibrary(audio.whisper)\n\n# Download or load from file the desired model (with GPU support)\nmodel &lt;- whisper(\"base\", use_gpu = TRUE)\n\n# Construct file path to example audio file in package data\njfk &lt;- system.file(package = \"audio.whisper\", \"samples\", \"jfk.wav\")\n\n# Run English transcription using the downloaded whisper model\nout &lt;- predict(model, newdata = jfk, language = \"en\")\n\n# Print transcript\nout$data\n\n\n\n\n\nsegment\nsegment_offset\nfrom\nto\ntext\n\n\n\n\n1\n0\n00:00:00.000\n00:00:07.600\nAnd so my fellow Americans, ask not what your country can do for you,\n\n\n2\n0\n00:00:07.600\n00:00:10.600\nask what you can do for your country.\n\n\n\n\n\nThe results look good/the same as before, but check out the timing!!!\n\nout$timing\n## $transcription_start\n## [1] \"2024-11-10 12:34:19 CST\"\n## \n## $transcription_end\n## [1] \"2024-11-10 12:34:20 CST\"\n## \n## $transcription_duration\n## Time difference of 0.006911568 mins\n\n\n\nLarge Model and Long Audio Clip\nWith such a boost in speed, we can afford to try a larger model (e.g., \"large-v3\") on a longer audio clip (e.g., a 1.35 min poetry reading by Gerard Malanga that is rather noisy and therefore a good test of the model’s accuracy in real-world settings). This is also a chance to show how to process a file downloaded from the internet, in case that is of interest to any readers. We’ll use the following commands in the R console in Ubuntu:\n# Load package from library (it was installed earlier via apt)\nlibrary(av)\n\n# Download audio file from ubu.com\ndownload.file(\n  url = \"https://ubu.com/media/sound/malanga_gerard/archives/Malanga-Gerard_Archives_01-Gerard-Malanga_To-The-Young-Model-Name-Unknown.mp3\", \n  destfile = \"malanga.mp3\", \n  mode = \"wb\"\n)\n\n# Convert audio from mp3 to 16 kHz wav\nav_audio_convert(\n  \"malanga.mp3\", \n  output = \"malanga.wav\", \n  format = \"wav\", \n  sample_rate = 16000\n)\n\n# Download or load from file the large model with GPU support\nmodel &lt;- whisper(\"large-v3\", use_gpu = TRUE)\n\n# Run English transcription using the downloaded whisper model\nout &lt;- predict(model, newdata = \"output.wav\", language = \"en\")\n\n# Print the transcript\nout$data\n\n\n\n\n\nsegment\nsegment_offset\nfrom\nto\ntext\n\n\n\n\n1\n0\n00:00:00.000\n00:00:06.000\nGary Malanga will read me some poems which are shortly to be illustrated by Andy Warhol.\n\n\n2\n0\n00:00:06.000\n00:00:10.000\nThe poems, as far as I can tell, do not relate particularly to this exhibit,\n\n\n3\n0\n00:00:10.000\n00:00:13.000\nbut Mr. Malanga thought it would be an appropriate setting for his poetry.\n\n\n4\n0\n00:00:13.000\n00:00:17.000\nThe length of the reading will be about 45 minutes.\n\n\n5\n0\n00:00:17.000\n00:00:19.000\nThank you so much for your patience, Mr. Malanga.\n\n\n6\n0\n00:00:19.000\n00:00:26.000\nCan everyone hear me?\n\n\n7\n0\n00:00:26.000\n00:00:33.000\nThis poem is actually the first poem I ever wrote in this series of fashion poems,\n\n\n8\n0\n00:00:33.000\n00:00:38.000\nentitled \"To a Young Model Name Unknown,\" photographed by Francesco Scuvullo.\n\n\n9\n0\n00:00:38.000\n00:00:47.000\nThe Peckin Peck girl applauds the strategy of Hadley Kashmir,\n\n\n10\n0\n00:00:47.000\n00:00:52.000\nnow gentle country air left, to go calling in the afternoon,\n\n\n11\n0\n00:00:52.000\n00:00:56.000\npale gray, flannel-dressed, gracefully princessed,\n\n\n12\n0\n00:00:56.000\n00:01:03.000\nits gray collar deeply cut, filled with a fluff of gray rabbit fur.\n\n\n13\n0\n00:01:03.000\n00:01:07.000\nThe new country look of the jumpsuit opposite.\n\n\n14\n0\n00:01:07.000\n00:01:14.000\nHere, fresh, bright and Irish in white, stitched sheer navy blue wool,\n\n\n15\n0\n00:01:14.000\n00:01:20.000\nIrish country airs, the changing outline of Irish fashion.\n\n\n16\n0\n00:01:20.000\n00:01:21.780\nThank you.\n\n\n\n\n\nThe results look really good despite the background noise. The only errors I noticed were line 11, I think he says “princess shaped” rather than “princessed” (though I could be wrong) and in line 16, I can’t hear him say “Thank you.” so that may have been hallucinated (or perhaps in the background). Not bad at all. And check out the timing.\n\nout$timing\n## $transcription_start\n## [1] \"2024-08-16 13:33:07 CDT\"\n## \n## $transcription_end\n## [1] \"2024-08-16 13:33:19 CDT\"\n## \n## $transcription_duration\n## Time difference of 0.1988164 mins"
  },
  {
    "objectID": "posts/whisper2024b/index.html#wrap-up",
    "href": "posts/whisper2024b/index.html#wrap-up",
    "title": "AI Transcription from R using Whisper: Part 2",
    "section": "Wrap-up",
    "text": "Wrap-up\nIf you want to save the transcript, you can enter the following command in the R console: saveRDS(out, \"malanga.rds\") and it will create a serialized R data file containing all the transcript data (e.g., text and time stamps). By default, this file will be saved in the same folder on your Windows file system that you ran the Command Prompt app from (e.g., “C:/Users/jeffg”). However, you can save anywhere using WSL’s /mnt/ system. For example, if you wanted to save it to “C:/Users/jeffg/Desktop”, then you would use \"/mnt/c/users/jeffg/Desktop/malanga.rds\" as the second argument to saveRDS(). Or if you wanted to save it to a mapped network drive like “Z:/affcomlab/transcription”, then you would use \"/mnt/z/affcomlab/transcription/malanga.rds\".\nThat wraps up this blog post. In the next part, I will discuss more practical aspects of using this technology. For example, I’ll talk about how to generate a list of audio/videos files on your hard drive (or elsewhere) and then iterate over them to create transcripts from many files all at once.\nPart 3 coming soon…"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "AffCom Lab",
    "section": "",
    "text": "Home\n    Research"
  },
  {
    "objectID": "research.html#research-topics",
    "href": "research.html#research-topics",
    "title": "AffCom Lab",
    "section": "Research Topics",
    "text": "Research Topics\n\nClinical Topics\n\nDepression – We are interested in improving the assessment and treatment of depression. In particular, we are interested in (a) developing computational depression screening tools that can be deployed in under-served (e.g., remote or low resource) communities, (b) studying the connections between mood, depression, and interpersonal functioning, and (c) understanding the psychotherapeutic processes that contribute to recovery from depression.\nPosttraumatic Stress Disorder – We are interested in how individuals who survive traumatic events respond to, make sense of, and communicate about their experiences. Toward this end, we collect and analyze trauma narratives and impact statements from patients seeking treatment for PTSD and from community members who experienced traumatic events but did not develop PTSD. We are using observational methods to measure and analyze the patterns of behaviors, emotions, and cognitions evident in these narratives and statements. Our goals are to improve PTSD assessment and to deepen clinical understanding of the psychological processes that contribute to the development, maintenance, and recovery from PTSD.\nStructure of psychopathology – We are interested in advancing scientific knowledge about the ways in which psychiatric signs and symptoms organize into transdiagnostic dimensions or “spectra.” As part of this effort, we are working on creating an R package for the “Bass-Ackwards” factor analytical technique (Goldberg, 2006; Forbes, 2020). We are also interested in disentangling measures of dysfunction and traits/symptoms.\nBehavioral synchrony – We are interested in studying the emergence of behavioral synchronization between members of interacting dyads (e.g., therapists and patients or parents and children). We are applying this to studying the development of the working alliance (i.e., therapeutic relationship) during psychotherapy for depression and the communication dynamics between parents and children at high risk of suicide. As part of this effort, we are working on creating an R package for the windowed cross-correlation technique for quantifying behavioral synchrony (Boker et al., 2002).\n\n\n\nMethodological Topics\n\nInterrater reliability statistics – We are interested in developing statistical techniques that quantify the degree of reliability (e.g., agreement or consistency) between raters in observational and judgment studies. As part of this effort, we are working on finalizing the agreement R package for chance-adjusted indexes of categorical agreement and are working on integrating insights and tools from Bayesian statistics and generalizability theory in the varde R pacakge.\nCircumplex statistics – We are interested in developing statistical techniques that leverage the circular structure of circumplex models of affect and interpersonal functioning. As part of this effort, we are working on expanding the circumplex R package with additional functionality and developing new extensions of the Structural Summary Method (Gurtman, 1992; Zimmermann & Wright, 2017) that accommodate multilevel (e.g., clustered and longitudinal) data and latent variables.\nObservational methods – We are interested in measuring behavior as it actually occurs using human and algorithmic observers. Toward this end, we are working on creating databases of observational records (e.g., videos) to share with other researchers and are developing software and educational resources to aid in the collection and analysis of observational data.\nApplied machine learning – We are interested in applying recent advances in predictive modeling techniques to the study of human behavior (e.g., to aid in clinical decision-making and observational measurement). We are particularly interested in model evaluation, explanation, and comparison techniques that combine insights and techniques from statistical inference and machine learning.\nBayesian multilevel modeling – We are interested in leveraging the wonderful flexibility of Bayesian mixed effects models to answer applied questions about multilevel (e.g., clustered or longitudinal) data. We are also interested in adapting this framework to improve estimation in other topics (e.g., inter-rater reliability statistics, circumplex statistics, and comparing machine learning models)."
  },
  {
    "objectID": "research.html#research-funding",
    "href": "research.html#research-funding",
    "title": "AffCom Lab",
    "section": "Research Funding",
    "text": "Research Funding\n\nCurrent Funding\n\n“Assessment of Eating Disorder and Comorbidity Risk and Resilience in Recent Military Enlistees”\n\nDepartment of Defense (DoD)\nForbush (PI), Co-I: Girard\n04/2025 – 03/2027\n\n“Building Healthy Eating and Self-Esteem Together for University Students (BEST-U): A Pilot Randomized Controlled Trial of an mHealth Intervention for Binge-Spectrum Disorders”\n\nR34, National Institutes of Health (NIMH)\nForbush, Christensen-Pacella (PIs), Co-I: Girard\n01/2025 – 12/2027\n\n“COBRE Computational Assessment of Communicative Behaviors in Posttraumatic Stress”\n\nP20, National Institutes of Health (COBRE)\nGirard (PI)\n01/2025 – 12/2026\n\n\n\n\nCompleted Funding\n\n“Context-Adaptive Multimodal Informatics for Psychiatric Discharge Planning”\n\nR01, National Institutes of Health (NIMH)\nBaker (PI), Co-Is: Girard, Morency\n04/2021 – 02/2025\n\n“Kansas Data Science Training Pathways | An Integrated Model”\n\nNational Science Foundation (EPSCoR REI)\nGirard (PI)\n01/2024 – 12/2024\n\n“Novel Scalable Mental Health Screening Procedures on Ubiquitous Sensing Devices”\n\nGoogler-Initiated Grant, Google LLC\nGirard (PI)\n11/2022\n\n“Multimodal Dynamics of Parent-Child Interactions and Suicide Risk”\n\nR21, National Institutes of Health (NIMH)\nBurke (PI), Co-Is: Girard, Li, Morency\n09/2022 – 07/2023\n\n“Towards Automated Multimodal Behavioral Screening for Depression”\n\nPittsburgh Health Data Alliance (CMLH)\nMorency (PI), Co-I: Girard\n03/2019 – 02/2021\n\n“Dyadic Behavior Informatics for Psychotherapy Process and Outcome”\n\nNational Science Foundation (IIS, SCH)\nCohn, Morency, Swartz (PIs), Co-Is: Bylsma, Fournier, Girard\n09/2020 – 08/2022"
  },
  {
    "objectID": "research.html#research-framework",
    "href": "research.html#research-framework",
    "title": "AffCom Lab",
    "section": "Research Framework",
    "text": "Research Framework\n\n\nSubstantive Pillars\nThe substantive focus of our work is on how humans communicate important affective and interpersonal information to one another. The four pillars of this work are on structure, context, dynamics, and functionality. In studying structure, we investigate the production and perception of visual behaviors (e.g., facial expressions, gestures, and body motion), vocal behaviors (e.g., pitch, loudness, and timbre), and verbal behaviors (e.g., syntax, semantics, and discourse). In studying context, we investigate the influence of cultural, relational, and situational factors. In studying dynamics, we investigate change over time, regulation, and how processes and individuals influence one another. Finally, in studying functionality, we investigate how communication interfaces with health, clinical assessment, and treatment.\n\n\nMethodological Foundation\nSupporting these pillars is a wide foundation of interdisciplinary methodology. The three main aspects of this foundation are statistics, computing, and open science. The statistical aspects largely focus on Bayesian estimation, multilevel (mixed-effects) modeling, and measurement validation. The computational aspects largely focus on data science (data wrangling and visualization), behavior sensing (computer vision, signal processing, and natural language processing), and artificial intelligence (machine learning and large language models). Finally, the open science aspects largely focus on research software, research databases, and educational resources."
  },
  {
    "objectID": "posts/moderation2022/presentation.html",
    "href": "posts/moderation2022/presentation.html",
    "title": "Adventuresin Moderation",
    "section": "",
    "text": "Jeffrey M. Girard University of Kansas"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#overview",
    "href": "posts/moderation2022/presentation.html#overview",
    "title": "Adventuresin Moderation",
    "section": "Overview",
    "text": "Overview\n\nWhat is moderation in ANOVA and regression models?\nWhat questions can (and can’t) moderation answer?\nWhat tools can we use to test and probe interactions?\nANOVA Examples: 2x2, 3x2\nRegression Examples: (2x2), 2xC, CxC\nWhat are some challenges and opportunities?"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#defining-moderation",
    "href": "posts/moderation2022/presentation.html#defining-moderation",
    "title": "Adventuresin Moderation",
    "section": "Defining Moderation",
    "text": "Defining Moderation\n\nModeration adds contextualization/complexity to a model\nIt allows an IV’s effect to depend on the values of other IVs\nIt is usually tested using (bilinear) product terms\n\n\n\nThere won’t be just one effect of the IV for everyone…\n…it depends on who each person is, in terms of other IVs\n(It is like a gateway drug for mixed effects modeling)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#example-research-questions",
    "href": "posts/moderation2022/presentation.html#example-research-questions",
    "title": "Adventuresin Moderation",
    "section": "Example Research Questions",
    "text": "Example Research Questions\nCategorical-by-Categorical - Does the effect of exercise program on weight loss depend on biological sex?\n\nCategorical-by-Continuous - Does the effect of hours of exercise on weight loss depend on exercise program?\n\n\nContinuous-by-Continuous - Does the effect of hours of exercise on weight loss depend on the effort put in?\n\n\nModeration is not mediation and cannot establish causation\nModeration hypotheses should be specific and falsifiable"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#free-and-open-source-tools",
    "href": "posts/moderation2022/presentation.html#free-and-open-source-tools",
    "title": "Adventuresin Moderation",
    "section": "Free and Open-Source Tools",
    "text": "Free and Open-Source Tools\nR is a cross-platform statistical computing environment\n\nhttps://cran.r-project.org/\n\n\nRStudio adds a helpful environment for working with R\n\nhttps://posit.co/download/rstudio-desktop/\n\n\n\nThe {easystats} package adds statistics functions\n\nhttps://easystats.github.io/easystats/"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#hypotheses-and-data",
    "href": "posts/moderation2022/presentation.html#hypotheses-and-data",
    "title": "Adventuresin Moderation",
    "section": "Hypotheses and Data",
    "text": "Hypotheses and Data\n\nTraining will increase scores (relative to a control condition)\nTraining will increase scores for women more than for men\n\n\n\nset.seed(2022)\nn &lt;- 200\ndata_2x2 &lt;- \n  tibble(\n    x1 = sample(0:1, size = n, replace = TRUE),\n    x2 = sample(0:1, size = n, replace = TRUE),\n    score = 5 + 0.5 * x1 - 0.6 * x2 + 0.6 * x1 * x2 + rnorm(n = n, sd = 1),\n    condition = factor(x1, levels = 0:1, labels = c(\"control\", \"training\")),\n    gender = factor(x2, levels = 0:1, labels = c(\"woman\", \"man\"))\n  ) |&gt; \n  select(score, condition, gender) |&gt; \n  mutate(subject = row_number(), .before = 1)\ndata_2x2 |&gt; \n  head(100) |&gt; \n  kable() |&gt; \n  kable_styling() |&gt; \n  scroll_box(height = \"360px\")\n\n\n\n\n\nsubject\nscore\ncondition\ngender\n\n\n\n\n1\n5.876475\ntraining\nwoman\n\n\n2\n4.805460\ncontrol\nwoman\n\n\n3\n3.384664\ntraining\nwoman\n\n\n4\n4.086933\ncontrol\nwoman\n\n\n5\n4.749709\ncontrol\nman\n\n\n6\n5.905577\ntraining\nwoman\n\n\n7\n6.005326\ntraining\nman\n\n\n8\n3.208133\ncontrol\nwoman\n\n\n9\n4.838511\ntraining\nwoman\n\n\n10\n5.127727\ntraining\nwoman\n\n\n11\n5.158661\ncontrol\nwoman\n\n\n12\n4.321588\ncontrol\nwoman\n\n\n13\n5.582489\ntraining\nman\n\n\n14\n6.743740\ntraining\nwoman\n\n\n15\n5.280495\ncontrol\nman\n\n\n16\n5.543959\ncontrol\nwoman\n\n\n17\n4.709095\ntraining\nman\n\n\n18\n4.827145\ncontrol\nman\n\n\n19\n6.451114\ntraining\nwoman\n\n\n20\n6.320117\ntraining\nman\n\n\n21\n2.589054\ncontrol\nman\n\n\n22\n4.720086\ncontrol\nwoman\n\n\n23\n5.071068\ncontrol\nman\n\n\n24\n7.096972\ntraining\nman\n\n\n25\n4.274268\ncontrol\nwoman\n\n\n26\n6.460575\ntraining\nman\n\n\n27\n6.456440\ntraining\nman\n\n\n28\n6.265848\ncontrol\nwoman\n\n\n29\n4.763044\ncontrol\nwoman\n\n\n30\n3.979886\ntraining\nwoman\n\n\n31\n5.339553\ncontrol\nwoman\n\n\n32\n6.895974\ntraining\nwoman\n\n\n33\n4.313938\ntraining\nman\n\n\n34\n5.095529\ntraining\nman\n\n\n35\n4.712894\ncontrol\nwoman\n\n\n36\n5.342586\ncontrol\nman\n\n\n37\n4.445261\ntraining\nwoman\n\n\n38\n4.342722\ncontrol\nwoman\n\n\n39\n3.985711\ncontrol\nman\n\n\n40\n5.953967\ntraining\nwoman\n\n\n41\n3.846046\ncontrol\nman\n\n\n42\n5.853868\ntraining\nman\n\n\n43\n5.648286\ntraining\nman\n\n\n44\n5.876512\ntraining\nwoman\n\n\n45\n6.506987\ncontrol\nwoman\n\n\n46\n4.572705\ntraining\nwoman\n\n\n47\n4.412859\ncontrol\nwoman\n\n\n48\n7.958926\ntraining\nwoman\n\n\n49\n6.173742\ntraining\nwoman\n\n\n50\n5.834424\ntraining\nman\n\n\n51\n6.609042\ncontrol\nwoman\n\n\n52\n4.384601\ntraining\nwoman\n\n\n53\n5.010130\ncontrol\nwoman\n\n\n54\n3.947642\ncontrol\nwoman\n\n\n55\n5.933923\ntraining\nwoman\n\n\n56\n5.058090\ncontrol\nman\n\n\n57\n2.959927\ncontrol\nman\n\n\n58\n4.873361\ncontrol\nman\n\n\n59\n3.594148\ncontrol\nman\n\n\n60\n4.714534\ntraining\nman\n\n\n61\n3.264649\ncontrol\nman\n\n\n62\n4.588575\ntraining\nwoman\n\n\n63\n5.089966\ncontrol\nwoman\n\n\n64\n4.163905\ncontrol\nman\n\n\n65\n7.127579\ntraining\nwoman\n\n\n66\n6.005201\ntraining\nman\n\n\n67\n4.437437\ncontrol\nman\n\n\n68\n5.365165\ncontrol\nwoman\n\n\n69\n4.105399\ntraining\nman\n\n\n70\n5.635135\ncontrol\nwoman\n\n\n71\n5.022247\ntraining\nwoman\n\n\n72\n4.180026\ncontrol\nwoman\n\n\n73\n6.752384\ntraining\nman\n\n\n74\n4.181650\ntraining\nwoman\n\n\n75\n6.044261\ntraining\nman\n\n\n76\n4.213103\ncontrol\nman\n\n\n77\n6.115964\ntraining\nwoman\n\n\n78\n5.527988\ncontrol\nwoman\n\n\n79\n5.734455\ntraining\nman\n\n\n80\n5.387702\ntraining\nman\n\n\n81\n4.183703\ntraining\nwoman\n\n\n82\n5.163233\ntraining\nwoman\n\n\n83\n5.478410\ncontrol\nwoman\n\n\n84\n3.306088\ncontrol\nman\n\n\n85\n6.261406\ncontrol\nwoman\n\n\n86\n5.536015\ntraining\nwoman\n\n\n87\n6.881067\ncontrol\nwoman\n\n\n88\n4.633545\ntraining\nwoman\n\n\n89\n4.505152\ntraining\nwoman\n\n\n90\n3.482766\ncontrol\nwoman\n\n\n91\n5.459823\ntraining\nman\n\n\n92\n3.713462\ntraining\nman\n\n\n93\n6.456246\ntraining\nman\n\n\n94\n4.719504\ncontrol\nwoman\n\n\n95\n5.088813\ntraining\nwoman\n\n\n96\n3.936570\ncontrol\nman\n\n\n97\n3.045063\ntraining\nman\n\n\n98\n5.812886\ntraining\nman\n\n\n99\n3.608662\ncontrol\nman\n\n\n100\n4.625465\ncontrol\nman"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#fitting-the-model-in-anova",
    "href": "posts/moderation2022/presentation.html#fitting-the-model-in-anova",
    "title": "Adventuresin Moderation",
    "section": "Fitting the Model in ANOVA",
    "text": "Fitting the Model in ANOVA\n\nWe can use the aov() function to fit simple ANOVAs like this\nThe formula will be DV ~ IV1 * IV2 for a two-way ANOVA\nWe will then use model_parameters() to get Type 3 results\nFinally, we will estimate_means() and estimate_contrasts()\n\n\n\nlibrary(easystats)\nmodel1 &lt;- aov(\n  formula = score ~ condition * gender,\n  data = data_2x2\n)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#anova-model-parameters",
    "href": "posts/moderation2022/presentation.html#anova-model-parameters",
    "title": "Adventuresin Moderation",
    "section": "ANOVA Model Parameters",
    "text": "ANOVA Model Parameters\n\nmp1 &lt;- model_parameters(\n  model = model1, \n  contrasts = c(\"contr.sum\", \"contr.poly\"), \n  type = 3\n)\nprint(mp1)\n\n\n\n\n\nModel Summary\n\n\nParameter\nSum_Squares\ndf\nMean_Square\nF\np\n\n\n\n\n(Intercept)\n1443.22\n1\n1443.22\n1612.45\n&lt; .001\n\n\ncondition\n4.16\n1\n4.16\n4.65\n0.032\n\n\ngender\n13.77\n1\n13.77\n15.38\n&lt; .001\n\n\ncondition:gender\n9.14\n1\n9.14\n10.22\n0.002\n\n\nResiduals\n175.43\n196\n0.90"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-marginal-means",
    "href": "posts/moderation2022/presentation.html#estimating-marginal-means",
    "title": "Adventuresin Moderation",
    "section": "Estimating Marginal Means",
    "text": "Estimating Marginal Means\n\nem1 &lt;- estimate_means(\n  model = model1, \n  at = c(\"condition\", \"gender\")\n)\nprint(em1)\n\n\n\n\n\nEstimated Marginal Means\n\n\ncondition\ngender\nMean\nSE\n95% CI\n\n\n\n\ncontrol\nwoman\n4.99\n0.12\n(4.74, 5.23)\n\n\ntraining\nwoman\n5.39\n0.14\n(5.12, 5.66)\n\n\ncontrol\nman\n4.25\n0.14\n(3.97, 4.53)\n\n\ntraining\nman\n5.51\n0.13\n(5.25, 5.77)\n\n\n\nMarginal means estimated at condition, gender"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#plotting-marginal-means",
    "href": "posts/moderation2022/presentation.html#plotting-marginal-means",
    "title": "Adventuresin Moderation",
    "section": "Plotting Marginal Means",
    "text": "Plotting Marginal Means\n\nplot(em1)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-and-testing-contrasts",
    "href": "posts/moderation2022/presentation.html#estimating-and-testing-contrasts",
    "title": "Adventuresin Moderation",
    "section": "Estimating and Testing Contrasts",
    "text": "Estimating and Testing Contrasts\n\nec1a &lt;- estimate_contrasts(\n  model = model1,\n  contrast = \"condition\",\n  at = \"gender\"\n)\nprint(ec1a)\n\n\n\n\n\nMarginal Contrasts Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nLevel1\nLevel2\ngender\nDifference\n95% CI\nSE\nt(196)\np\n\n\n\n\ncontrol\ntraining\nman\n-1.26\n(-1.64, -0.88)\n0.19\n-6.48\n&lt; .001\n\n\ncontrol\ntraining\nwoman\n-0.40\n(-0.77, -0.03)\n0.19\n-2.16\n0.032\n\n\n\nMarginal contrasts estimated at condition, p-value adjustment method: Holm (1979)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-and-testing-contrasts-1",
    "href": "posts/moderation2022/presentation.html#estimating-and-testing-contrasts-1",
    "title": "Adventuresin Moderation",
    "section": "Estimating and Testing Contrasts",
    "text": "Estimating and Testing Contrasts\n\nec1b &lt;- estimate_contrasts(\n  model = model1,\n  contrast = \"gender\",\n  at = \"condition\"\n)\nprint(ec1b)\n\n\n\n\n\nMarginal Contrasts Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nLevel1\nLevel2\ncondition\nDifference\n95% CI\nSE\nt(196)\np\n\n\n\n\nwoman\nman\ncontrol\n0.74\n( 0.37, 1.11)\n0.19\n3.92\n&lt; .001\n\n\nwoman\nman\ntraining\n-0.12\n(-0.50, 0.26)\n0.19\n-0.64\n0.526\n\n\n\nMarginal contrasts estimated at gender, p-value adjustment method: Holm (1979)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#hypotheses-and-data-1",
    "href": "posts/moderation2022/presentation.html#hypotheses-and-data-1",
    "title": "Adventuresin Moderation",
    "section": "Hypotheses and Data",
    "text": "Hypotheses and Data\n\nTreatment will decrease symptoms (vs. control and placebo)\nTreatment will be more effective for women than for men\n\n\n\nset.seed(2022)\nn &lt;- 200\ndata_3x2 &lt;- \n  tibble(\n    group = factor(\n      sample(0:2, size = n, replace = TRUE), \n      levels = 0:2, \n      labels = c(\"control\", \"placebo\", \"treatment\")\n    ),\n    gender = factor(\n      sample(0:1, size = n, replace = TRUE),\n      levels = 0:1,\n      labels = c(\"woman\", \"man\")\n    ),\n    x1 = as.integer(group == \"placebo\"),\n    x2 = as.integer(group == \"treatment\"),\n    x3 = as.integer(gender == \"woman\"),\n    symptoms = 5 - 1.5*x1 - 1.5*x2 - 0.1*x1*x3 - 1.0*x2*x3 + rnorm(n = n, sd = 0.5)\n  ) |&gt; \n  select(symptoms, group, gender) |&gt; \n  mutate(patient = row_number(), .before = 1)\ndata_3x2 |&gt; \n  head(100) |&gt; \n  kable() |&gt; \n  kable_styling() |&gt; \n  scroll_box(height = \"340px\")\n\n\n\n\n\npatient\nsymptoms\ngroup\ngender\n\n\n\n\n1\n3.683067\ntreatment\nman\n\n\n2\n3.831002\nplacebo\nman\n\n\n3\n3.751012\ntreatment\nman\n\n\n4\n3.350506\ntreatment\nman\n\n\n5\n3.999081\nplacebo\nman\n\n\n6\n4.040540\ntreatment\nman\n\n\n7\n3.789107\nplacebo\nwoman\n\n\n8\n1.631338\ntreatment\nwoman\n\n\n9\n5.865339\ncontrol\nman\n\n\n10\n3.959936\nplacebo\nwoman\n\n\n11\n4.208445\ntreatment\nman\n\n\n12\n5.238969\ncontrol\nman\n\n\n13\n3.956852\nplacebo\nwoman\n\n\n14\n4.101691\ntreatment\nman\n\n\n15\n4.584589\nplacebo\nwoman\n\n\n16\n4.654177\ncontrol\nwoman\n\n\n17\n5.096955\ncontrol\nwoman\n\n\n18\n3.757290\ntreatment\nman\n\n\n19\n5.972139\ncontrol\nwoman\n\n\n20\n3.650344\nplacebo\nman\n\n\n21\n3.960412\nplacebo\nman\n\n\n22\n2.371775\ntreatment\nwoman\n\n\n23\n4.471384\ncontrol\nwoman\n\n\n24\n4.223687\ntreatment\nman\n\n\n25\n3.720213\nplacebo\nman\n\n\n26\n2.636796\ntreatment\nwoman\n\n\n27\n2.561717\ntreatment\nwoman\n\n\n28\n3.483105\nplacebo\nwoman\n\n\n29\n4.853836\ncontrol\nman\n\n\n30\n2.742955\ntreatment\nwoman\n\n\n31\n2.998217\ntreatment\nwoman\n\n\n32\n3.874644\nplacebo\nwoman\n\n\n33\n2.410750\ntreatment\nwoman\n\n\n34\n4.280384\ncontrol\nwoman\n\n\n35\n3.993854\nplacebo\nwoman\n\n\n36\n4.176368\nplacebo\nman\n\n\n37\n5.342086\ncontrol\nman\n\n\n38\n3.502481\ntreatment\nman\n\n\n39\n2.609248\ntreatment\nwoman\n\n\n40\n4.101993\nplacebo\nwoman\n\n\n41\n3.875935\ntreatment\nman\n\n\n42\n5.168324\ncontrol\nman\n\n\n43\n3.295567\ntreatment\nman\n\n\n44\n3.545530\ntreatment\nman\n\n\n45\n5.174403\ncontrol\nman\n\n\n46\n3.158648\nplacebo\nwoman\n\n\n47\n2.672171\ntreatment\nman\n\n\n48\n5.058088\ncontrol\nman\n\n\n49\n3.227256\nplacebo\nman\n\n\n50\n5.128927\ncontrol\nwoman\n\n\n51\n3.824916\ncontrol\nwoman\n\n\n52\n3.916943\ntreatment\nman\n\n\n53\n3.208675\ntreatment\nman\n\n\n54\n3.885213\nplacebo\nman\n\n\n55\n4.717599\ncontrol\nman\n\n\n56\n2.829732\ntreatment\nman\n\n\n57\n2.984161\nplacebo\nwoman\n\n\n58\n3.834124\nplacebo\nwoman\n\n\n59\n3.786460\ntreatment\nman\n\n\n60\n1.984711\ntreatment\nwoman\n\n\n61\n3.521415\ntreatment\nman\n\n\n62\n5.283700\ncontrol\nman\n\n\n63\n3.957941\nplacebo\nwoman\n\n\n64\n2.003426\ntreatment\nwoman\n\n\n65\n3.302742\nplacebo\nman\n\n\n66\n4.519169\ncontrol\nwoman\n\n\n67\n3.315367\ntreatment\nman\n\n\n68\n3.321480\nplacebo\nman\n\n\n69\n3.092005\nplacebo\nwoman\n\n\n70\n4.486210\ncontrol\nwoman\n\n\n71\n3.889311\ncontrol\nwoman\n\n\n72\n3.239938\nplacebo\nwoman\n\n\n73\n2.494876\nplacebo\nwoman\n\n\n74\n4.865170\ncontrol\nman\n\n\n75\n3.099088\nplacebo\nman\n\n\n76\n2.703681\ntreatment\nwoman\n\n\n77\n3.294807\ntreatment\nman\n\n\n78\n3.167455\nplacebo\nwoman\n\n\n79\n4.626240\ncontrol\nman\n\n\n80\n3.111319\ntreatment\nwoman\n\n\n81\n5.244846\ncontrol\nwoman\n\n\n82\n2.789232\ntreatment\nwoman\n\n\n83\n4.391974\nplacebo\nman\n\n\n84\n2.671924\ntreatment\nwoman\n\n\n85\n4.258345\ncontrol\nman\n\n\n86\n3.574922\ntreatment\nman\n\n\n87\n4.862326\ncontrol\nman\n\n\n88\n3.094355\ntreatment\nwoman\n\n\n89\n3.424623\ntreatment\nman\n\n\n90\n3.275555\ntreatment\nwoman\n\n\n91\n2.438367\ntreatment\nwoman\n\n\n92\n3.503636\nplacebo\nman\n\n\n93\n4.049056\nplacebo\nman\n\n\n94\n4.420680\ncontrol\nwoman\n\n\n95\n3.436439\ntreatment\nman\n\n\n96\n3.354488\nplacebo\nwoman\n\n\n97\n3.983255\ncontrol\nman\n\n\n98\n5.182548\ncontrol\nman\n\n\n99\n2.613542\nplacebo\nwoman\n\n\n100\n4.055499\ncontrol\nwoman"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#fitting-the-model-in-anova-1",
    "href": "posts/moderation2022/presentation.html#fitting-the-model-in-anova-1",
    "title": "Adventuresin Moderation",
    "section": "Fitting the Model in ANOVA",
    "text": "Fitting the Model in ANOVA\n\n\nmodel2 &lt;- aov(\n  formula = symptoms ~ group * gender,\n  data = data_3x2\n)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#anova-model-parameters-1",
    "href": "posts/moderation2022/presentation.html#anova-model-parameters-1",
    "title": "Adventuresin Moderation",
    "section": "ANOVA Model Parameters",
    "text": "ANOVA Model Parameters\n\nmp2 &lt;- model_parameters(\n  model = model2, \n  contrasts = c(\"contr.sum\", \"contr.poly\"), \n  type = 3\n)\nprint(mp2)\n\n\n\n\n\nModel Summary\n\n\nParameter\nSum_Squares\ndf\nMean_Square\nF\np\n\n\n\n\n(Intercept)\n615.35\n1\n615.35\n2364.66\n&lt; .001\n\n\ngroup\n86.82\n2\n43.41\n166.81\n&lt; .001\n\n\ngender\n0.36\n1\n0.36\n1.39\n0.240\n\n\ngroup:gender\n13.09\n2\n6.54\n25.15\n&lt; .001\n\n\nResiduals\n50.48\n194\n0.26"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-marginal-means-1",
    "href": "posts/moderation2022/presentation.html#estimating-marginal-means-1",
    "title": "Adventuresin Moderation",
    "section": "Estimating Marginal Means",
    "text": "Estimating Marginal Means\n\nem2 &lt;- estimate_means(model2, at = c(\"group\", \"gender\"))\nprint(em2)\n\n\n\n\n\nEstimated Marginal Means\n\n\ngroup\ngender\nMean\nSE\n95% CI\n\n\n\n\ncontrol\nwoman\n4.77\n0.10\n(4.58, 4.97)\n\n\nplacebo\nwoman\n3.50\n0.09\n(3.33, 3.67)\n\n\ntreatment\nwoman\n2.48\n0.08\n(2.33, 2.64)\n\n\ncontrol\nman\n4.93\n0.09\n(4.75, 5.11)\n\n\nplacebo\nman\n3.63\n0.10\n(3.44, 3.82)\n\n\ntreatment\nman\n3.68\n0.09\n(3.51, 3.84)\n\n\n\nMarginal means estimated at group, gender"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#plotting-marginal-means-1",
    "href": "posts/moderation2022/presentation.html#plotting-marginal-means-1",
    "title": "Adventuresin Moderation",
    "section": "Plotting Marginal Means",
    "text": "Plotting Marginal Means\n\nplot(em2)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-and-testing-contrasts-2",
    "href": "posts/moderation2022/presentation.html#estimating-and-testing-contrasts-2",
    "title": "Adventuresin Moderation",
    "section": "Estimating and Testing Contrasts",
    "text": "Estimating and Testing Contrasts\n\nec2a &lt;- estimate_contrasts(model2, contrast = \"group\", at = \"gender\")\nprint(ec2a)\n\n\n\n\n\nMarginal Contrasts Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nLevel1\nLevel2\ngender\nDifference\n95% CI\nSE\nt(194)\np\n\n\n\n\ncontrol\nplacebo\nman\n1.30\n( 0.98, 1.62)\n0.13\n9.81\n&lt; .001\n\n\ncontrol\nplacebo\nwoman\n1.27\n( 0.96, 1.59)\n0.13\n9.73\n&lt; .001\n\n\ncontrol\ntreatment\nman\n1.25\n( 0.96, 1.55)\n0.12\n10.20\n&lt; .001\n\n\ncontrol\ntreatment\nwoman\n2.29\n( 1.99, 2.60)\n0.13\n18.23\n&lt; .001\n\n\nplacebo\ntreatment\nman\n-0.04\n(-0.36, 0.27)\n0.13\n-0.35\n0.730\n\n\nplacebo\ntreatment\nwoman\n1.02\n( 0.74, 1.30)\n0.12\n8.76\n&lt; .001\n\n\n\nMarginal contrasts estimated at group, p-value adjustment method: Holm (1979)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-and-testing-contrasts-3",
    "href": "posts/moderation2022/presentation.html#estimating-and-testing-contrasts-3",
    "title": "Adventuresin Moderation",
    "section": "Estimating and Testing Contrasts",
    "text": "Estimating and Testing Contrasts\n\nec2b &lt;- estimate_contrasts(model2, contrast = \"gender\", at = \"group\")\nprint(ec2b)\n\n\n\n\n\nMarginal Contrasts Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nLevel1\nLevel2\ngroup\nDifference\n95% CI\nSE\nt(194)\np\n\n\n\n\nwoman\nman\ncontrol\n-0.16\n(-0.42, 0.11)\n0.13\n-1.18\n0.240\n\n\nwoman\nman\nplacebo\n-0.13\n(-0.39, 0.13)\n0.13\n-0.98\n0.327\n\n\nwoman\nman\ntreatment\n-1.20\n(-1.42, -0.97)\n0.12\n-10.32\n&lt; .001\n\n\n\nMarginal contrasts estimated at gender, p-value adjustment method: Holm (1979)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#opportunities",
    "href": "posts/moderation2022/presentation.html#opportunities",
    "title": "Adventuresin Moderation",
    "section": "Opportunities",
    "text": "Opportunities\n\nRecreate the results from ANOVA\nIncorporate continuous predictors/IVs\nIncorporate any kind/number of “covariates”\nIncorporate nonlinearity (e.g., polynomials)\nExtend to GLM for non-normal outcomes/DVs\nExtend to MLM for clustered/nested data\nExtend to SEM for latent variables"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#fitting-the-model-in-regression",
    "href": "posts/moderation2022/presentation.html#fitting-the-model-in-regression",
    "title": "Adventuresin Moderation",
    "section": "Fitting the Model in Regression",
    "text": "Fitting the Model in Regression\nANOVA code (for reference)\n\nmodel1 &lt;- aov(\n  formula = score ~ condition * gender,\n  data = data_2x2\n)\n\n\nRegression code\n\nmodel1b &lt;- lm(\n  formula = score ~ condition * gender,\n  data = data_2x2\n)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#comparing-moderation-models",
    "href": "posts/moderation2022/presentation.html#comparing-moderation-models",
    "title": "Adventuresin Moderation",
    "section": "Comparing Moderation Models",
    "text": "Comparing Moderation Models\n\n\n\n\n\n\nParameter\nSum_Squares\ndf\nMean_Square\nF\np\n\n\n\n\n(Intercept)\n1443.22\n1\n1443.22\n1612.45\n&lt; .001\n\n\ncondition\n4.16\n1\n4.16\n4.65\n0.032\n\n\ngender\n13.77\n1\n13.77\n15.38\n&lt; .001\n\n\ncondition:gender\n9.14\n1\n9.14\n10.22\n0.002\n\n\nResiduals\n175.43\n196\n0.90\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(196)\np\n\n\n\n\n(Intercept)\n4.99\n0.12\n(4.74, 5.23)\n40.16\n&lt; .001\n\n\ncondition (training)\n0.40\n0.19\n(0.03, 0.77)\n2.16\n0.032\n\n\ngender (man)\n-0.74\n0.19\n(-1.11, -0.37)\n-3.92\n&lt; .001\n\n\ncondition (training) × gender (man)\n0.86\n0.27\n(0.33, 1.39)\n3.20\n0.002\n\n\n\n\n\n\n\nNote the same \\(p\\)-values and that each \\(F\\) is equal to the corresponding \\(t\\) squared."
  },
  {
    "objectID": "posts/moderation2022/presentation.html#extensions",
    "href": "posts/moderation2022/presentation.html#extensions",
    "title": "Adventuresin Moderation",
    "section": "Extensions",
    "text": "Extensions\nBy giving the lm() model to the same functions, we can…\n\nEstimate and plot (the same) marginal means\n\nem1b &lt;- estimate_means(model1b, at = c(\"condition\", \"gender\"))\nprint(em1b)\nplot(em1b)\n\n\n\nEstimate and test (the same) contrasts\n\nestimate_contrasts(model1b, contrast = \"condition\", at = \"gender\")\nestimate_contrasts(model1b, contrast = \"gender\", at = \"condition\")"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#hypotheses-and-data-2",
    "href": "posts/moderation2022/presentation.html#hypotheses-and-data-2",
    "title": "Adventuresin Moderation",
    "section": "Hypotheses and Data",
    "text": "Hypotheses and Data\n\nExercising for a longer duration will burn more calories\nLong swims will be more effective than long runs\n\n\n\nset.seed(2022)\nn &lt;- 300\ndata_2xC &lt;- \n  tibble(\n    exercise = factor(\n      sample(0:1, size = n, replace = TRUE), \n      levels = 0:1, \n      labels = c(\"run\", \"swim\")\n    ),\n    duration = rnorm(n = n, mean = 2, sd = 0.5),\n    x1 = as.integer(exercise == \"swim\"),\n    calories = 90 + 200*duration + 0*x1 + 30*duration*x1 + rnorm(n = n, sd = 50)\n  ) |&gt; \n  select(calories, duration, exercise) |&gt; \n  mutate(participant = row_number(), .before = 1)\ndata_2xC |&gt; \n  head(100) |&gt; \n  kable() |&gt; \n  kable_styling() |&gt; \n  scroll_box(height = \"340px\")\n\n\n\n\n\nparticipant\ncalories\nduration\nexercise\n\n\n\n\n1\n486.9765\n2.024798\nswim\n\n\n2\n472.8959\n1.382173\nrun\n\n\n3\n381.9000\n1.368124\nswim\n\n\n4\n384.2023\n1.428567\nrun\n\n\n5\n459.0924\n2.327206\nrun\n\n\n6\n539.3930\n1.661590\nswim\n\n\n7\n401.4003\n1.463045\nswim\n\n\n8\n397.0844\n1.512171\nrun\n\n\n9\n246.7168\n1.436863\nswim\n\n\n10\n272.5706\n1.015344\nswim\n\n\n11\n364.9235\n1.709005\nrun\n\n\n12\n298.7456\n1.118746\nrun\n\n\n13\n517.4730\n1.736087\nswim\n\n\n14\n238.2022\n1.312836\nswim\n\n\n15\n273.5526\n1.033144\nrun\n\n\n16\n696.8010\n2.489892\nrun\n\n\n17\n396.4383\n1.252155\nswim\n\n\n18\n711.3158\n2.850405\nrun\n\n\n19\n421.4667\n1.299487\nswim\n\n\n20\n520.9031\n1.757769\nswim\n\n\n21\n399.1551\n1.391911\nrun\n\n\n22\n490.5664\n2.052385\nrun\n\n\n23\n461.3814\n1.878731\nrun\n\n\n24\n418.8214\n1.292110\nswim\n\n\n25\n386.2042\n1.475270\nrun\n\n\n26\n553.7873\n2.134529\nswim\n\n\n27\n459.1941\n1.760642\nswim\n\n\n28\n359.6371\n1.850276\nrun\n\n\n29\n591.2767\n2.519074\nrun\n\n\n30\n648.0497\n2.514463\nswim\n\n\n31\n311.0183\n1.014162\nrun\n\n\n32\n590.6766\n1.358335\nswim\n\n\n33\n554.7981\n2.024291\nswim\n\n\n34\n753.9970\n2.605316\nswim\n\n\n35\n410.9739\n1.757665\nrun\n\n\n36\n405.6490\n1.603345\nrun\n\n\n37\n504.7794\n2.086207\nswim\n\n\n38\n703.8329\n2.552607\nrun\n\n\n39\n520.8601\n2.370438\nrun\n\n\n40\n496.4263\n1.879277\nswim\n\n\n41\n592.9068\n2.307479\nrun\n\n\n42\n688.9164\n2.691273\nswim\n\n\n43\n459.9332\n1.708399\nswim\n\n\n44\n604.1794\n1.901035\nswim\n\n\n45\n442.8850\n1.698385\nrun\n\n\n46\n407.9747\n1.455676\nswim\n\n\n47\n480.6806\n2.091958\nrun\n\n\n48\n745.8023\n2.653568\nswim\n\n\n49\n489.1903\n1.915858\nswim\n\n\n50\n658.7930\n2.171920\nswim\n\n\n51\n525.0054\n2.188237\nrun\n\n\n52\n553.2563\n1.902730\nswim\n\n\n53\n225.7835\n0.942332\nrun\n\n\n54\n373.1410\n1.543466\nrun\n\n\n55\n649.7859\n2.174855\nswim\n\n\n56\n465.0962\n2.202789\nrun\n\n\n57\n524.8834\n2.252663\nrun\n\n\n58\n298.0463\n1.104066\nrun\n\n\n59\n503.7600\n1.669256\nrun\n\n\n60\n535.0446\n1.813864\nswim\n\n\n61\n453.8953\n2.079331\nrun\n\n\n62\n448.7485\n1.660794\nswim\n\n\n63\n522.9352\n2.041244\nrun\n\n\n64\n611.5877\n2.621870\nrun\n\n\n65\n693.7688\n2.440248\nswim\n\n\n66\n494.5699\n2.271979\nswim\n\n\n67\n436.5890\n1.604547\nrun\n\n\n68\n506.5195\n2.213572\nrun\n\n\n69\n657.3150\n2.475557\nswim\n\n\n70\n566.0532\n2.410058\nrun\n\n\n71\n292.0450\n1.094527\nswim\n\n\n72\n474.1026\n1.860043\nrun\n\n\n73\n617.6613\n2.335534\nswim\n\n\n74\n816.7769\n2.798486\nswim\n\n\n75\n462.7039\n1.637134\nswim\n\n\n76\n551.6663\n2.480288\nrun\n\n\n77\n713.1180\n2.478220\nswim\n\n\n78\n609.7088\n2.632924\nrun\n\n\n79\n476.8087\n1.881522\nswim\n\n\n80\n370.2146\n1.239943\nswim\n\n\n81\n668.3936\n2.169777\nswim\n\n\n82\n686.7034\n2.697987\nswim\n\n\n83\n429.6422\n1.406969\nrun\n\n\n84\n442.3489\n1.797764\nrun\n\n\n85\n425.3900\n1.856447\nrun\n\n\n86\n672.3602\n2.471293\nswim\n\n\n87\n372.3098\n1.472631\nrun\n\n\n88\n456.4376\n1.671361\nswim\n\n\n89\n563.7933\n1.792856\nswim\n\n\n90\n496.6736\n2.226983\nrun\n\n\n91\n509.8858\n1.723023\nswim\n\n\n92\n601.9327\n2.176934\nswim\n\n\n93\n485.9239\n2.074143\nswim\n\n\n94\n510.3737\n2.188256\nrun\n\n\n95\n750.1839\n2.753494\nswim\n\n\n96\n367.3109\n1.536353\nrun\n\n\n97\n497.9109\n1.706429\nswim\n\n\n98\n840.7540\n3.229463\nswim\n\n\n99\n616.4693\n2.336871\nrun\n\n\n100\n466.5235\n2.167212\nrun"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#fitting-the-model-in-regression-1",
    "href": "posts/moderation2022/presentation.html#fitting-the-model-in-regression-1",
    "title": "Adventuresin Moderation",
    "section": "Fitting the Model in Regression",
    "text": "Fitting the Model in Regression\n\n\nmodel3 &lt;- lm(\n  formula = calories ~ duration * exercise,\n  data = data_2xC\n)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#regression-model-parameters",
    "href": "posts/moderation2022/presentation.html#regression-model-parameters",
    "title": "Adventuresin Moderation",
    "section": "Regression Model Parameters",
    "text": "Regression Model Parameters\n\nmp3 &lt;- model_parameters(model = model3)\nprint(mp3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(296)\np\n\n\n\n\n(Intercept)\n101.44\n16.54\n(68.89, 133.99)\n6.13\n&lt; .001\n\n\nduration\n191.42\n8.28\n(175.12, 207.72)\n23.11\n&lt; .001\n\n\nexercise (swim)\n-38.85\n24.49\n(-87.05, 9.35)\n-1.59\n0.114\n\n\nduration × exercise (swim)\n52.51\n12.15\n(28.60, 76.42)\n4.32\n&lt; .001"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#plotting-the-relations",
    "href": "posts/moderation2022/presentation.html#plotting-the-relations",
    "title": "Adventuresin Moderation",
    "section": "Plotting the Relations",
    "text": "Plotting the Relations\n\nplot(estimate_relation(model3))"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-simple-slopes",
    "href": "posts/moderation2022/presentation.html#estimating-simple-slopes",
    "title": "Adventuresin Moderation",
    "section": "Estimating Simple Slopes",
    "text": "Estimating Simple Slopes\nThe slope of duration is different for running vs. swimming\nWe can use estimate_slopes() to get these “simple slopes”\n\n\nes3 &lt;- estimate_slopes(model3, trend = \"duration\", at = \"exercise\")\nprint(es3)\n\n\n\n\n\n\nEstimated Marginal Effects\n\n\nexercise\nCoefficient\nSE\n95% CI\nt(296)\np\n\n\n\n\nrun\n191.42\n8.28\n(175.12, 207.72)\n23.11\n&lt; .001\n\n\nswim\n243.93\n8.89\n(226.44, 261.42)\n27.45\n&lt; .001\n\n\n\nMarginal effects estimated for duration"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#plotting-simple-slopes",
    "href": "posts/moderation2022/presentation.html#plotting-simple-slopes",
    "title": "Adventuresin Moderation",
    "section": "Plotting Simple Slopes",
    "text": "Plotting Simple Slopes\n\nplot(es3)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-contrasts",
    "href": "posts/moderation2022/presentation.html#estimating-contrasts",
    "title": "Adventuresin Moderation",
    "section": "Estimating Contrasts",
    "text": "Estimating Contrasts\n\nec3 &lt;- estimate_contrasts(\n  model = model3, \n  contrast = \"exercise\", \n  at = \"duration = c(1, 2, 3)\"\n)\nprint(es3b)\n\n\n\n\n\nMarginal Contrasts Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nLevel1\nLevel2\nduration\nDifference\n95% CI\nSE\nt(296)\np\n\n\n\n\nrun\nswim\n1.00\n-13.66\n( -39.28, 11.96)\n13.02\n-1.05\n0.295\n\n\nrun\nswim\n2.00\n-66.16\n( -77.68, -54.65)\n5.85\n-11.31\n&lt; .001\n\n\nrun\nswim\n3.00\n-118.67\n(-146.10, -91.24)\n13.94\n-8.51\n&lt; .001\n\n\n\nMarginal contrasts estimated at exercise, p-value adjustment method: Holm (1979)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#hypotheses-and-data-3",
    "href": "posts/moderation2022/presentation.html#hypotheses-and-data-3",
    "title": "Adventuresin Moderation",
    "section": "Hypotheses and Data",
    "text": "Hypotheses and Data\n\nAttachment anxiety and avoidance will both predict reduced marital satisfaction\nThe lowest satisfaction will be from those high on both anxiety and avoidance\n\n\n\nset.seed(2022)\nn &lt;- 300\ndata_CxC &lt;- \n  tibble(\n    aa_anxiety = rnorm(n = n),\n    aa_avoidance = standardize(-1 + 0.4*aa_anxiety + rnorm(n = n)),\n    satisfaction = 0 - 0.5*aa_anxiety - 0.3*aa_avoidance - 0.1*aa_anxiety*aa_avoidance + rnorm(n = n)\n  ) |&gt; \n  select(satisfaction, aa_anxiety, aa_avoidance) |&gt; \n  mutate(participant = row_number(), .before = 1)\ndata_CxC |&gt; \n  head(100) |&gt; \n  kable() |&gt; \n  kable_styling() |&gt; \n  scroll_box(height = \"340px\")\n\n\n\n\n\nparticipant\nsatisfaction\naa_anxiety\naa_avoidance\n\n\n\n\n1\n0.431879247\n0.9001420\n-1.297939662\n\n\n2\n0.746419142\n-1.1733458\n-1.682195939\n\n\n3\n0.386141988\n-0.8974854\n0.051376248\n\n\n4\n0.223237298\n-1.4445014\n-0.268264733\n\n\n5\n-0.768496798\n-0.3310136\n-0.022763908\n\n\n6\n1.000404796\n-2.9006290\n0.381256632\n\n\n7\n-0.343684014\n-1.0592557\n0.362953888\n\n\n8\n-0.492258931\n0.2779547\n0.437596629\n\n\n9\n-0.130013645\n0.7494859\n-0.730239562\n\n\n10\n-0.374190517\n0.2415825\n-0.855773523\n\n\n11\n0.170935396\n1.0061857\n0.216484323\n\n\n12\n-1.179074681\n-0.1851460\n1.140081555\n\n\n13\n1.641055162\n-0.9818267\n-1.668092627\n\n\n14\n-0.969792053\n0.0929079\n-0.305004314\n\n\n15\n0.831123681\n-0.0527844\n-0.465415702\n\n\n16\n1.466304646\n-0.0803279\n0.051780263\n\n\n17\n0.322759037\n-0.6541037\n0.245552680\n\n\n18\n0.561337369\n-0.9506835\n-1.518215283\n\n\n19\n0.286701311\n1.0195618\n-0.579941309\n\n\n20\n0.225275907\n0.8590464\n-0.109149305\n\n\n21\n0.421921842\n0.3644608\n0.827752583\n\n\n22\n-0.417453680\n0.3836510\n-0.041032775\n\n\n23\n-0.830772001\n1.1134057\n1.641707224\n\n\n24\n-1.465386797\n1.2115098\n0.235375304\n\n\n25\n1.610962429\n-0.3483255\n-0.614587576\n\n\n26\n-0.194985485\n-0.8595534\n0.839564514\n\n\n27\n-0.457105604\n0.6500272\n-0.237961474\n\n\n28\n0.365488441\n0.3280591\n0.691528406\n\n\n29\n-1.296877532\n-0.5179466\n0.245441084\n\n\n30\n0.886004166\n-0.2389821\n-0.683787989\n\n\n31\n0.542570872\n0.1177789\n-1.998068236\n\n\n32\n0.156048276\n0.8315185\n0.326002402\n\n\n33\n-0.419288577\n-1.5589189\n0.699155582\n\n\n34\n0.594784180\n-0.2205191\n-1.290554360\n\n\n35\n0.688724218\n-0.8171944\n-0.052312730\n\n\n36\n-2.339678681\n1.0766774\n-0.655731087\n\n\n37\n-2.912812300\n1.0796640\n1.578970839\n\n\n38\n-0.323295275\n0.1421281\n-0.184938564\n\n\n39\n0.429499179\n0.1569795\n-0.133040850\n\n\n40\n-1.011571785\n-0.1687203\n-0.464179272\n\n\n41\n0.953965228\n-0.2690377\n-0.184774263\n\n\n42\n-0.742394409\n0.8077684\n0.286315250\n\n\n43\n0.609720987\n-1.1247172\n-0.765354434\n\n\n44\n1.104435343\n-1.4307880\n-0.819602195\n\n\n45\n-0.052903213\n0.0603567\n0.558586687\n\n\n46\n0.942914179\n-0.7929825\n-0.499114896\n\n\n47\n0.098501326\n0.3402759\n-1.418395469\n\n\n48\n0.072526822\n-0.2594687\n-1.274938955\n\n\n49\n0.793081560\n-1.3048486\n0.489336786\n\n\n50\n0.490492759\n0.3681734\n-0.560410712\n\n\n51\n-1.562719810\n1.6931900\n1.013145153\n\n\n52\n-1.805093056\n0.9958370\n-0.289695709\n\n\n53\n-2.548433604\n0.1867521\n0.602950535\n\n\n54\n0.065583432\n1.2383374\n-0.009983538\n\n\n55\n0.820340255\n0.3093733\n-0.800916638\n\n\n56\n0.505302845\n0.6357179\n-0.026735356\n\n\n57\n1.005976388\n0.0231833\n-0.890694544\n\n\n58\n-0.458843197\n1.1778636\n0.984843824\n\n\n59\n0.199413686\n-0.4535466\n0.371075569\n\n\n60\n0.431296206\n0.4159275\n-0.365996037\n\n\n61\n1.150233872\n-1.3384424\n0.467640169\n\n\n62\n-0.320824319\n-1.2919747\n-0.757080168\n\n\n63\n-0.259949719\n-0.3090742\n-0.131494356\n\n\n64\n0.248182571\n0.1565121\n0.290605617\n\n\n65\n2.238847488\n-0.8339168\n0.957765007\n\n\n66\n-0.576031454\n-0.0245493\n1.236569926\n\n\n67\n2.454918363\n-1.1373516\n-0.446288457\n\n\n68\n-1.010066059\n1.0720542\n-0.136743305\n\n\n69\n-3.302638877\n2.3144986\n1.437525398\n\n\n70\n-0.825252189\n0.4229731\n0.934149403\n\n\n71\n0.776507340\n-0.1369390\n-0.223449357\n\n\n72\n-1.048895385\n1.3283965\n-0.679285974\n\n\n73\n-0.786464332\n0.4365357\n0.555531866\n\n\n74\n0.519995823\n0.0664285\n0.576018392\n\n\n75\n-1.099304679\n1.3046598\n-0.108575916\n\n\n76\n1.764975468\n-0.2092595\n-1.025494890\n\n\n77\n-1.079996779\n1.0182530\n0.704214302\n\n\n78\n-1.129907056\n1.3660350\n-0.151365815\n\n\n79\n-1.612603053\n1.4766958\n0.476644484\n\n\n80\n-1.520932528\n0.8873065\n1.900673007\n\n\n81\n1.573886331\n-1.0159089\n-0.621593661\n\n\n82\n-2.655638182\n1.8708691\n1.603215709\n\n\n83\n-1.166521056\n1.0761984\n-0.250230829\n\n\n84\n0.949832740\n-1.0744076\n-0.164695096\n\n\n85\n-0.785293807\n-2.1955760\n0.882885288\n\n\n86\n-0.171829792\n0.5345446\n-0.347303904\n\n\n87\n-0.790922368\n1.3437334\n-0.357056895\n\n\n88\n-0.643979606\n1.3850035\n1.037789348\n\n\n89\n-2.545346034\n2.7469268\n1.338605739\n\n\n90\n-1.991499858\n-0.0459447\n-0.362004510\n\n\n91\n-0.878797469\n0.7430853\n-0.645091863\n\n\n92\n-1.023980511\n0.2604247\n-0.464250573\n\n\n93\n2.369047046\n0.4282819\n0.901133644\n\n\n94\n-3.227321712\n-0.3682588\n2.374484976\n\n\n95\n-2.774785691\n2.8874233\n2.811740910\n\n\n96\n0.291586969\n-0.6070726\n0.884398389\n\n\n97\n1.946240482\n-1.8791987\n-0.836112268\n\n\n98\n-0.621336519\n0.7183566\n-1.543285198\n\n\n99\n-0.009261351\n0.2514256\n-0.110892289\n\n\n100\n-0.913324874\n0.4670230\n-0.774792787"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#fitting-the-model-in-regression-2",
    "href": "posts/moderation2022/presentation.html#fitting-the-model-in-regression-2",
    "title": "Adventuresin Moderation",
    "section": "Fitting the Model in Regression",
    "text": "Fitting the Model in Regression\n\nmodel4 &lt;- lm(\n  formula = satisfaction ~ aa_anxiety * aa_avoidance,\n  data = data_CxC\n)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#regression-model-parameters-1",
    "href": "posts/moderation2022/presentation.html#regression-model-parameters-1",
    "title": "Adventuresin Moderation",
    "section": "Regression Model Parameters",
    "text": "Regression Model Parameters\n\nmp4 &lt;- model_parameters(model = model4)\nprint(mp4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(296)\np\n\n\n\n\n(Intercept)\n-9.64e-03\n0.06\n(-0.12, 0.11)\n-0.17\n0.869\n\n\naa anxiety\n-0.58\n0.06\n(-0.70, -0.46)\n-9.61\n&lt; .001\n\n\naa avoidance\n-0.37\n0.06\n(-0.49, -0.25)\n-6.09\n&lt; .001\n\n\naa anxiety × aa avoidance\n-0.12\n0.05\n(-0.21, -0.02)\n-2.41\n0.017"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#plotting-the-relations-1",
    "href": "posts/moderation2022/presentation.html#plotting-the-relations-1",
    "title": "Adventuresin Moderation",
    "section": "Plotting the Relations",
    "text": "Plotting the Relations\n\nplot(estimate_relation(model4))"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-simple-slopes-1",
    "href": "posts/moderation2022/presentation.html#estimating-simple-slopes-1",
    "title": "Adventuresin Moderation",
    "section": "Estimating Simple Slopes",
    "text": "Estimating Simple Slopes\n\nestimate_slopes(\n  model = model4, \n  trend = \"aa_anxiety\",\n  at = \"aa_avoidance = c(-3, -1.5, 0, 1.5, 3)\",\n)\n\n\n\n\n\nEstimated Marginal Effects\n\n\naa_avoidance\nCoefficient\nSE\n95% CI\nt(296)\np\n\n\n\n\n-3.00\n-0.23\n0.16\n(-0.55, 0.09)\n-1.39\n0.165\n\n\n-1.50\n-0.40\n0.10\n(-0.60, -0.21)\n-4.03\n&lt; .001\n\n\n0.00\n-0.58\n0.06\n(-0.70, -0.46)\n-9.61\n&lt; .001\n\n\n1.50\n-0.75\n0.09\n(-0.93, -0.58)\n-8.55\n&lt; .001\n\n\n3.00\n-0.93\n0.15\n(-1.22, -0.63)\n-6.20\n&lt; .001\n\n\n\nMarginal effects estimated for aa_anxiety"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#plotting-simple-slopes-1",
    "href": "posts/moderation2022/presentation.html#plotting-simple-slopes-1",
    "title": "Adventuresin Moderation",
    "section": "Plotting Simple Slopes",
    "text": "Plotting Simple Slopes\n\nes4a &lt;- estimate_slopes(model4, trend = \"aa_anxiety\", \n                        at = \"aa_avoidance\", length = 1000)\nplot(es4a)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#estimating-simple-slopes-2",
    "href": "posts/moderation2022/presentation.html#estimating-simple-slopes-2",
    "title": "Adventuresin Moderation",
    "section": "Estimating Simple Slopes",
    "text": "Estimating Simple Slopes\n\nestimate_slopes(\n  model = model4, \n  trend = \"aa_avoidance\",\n  at = \"aa_anxiety = c(-3, -1.5, 0, 1.5, 3)\"\n)\n\n\n\n\n\nEstimated Marginal Effects\n\n\naa_anxiety\nCoefficient\nSE\n95% CI\nt(296)\np\n\n\n\n\n-3.00\n-0.02\n0.15\n(-0.32, 0.28)\n-0.11\n0.912\n\n\n-1.50\n-0.19\n0.09\n(-0.37, -0.01)\n-2.12\n0.035\n\n\n0.00\n-0.37\n0.06\n(-0.49, -0.25)\n-6.09\n&lt; .001\n\n\n1.50\n-0.54\n0.10\n(-0.73, -0.35)\n-5.52\n&lt; .001\n\n\n3.00\n-0.72\n0.16\n(-1.03, -0.40)\n-4.43\n&lt; .001\n\n\n\nMarginal effects estimated for aa_avoidance"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#plotting-simple-slopes-2",
    "href": "posts/moderation2022/presentation.html#plotting-simple-slopes-2",
    "title": "Adventuresin Moderation",
    "section": "Plotting Simple Slopes",
    "text": "Plotting Simple Slopes\n\nes4b &lt;- estimate_slopes(model4, trend = \"aa_avoidance\", \n                        at = \"aa_anxiety\", length = 1000)\nplot(es4b)"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#challenges-and-opportunities",
    "href": "posts/moderation2022/presentation.html#challenges-and-opportunities",
    "title": "Adventuresin Moderation",
    "section": "Challenges and Opportunities",
    "text": "Challenges and Opportunities\n\nProducts are only one type of interaction (bilinear)\n\nOthers include nonlinear, threshold, etc.\n\n\n\n\nMeasurement error gets compounded in moderation\n\nTesting in the SEM framework may be necessary\nMultivariate outliers can have a large influence\n\n\n\n\n\nPower analysis is complicated for interactions\n\nInteractions tend to be very power hungry\nSimulation-based power analysis may be needed\n\n\n\n\n\nInterpretations may differ between scales in GLM\n\nCaution and justification are warranted"
  },
  {
    "objectID": "posts/moderation2022/presentation.html#references",
    "href": "posts/moderation2022/presentation.html#references",
    "title": "Adventuresin Moderation",
    "section": "References",
    "text": "References\n\n\nBauer, D. J., & Curran, P. J. (2005). Probing interactions in fixed and multilevel regression: Inferential and graphical techniques. Multivariate Behavioral Research, 40(3), 373–400. https://doi.org/10/d5wzg5\nEsarey, J., & Sumner, J. L. (2018). Marginal effects in interaction models: Determining and controlling the false positive rate. Comparative Political Studies, 51(9), 1144–1176. https://doi.org/10/gdw8xw\nFinsaas, M. C., & Goldstein, B. L. (2021). Do simple slopes follow-up tests lead us astray? Advancements in the visualization and reporting of interactions. Psychological Methods, 26(1), 38–60. https://doi.org/10/ggsng9\nMcCabe, C. J., Kim, D. S., & King, K. M. (2018). Improving present practices in the visual display of interactions. Advances in Methods and Practices in Psychological Science, 1(2), 147–165. https://doi.org/10/gf5sqb\nMcClelland, G. H., & Judd, C. M. (1993). Statistical difficulties of detecting interactions and moderator effects. Psychological Bulletin, 114(2), 376–390. https://doi.org/10/cj3kvv\nMiller, J. W., Stromeyer, W. R., & Schwieterman, M. A. (2013). Extensions of the Johnson-Neyman technique to linear models with curvilinear effects: Derivations and analytical tools. Multivariate Behavioral Research, 48(2), 267–300. https://doi.org/10/ggwpvb\nRohrer, J. M., & Arslan, R. C. (2021). Precise answers to vague questions: Issues with interactions. Advances in Methods and Practices in Psychological Science, 4(2), 1–19. https://doi.org/10/gk9zkd"
  },
  {
    "objectID": "posts/mvac2023/Presentation.html",
    "href": "posts/mvac2023/Presentation.html",
    "title": "Measurement Validation in Affective Computing",
    "section": "",
    "text": "ACII 2023 | Jeffrey M. Girard https://affcom.ku.edu/mvac"
  },
  {
    "objectID": "posts/tenure2025/index.html",
    "href": "posts/tenure2025/index.html",
    "title": "Tenure Talk",
    "section": "",
    "text": "In celebration of my promotion in August, I was invited to give a “tenure talk” to the department of psychology, reflecting on the last five years of my work as an assistant professor at the University of Kansas. In the slides below, I walk through my accomplishments, how my research framework has evolved over time, and provide two examples of my interdisciplinary work: one psychological project that was enhanced by my computational and quantitative background and one computational project that was enhanced by my psychological background.\n\nAffective and Interpersonal Communication: Advancing an Interdisciplinary Program of Research"
  }
]